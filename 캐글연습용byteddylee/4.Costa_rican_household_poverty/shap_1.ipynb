{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNzMbruU4XBHvT+N/1Qe+kS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install description"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O5lT4lIN91rK","executionInfo":{"status":"ok","timestamp":1669080773877,"user_tz":-540,"elapsed":1635,"user":{"displayName":"김시윤","userId":"10463797081079810917"}},"outputId":"97a7bd20-b0f5-4526-9e4a-21a3f29ffd16"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","\u001b[31mERROR: Could not find a version that satisfies the requirement description (from versions: none)\u001b[0m\n","\u001b[31mERROR: No matching distribution found for description\u001b[0m\n"]}]},{"cell_type":"code","source":["!pip install featuretools"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"72PfKTYR9LLv","executionInfo":{"status":"ok","timestamp":1669080610295,"user_tz":-540,"elapsed":11127,"user":{"displayName":"김시윤","userId":"10463797081079810917"}},"outputId":"d8fd636d-d682-457f-cd6b-051426379b01"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting featuretools\n","  Downloading featuretools-1.11.1-py3-none-any.whl (362 kB)\n","\u001b[K     |████████████████████████████████| 362 kB 5.0 MB/s \n","\u001b[?25hCollecting woodwork>=0.16.2\n","  Downloading woodwork-0.16.4-py3-none-any.whl (207 kB)\n","\u001b[K     |████████████████████████████████| 207 kB 38.7 MB/s \n","\u001b[?25hCollecting psutil>=5.6.6\n","  Downloading psutil-5.9.4-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (280 kB)\n","\u001b[K     |████████████████████████████████| 280 kB 7.7 MB/s \n","\u001b[?25hRequirement already satisfied: pandas>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from featuretools) (1.3.5)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.7/dist-packages (from featuretools) (1.21.6)\n","Requirement already satisfied: cloudpickle>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from featuretools) (1.5.0)\n","Requirement already satisfied: dask[dataframe]>=2021.10.0 in /usr/local/lib/python3.7/dist-packages (from featuretools) (2022.2.0)\n","Requirement already satisfied: distributed>=2021.10.0 in /usr/local/lib/python3.7/dist-packages (from featuretools) (2022.2.0)\n","Requirement already satisfied: click>=7.0.0 in /usr/local/lib/python3.7/dist-packages (from featuretools) (7.1.2)\n","Requirement already satisfied: scipy>=1.3.3 in /usr/local/lib/python3.7/dist-packages (from featuretools) (1.7.3)\n","Requirement already satisfied: tqdm>=4.32.0 in /usr/local/lib/python3.7/dist-packages (from featuretools) (4.64.1)\n","Requirement already satisfied: holidays>=0.13 in /usr/local/lib/python3.7/dist-packages (from featuretools) (0.17)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from dask[dataframe]>=2021.10.0->featuretools) (21.3)\n","Requirement already satisfied: partd>=0.3.10 in /usr/local/lib/python3.7/dist-packages (from dask[dataframe]>=2021.10.0->featuretools) (1.3.0)\n","Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.7/dist-packages (from dask[dataframe]>=2021.10.0->featuretools) (6.0)\n","Requirement already satisfied: fsspec>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from dask[dataframe]>=2021.10.0->featuretools) (2022.11.0)\n","Requirement already satisfied: toolz>=0.8.2 in /usr/local/lib/python3.7/dist-packages (from dask[dataframe]>=2021.10.0->featuretools) (0.12.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from distributed>=2021.10.0->featuretools) (57.4.0)\n","Requirement already satisfied: msgpack>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed>=2021.10.0->featuretools) (1.0.4)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from distributed>=2021.10.0->featuretools) (2.11.3)\n","Requirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in /usr/local/lib/python3.7/dist-packages (from distributed>=2021.10.0->featuretools) (2.4.0)\n","Requirement already satisfied: tornado>=5 in /usr/local/lib/python3.7/dist-packages (from distributed>=2021.10.0->featuretools) (6.0.4)\n","Requirement already satisfied: zict>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from distributed>=2021.10.0->featuretools) (2.2.0)\n","Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed>=2021.10.0->featuretools) (1.7.0)\n","Requirement already satisfied: hijri-converter in /usr/local/lib/python3.7/dist-packages (from holidays>=0.13->featuretools) (2.2.4)\n","Requirement already satisfied: convertdate>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from holidays>=0.13->featuretools) (2.4.0)\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from holidays>=0.13->featuretools) (2.8.2)\n","Requirement already satisfied: korean-lunar-calendar in /usr/local/lib/python3.7/dist-packages (from holidays>=0.13->featuretools) (0.3.1)\n","Requirement already satisfied: pymeeus<=1,>=0.3.13 in /usr/local/lib/python3.7/dist-packages (from convertdate>=2.3.0->holidays>=0.13->featuretools) (0.5.11)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->dask[dataframe]>=2021.10.0->featuretools) (3.0.9)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.3.0->featuretools) (2022.6)\n","Requirement already satisfied: locket in /usr/local/lib/python3.7/dist-packages (from partd>=0.3.10->dask[dataframe]>=2021.10.0->featuretools) (1.0.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil->holidays>=0.13->featuretools) (1.15.0)\n","Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.7/dist-packages (from woodwork>=0.16.2->featuretools) (1.0.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22->woodwork>=0.16.2->featuretools) (3.1.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22->woodwork>=0.16.2->featuretools) (1.2.0)\n","Requirement already satisfied: heapdict in /usr/local/lib/python3.7/dist-packages (from zict>=0.1.3->distributed>=2021.10.0->featuretools) (1.0.1)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->distributed>=2021.10.0->featuretools) (2.0.1)\n","Installing collected packages: psutil, woodwork, featuretools\n","  Attempting uninstall: psutil\n","    Found existing installation: psutil 5.4.8\n","    Uninstalling psutil-5.4.8:\n","      Successfully uninstalled psutil-5.4.8\n","Successfully installed featuretools-1.11.1 psutil-5.9.4 woodwork-0.16.4\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["psutil"]}}},"metadata":{}}]},{"cell_type":"code","source":["!pip install shap"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vpkaUFXY86yS","executionInfo":{"status":"ok","timestamp":1669080543479,"user_tz":-540,"elapsed":5433,"user":{"displayName":"김시윤","userId":"10463797081079810917"}},"outputId":"e9e3e41e-b5d3-494d-f41c-14e1508d5222"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting shap\n","  Downloading shap-0.41.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (569 kB)\n","\u001b[K     |████████████████████████████████| 569 kB 4.7 MB/s \n","\u001b[?25hRequirement already satisfied: numba in /usr/local/lib/python3.7/dist-packages (from shap) (0.56.4)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from shap) (1.21.6)\n","Requirement already satisfied: tqdm>4.25.0 in /usr/local/lib/python3.7/dist-packages (from shap) (4.64.1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from shap) (1.7.3)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from shap) (1.0.2)\n","Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.7/dist-packages (from shap) (21.3)\n","Collecting slicer==0.0.7\n","  Downloading slicer-0.0.7-py3-none-any.whl (14 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from shap) (1.3.5)\n","Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from shap) (1.5.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>20.9->shap) (3.0.9)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from numba->shap) (4.13.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba->shap) (57.4.0)\n","Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.7/dist-packages (from numba->shap) (0.39.1)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->numba->shap) (4.1.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->numba->shap) (3.10.0)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->shap) (2022.6)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->shap) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->shap) (1.15.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->shap) (3.1.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->shap) (1.2.0)\n","Installing collected packages: slicer, shap\n","Successfully installed shap-0.41.0 slicer-0.0.7\n"]}]},{"cell_type":"code","execution_count":1,"metadata":{"id":"GVE1Er_xKroS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669080642392,"user_tz":-540,"elapsed":5123,"user":{"displayName":"김시윤","userId":"10463797081079810917"}},"outputId":"f3a10a30-9abc-4f22-9f64-f5c4b783be5d"},"outputs":[{"output_type":"stream","name":"stderr","text":["Woodwork may not support Python 3.7 in next non-bugfix release.\n","Featuretools may not support Python 3.7 in next non-bugfix release.\n"]}],"source":["import pandas as pd\n","import numpy as np\n","\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","sns.set(font_scale=2.2)\n","plt.style.use('seaborn')\n","\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder\n","from sklearn.metrics import f1_score\n","import lightgbm as lgb\n","import itertools\n","import xgboost as xgb\n","from sklearn.impute import SimpleImputer\n","\n","from xgboost import XGBClassifier\n","\n","import shap\n","from tqdm import tqdm\n","import featuretools as ft\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","import time\n"]},{"cell_type":"markdown","source":["Check datasets"],"metadata":{"id":"g5FtB3GCQ1mH"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive/')\n","\n","direc='/content/drive/MyDrive/주피터_대피소/kaggle&github/Kaggle/4.Costa_Rican_Household_Poverty/'\n","\n","df_train=pd.read_csv(direc+'train.csv')\n","df_test=pd.read_csv(direc+'test.csv')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ShDgwigr8Jda","executionInfo":{"status":"ok","timestamp":1669080737225,"user_tz":-540,"elapsed":4303,"user":{"displayName":"김시윤","userId":"10463797081079810917"}},"outputId":"3f3bcc74-d2d0-4fd9-8f1a-122316e43377"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"]}]},{"cell_type":"code","source":["# df_train = pd.read_csv('../input/train.csv')\n","# df_test = pd.read_csv('../input/test.csv')"],"metadata":{"id":"_lC5rpG2Q0uv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('df_train shape : ', df_train.shape, '  ', 'df_test_shape : ',df_test.shape)"],"metadata":{"id":"-wxxia25Q7dm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669080743883,"user_tz":-540,"elapsed":619,"user":{"displayName":"김시윤","userId":"10463797081079810917"}},"outputId":"d5624db2-5bd8-4c78-f92a-a758c5fbc1f8"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["df_train shape :  (9557, 143)    df_test_shape :  (23856, 142)\n"]}]},{"cell_type":"markdown","source":["Make description df"],"metadata":{"id":"74ooglCmREAn"}},{"cell_type":"code","source":[],"metadata":{"id":"oxj2ZzIyRDGT","colab":{"base_uri":"https://localhost:8080/","height":172},"executionInfo":{"status":"error","timestamp":1669080747177,"user_tz":-540,"elapsed":364,"user":{"displayName":"김시윤","userId":"10463797081079810917"}},"outputId":"0eb9dcf7-6611-4726-f788-f723bfe1e602"},"execution_count":4,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-d72bc153cb4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdescription\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'description' is not defined"]}]},{"cell_type":"markdown","source":["Check null data"],"metadata":{"id":"fNzLdpYKRIlY"}},{"cell_type":"code","source":["total = df_train.isnull().sum().sort_values(ascending=False)\n","\n","percent = 100 * (df_train.isnull().sum() / df_train.isnull().count().sort_values(ascending=False))\n","\n","missing_df = pd.concat([total, percent], axis=1, keys = ['Total', 'Percent'])\n","\n","missing_df.head()"],"metadata":{"id":"nPJjTWV5RKDq","colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"status":"ok","timestamp":1669080899423,"user_tz":-540,"elapsed":810,"user":{"displayName":"김시윤","userId":"10463797081079810917"}},"outputId":"eb6aed6f-3144-4877-cb7d-220e82009259"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["           Total    Percent\n","rez_esc     7928  82.954902\n","v18q1       7342  76.823271\n","v2a1        6860  71.779847\n","SQBmeaned      5   0.052318\n","meaneduc       5   0.052318"],"text/html":["\n","  <div id=\"df-e23d160d-2068-41ea-92fc-0a898d7ba094\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Total</th>\n","      <th>Percent</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>rez_esc</th>\n","      <td>7928</td>\n","      <td>82.954902</td>\n","    </tr>\n","    <tr>\n","      <th>v18q1</th>\n","      <td>7342</td>\n","      <td>76.823271</td>\n","    </tr>\n","    <tr>\n","      <th>v2a1</th>\n","      <td>6860</td>\n","      <td>71.779847</td>\n","    </tr>\n","    <tr>\n","      <th>SQBmeaned</th>\n","      <td>5</td>\n","      <td>0.052318</td>\n","    </tr>\n","    <tr>\n","      <th>meaneduc</th>\n","      <td>5</td>\n","      <td>0.052318</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e23d160d-2068-41ea-92fc-0a898d7ba094')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-e23d160d-2068-41ea-92fc-0a898d7ba094 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-e23d160d-2068-41ea-92fc-0a898d7ba094');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","source":["Fill Missin gvalues"],"metadata":{"id":"bcfwTPkRRjuk"}},{"cell_type":"code","source":["#if education is yes and person is head of households, fill with escolari\n","\n","df_train.loc[(df_train['edjefa'] == 'yes') & (df_train['parentesco1'] ==1), 'edjefa'] = df_train.loc[(df_train['edjefa'] == 'yes') & (df_train['parentesco1'] ==1), 'escolari']\n","\n","df_train.loc[(df_train['edfeje'] =='yesa') & (df_train['parentesco1'] ==1), 'edjefe'] = df_train.loc[(df_train['edjefe'] == 'yes') & (df_train['parentesco1'] ==1), 'escolari']\n","\n","df_test.loc[(df_test['edjefa'] == \"yes\") & (df_test['parentesco1'] == 1), \"edjefa\"] = df_test.loc[(df_test['edjefa'] == \"yes\") & (df_test['parentesco1'] == 1), \"escolari\"]\n","df_test.loc[(df_test['edjefe'] == \"yes\") & (df_test['parentesco1'] == 1), \"edjefe\"] = df_test.loc[(df_test['edjefe'] == \"yes\") & (df_test['parentesco1'] == 1), \"escolari\"]\n","\n","#this filed is supposed to be interaction btw gender and escolari, but it isn;t clear what yes means, let's fill in with 4\n","\n","df_train.loc[df_train['edjefa'] =='yes', 'edjefa'] =4\n","df_train.loc[df_train['edjefe'] == 'yes', 'edfeje'] = 4\n","\n","df_test.loc[df_test['edjefa'] =='yes', 'edjefa'] =4\n","df_test.loc[df_test['edjefe'] =='yes', 'edjefe'] =4\n","\n","#create featur with max education of either head of household\n","\n","df_train['dejef'] = np.max(df_train[['edjefa','edjefe']], axis=1)\n","df_test['dejef'] = np.max(df_test[['edjefa','edjefe']], axis=1)\n","\n","#fix some inconsistencies in the data - some rows indicate both that the household does and does not have a toilet,\n","#if there is no water we'll assume they do not\n","\n","df_train.loc[(df_train['v14a']==1) & (df_train.sanitario1 ==1) & (df_train['abastaguano'] == 0 ), 'v14a'] =0\n","df_train.loc[(df_train.v14a ==1) & (df_train.sanitario1 ==1) & (df_train.abastaguano == 0 ), 'sanitario1']=0\n","\n","df_test.loc[(df_test.v14a ==  1) & (df_test.sanitario1 ==  1) & (df_test.abastaguano == 0), \"v14a\"] = 0\n","df_test.loc[(df_test.v14a ==  1) & (df_test.sanitario1 ==  1) & (df_test.abastaguano == 0), \"sanitario1\"] = 0"],"metadata":{"id":"nvk9e9KjRk-V"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["rez_esz, SQBmeaned"],"metadata":{"id":"7SOUMzRKoiV6"}},{"cell_type":"code","source":["df_train.rez_esc.fillna(0, inpalce=True)\n","df_test = df_test.rez_esc.fillna(0)"],"metadata":{"id":"jg3uysqXok5Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_train['SQBmeaned'].fillna(0, inplace=True)\n","df_test['SQBmeaned'].fillna(0, inplace=True)"],"metadata":{"id":"o6csRUdpoqXa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["meaneduc"],"metadata":{"id":"CJNZMfB0ovNb"}},{"cell_type":"code","source":["df_train.meaneduc.fillna(0, inplace=True)\n","df_test.meaneduc.fillna(0, inplace=True)"],"metadata":{"id":"gWy7P0aaow94"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["v18q1"],"metadata":{"id":"SX5uBZZco2ro"}},{"cell_type":"code","source":["df_train['v18q'].value_counts()"],"metadata":{"id":"N8Y4icz4o4cP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_train.loc[df_train['v18q'] ==1].value_counts()"],"metadata":{"id":"_ma2mMdEz_qU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_train.loc[df_train['v18q'] ==0, 'v18q'].value_counts()"],"metadata":{"id":"jxlZfXZE0KjA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_train['v18q'].fillna(0, inplace=True)\n","df_test['v18q'].fillna(0, inplace=True)"],"metadata":{"id":"E1D--1ZQ0U_y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_train['topivivi3'].value_counts()"],"metadata":{"id":"CCgQ0kz60a4f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sns.kdeplot(df_train.loc[df_train['tipovivi3'] ==1, 'v2a1'], label='Monthly rent payment of household(rented=1)' )\n","\n","sns.kdeplot(df_train.loc[df_train['tipovivi3'] ==0, 'v2a1'], label='monthly rent payment of household(rented=0')\n","\n","plt.xscale('log')\n","plt.show()"],"metadata":{"id":"FPYym4La0fz0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_train['v2a1'].fillna(0, inplace=True)\n","df_test['v2a1'].fillna(0, inplace=True)"],"metadata":{"id":"lK9kqlpr1DRg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["total = df_train.isnull().sum().sort_values(ascending=False)\n","\n","percent = 100 * (df_train.isnull().sum() / df_train.isnull().count()).sort_values(ascending=False)\n","\n","#can also apply 100*(df_train.isnull().sum() / len(df_train.isnull()))\n","\n","missing_df =  pd.concat([total,percent], axis=1, keys=['Total', 'Percent'])\n","\n","missing_df.head()"],"metadata":{"id":"UjUiyCqs1Jg5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["total = df_test.isnull().sum().sort_values(ascending=False)\n","\n","percent = 100 * (df_test.isnull().sum() / df_test.isnull().count()).sort_values(ascending=False)\n","\n","#can also apply 100*(df_train.isnull().sum() / len(df_train.isnull()))\n","\n","missing_df =  pd.concat([total,percent], axis=1, keys=['Total', 'Percent'])\n","\n","missing_df.head()"],"metadata":{"id":"ddDbxlxQ1lND"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Feature engineering"],"metadata":{"id":"12YaGiBm1xT-"}},{"cell_type":"markdown","source":["Object Features"],"metadata":{"id":"HZS7tDrH122U"}},{"cell_type":"code","source":["features_object = [col for col in df_train.columns if df_train[col].dtype == 'object']"],"metadata":{"id":"hMg1XKDN135P"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["dependecncy"],"metadata":{"id":"dxVnYNno2QGT"}},{"cell_type":"code","source":["df_train['dependency'] = np.sqrt(df_train['SQBdependency'])\n","\n","df_test['dependency'] = np.sqrt(df_test['SQBdependency'])"],"metadata":{"id":"2HcJQmi21uvq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["edjefe"],"metadata":{"id":"aRjfDYi42Zxl"}},{"cell_type":"code","source":["def replace_binary(x) :\n","  if x == 'yes' :\n","    return 1\n","  elif x == 'no' :\n","    return 0\n","  else :\n","    return np.nan\n","\n","df_train['edjefe'] = df_train['edjefe'].apply(replace_binary).astype(float)\n","df_test['edjefe'] = df_test['edjefe'].apply(replace_binary).astype(float)\n","\n","df_train['edjefa'] = df_test['edjefa'].apply(replace_binary).astype(float)\n","df_test['edjefa'] = df_test['edjefa'].apply(reaplce_binary).astype(float)"],"metadata":{"id":"ef_DRLid2aON"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#create feature with max education of either head of household\n","df_train['edjef'] = np.max(df_train[['edjefe','edjefa']],axis=1)\n","df_test['edjef'] = np.max(df_test[['edjefe','edjefa']],axis=1)"],"metadata":{"id":"fN0KZQH9289y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["roof and electricity"],"metadata":{"id":"Jc1TpvGm3nWO"}},{"cell_type":"code","source":["df_train['roof_waste_material'] =np.nan\n","df_test['roof_waste_material'] =np.nan\n","df_train['electricity_other'] =np.nan\n","df_test['electricity_other'] =np.nan\n","\n","def fill_roof_exception(x) :\n","  if (x['techozinc'==0]) & (x['techoentrepiso'] ==0) & (x['techocane']==0) & (x['techootro']==0) :\n","    return 1\n","  else :\n","    return 0\n","\n","def fill_no_electricity(x) :\n","  if (x['public'] ==0) & (x['planpri']==0) & (x['noelec'] ==0) & (x['coopele']==0) :\n","    return 1\n","  else :\n","    return 0\n","\n","\n","df_train['roof_waste_material'] = df_train.apply(lambda x : fill_roof_exception(x), axis=1)\n","df_test['roof_waste_material'] = df_test.apply(lambda x : fill_roof_exception(x), axis=1)\n","df_train['electiricity_other'] = df_train.apply(lambda x : fill_no_electricity(x), axis=1)\n","df_test['electiricity_other'] = df_test.apply(lambda x : fill_no_electricity(x), axis=1)"],"metadata":{"id":"BF3-xEv32YLB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bin_cat_feature = [col for col in df_train.columns if df_train[col].value_counts().shape[0] ==2]"],"metadata":{"id":"PGzXdwci4kiL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2-3. Make new FEatures using continuous feature"],"metadata":{"id":"DoEY_emXar0j"}},{"cell_type":"code","source":["continuous_features = [col for col in df_train.columns if col not in bin_cat_feature]\n","\n","continuous_features = [col for col in continuous_features if col not in features_object]\n","\n","continuous_features = [col for col incontinuous_features if col not in ['Id','Target','idhogar']]"],"metadata":{"id":"3eU0qbA_auNY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('There are {} continuous features'.format(len(continuous_features)))\n","\n","for col in continuous_features :\n","  print('{} {}'.foramt(col, description.loc[description['varname']==col, 'description'].values))"],"metadata":{"id":"rpOUokcca-6e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_train['edjef'].value_counts()"],"metadata":{"id":"Qbw_wN_xbLhN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_train.drop('tamhog',axis=1, inplace=True)\n","\n","df_test.drop('tamhog', axis=1, inplace=True)"],"metadata":{"id":"4DXw1UK7bOjT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Squared features\n","\n","- There are many squared features. Actually, tree models like lightgbm dont need them. But at this kernel, I want to use lightgbm as feature filter model and set entity - embedding as classifier. So let's keep them."],"metadata":{"id":"FOXeCnNNbUMN"}},{"cell_type":"markdown","source":["Family features\n","- hogar_nin, hogar_adul, hogar_mayor, hogar_total, r4h1, r4h2, r4h3, r4m1, r4m2, r4m3, r4t1, r4t2, r4t3, tmbhog, tamvid, rez_esc, escolari\n","- Family size features (substract, ratio)"],"metadata":{"id":"eDSkL-_obrgn"}},{"cell_type":"code","source":["df_train['adult'] = df_train['hogar_adul'] - df_train['hogar_mayor']\n","df_train['dependency_count'] = df_train['hogar_nin'] + df_train['hogar_mayor']\n","df_train['dependency'] = df_train['dependency_count'] / df_train['adult']\n","df_train['child_percent'] = df_train['hogar_nin'] / df_train['hogar_total']\n","df_train['elder_percent'] = df_train['hogar_mayor'] / df_train['hogar_total']\n","df_train['adult_percent'] = df_train['hogar_adul'] / df_train['hogar_total']\n","df_train['males_younger_12_years_percent'] = df_train['r4h1'] / df_train['hogar_total']\n","df_train['males_older_12_years_percent'] = df_train['r4h2'] / df_train['hogar_total']\n","df_train['males_percent'] = df_train['r4h3'] / df_train['hogar_total']\n","df_train['females_younger_12_years_percent'] = df_train['r4m1'] / df_train['hogar_total']\n","df_train['females_older_12_years_percent'] = df_train['r4m2'] / df_train['hogar_total']\n","df_train['females_percent'] = df_train['r4m3'] / df_train['hogar_total']\n","df_train['persons_younger_12_years_percent'] = df_train['r4t1'] / df_train['hogar_total']\n","df_train['persons_older_12_years_percent'] = df_train['r4t2'] / df_train['hogar_total']\n","df_train['persons_percent'] = df_train['r4t3'] / df_train['hogar_total']"],"metadata":{"id":"lYMrn4a_4w3u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_test['adult'] = df_test['hogar_adul'] - df_test['hogar_mayor']\n","df_test['dependency_count'] = df_test['hogar_nin'] + df_test['hogar_mayor']\n","df_test['dependency'] = df_test['dependency_count'] / df_test['adult']\n","df_test['child_percent'] = df_test['hogar_nin'] / df_test['hogar_total']\n","df_test['elder_percent'] = df_test['hogar_mayor'] / df_test['hogar_total']\n","df_test['adult_percent'] = df_test['hogar_adul'] / df_test['hogar_total']\n","df_test['males_younger_12_years_percent'] = df_test['r4h1'] / df_test['hogar_total']\n","df_test['males_older_12_years_percent'] = df_test['r4h2'] / df_test['hogar_total']\n","df_test['males_percent'] = df_test['r4h3'] / df_test['hogar_total']\n","df_test['females_younger_12_years_percent'] = df_test['r4m1'] / df_test['hogar_total']\n","df_test['females_older_12_years_percent'] = df_test['r4m2'] / df_test['hogar_total']\n","df_test['females_percent'] = df_test['r4m3'] / df_test['hogar_total']\n","df_test['persons_younger_12_years_percent'] = df_test['r4t1'] / df_test['hogar_total']\n","df_test['persons_older_12_years_percent'] = df_test['r4t2'] / df_test['hogar_total']\n","df_test['persons_percent'] = df_test['r4t3'] / df_test['hogar_total']"],"metadata":{"id":"NlvYmwcQb91V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_train['males_younger_12_years_in_household_size'] = df_train['r4h1'] / df_train['hhsize']\n","df_train['males_older_12_years_in_household_size'] = df_train['r4h2'] / df_train['hhsize']\n","df_train['males_in_household_size'] = df_train['r4h3'] / df_train['hhsize']\n","df_train['females_younger_12_years_in_household_size'] = df_train['r4m1'] / df_train['hhsize']\n","df_train['females_older_12_years_in_household_size'] = df_train['r4m2'] / df_train['hhsize']\n","df_train['females_in_household_size'] = df_train['r4m3'] / df_train['hogar_total']\n","df_train['persons_younger_12_years_in_household_size'] = df_train['r4t1'] / df_train['hhsize']\n","df_train['persons_older_12_years_in_household_size'] = df_train['r4t2'] / df_train['hhsize']\n","df_train['persons_in_household_size'] = df_train['r4t3'] / df_train['hhsize']"],"metadata":{"id":"5Zr2z30fcBu5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_test['males_younger_12_years_in_household_size'] = df_test['r4h1'] / df_test['hhsize']\n","df_test['males_older_12_years_in_household_size'] = df_test['r4h2'] / df_test['hhsize']\n","df_test['males_in_household_size'] = df_test['r4h3'] / df_test['hhsize']\n","df_test['females_younger_12_years_in_household_size'] = df_test['r4m1'] / df_test['hhsize']\n","df_test['females_older_12_years_in_household_size'] = df_test['r4m2'] / df_test['hhsize']\n","df_test['females_in_household_size'] = df_test['r4m3'] / df_test['hogar_total']\n","df_test['persons_younger_12_years_in_household_size'] = df_test['r4t1'] / df_test['hhsize']\n","df_test['persons_older_12_years_in_household_size'] = df_test['r4t2'] / df_test['hhsize']\n","df_test['persons_in_household_size'] = df_test['r4t3'] / df_test['hhsize']"],"metadata":{"id":"OzVhqeuRcIoc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_train['overcrowding_room_and_bedroom'] = (df_train['hacdor'] + df_train['hacapo'])/2\n","df_test['overcrowding_room_and_bedroom'] = (df_test['hacdor'] + df_test['hacapo'])/2"],"metadata":{"id":"jRDc95wFcKuQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_train['escolari_age'] = df_train['escolari']/df_train['age']\n","df_test['escolari_age'] = df_test['escolari']/df_test['age']\n","\n","df_train['age_12_19'] = df_train['hogar_nin'] - df_train['r4t1']\n","df_test['age_12_19'] = df_test['hogar_nin'] - df_test['r4t1']"],"metadata":{"id":"oTh3uX1ucMOF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_train['phones-per-capita'] = df_train['qmobilephone'] / df_train['tamviv']\n","df_train['tablets-per-capita'] = df_train['v18q1'] / df_train['tamviv']\n","df_train['rooms-per-capita'] = df_train['rooms'] / df_train['tamviv']\n","df_train['rent-per-capita'] = df_train['v2a1'] / df_train['tamviv']"],"metadata":{"id":"CkU1DOQdcNXC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_test['phones-per-capita'] = df_test['qmobilephone'] / df_test['tamviv']\n","df_test['tablets-per-capita'] = df_test['v18q1'] / df_test['tamviv']\n","df_test['rooms-per-capita'] = df_test['rooms'] / df_test['tamviv']\n","df_test['rent-per-capita'] = df_test['v2a1'] / df_test['tamviv']"],"metadata":{"id":"hhjlsjiqcOod"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- You can see that ' total person in the household' != '# fo total individuals in the household'.\n","\n","- somewhat weired. but for now i will keep it."],"metadata":{"id":"_TUfDjFAcR4x"}},{"cell_type":"code","source":["(df_train['hogar_total'] == df_train['r4t3']).sum()"],"metadata":{"id":"dDJGWJF5caur"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Rent per family features\n","\n","- i will reduce the # features using shap, so let's generate many features."],"metadata":{"id":"hG4dR6NZcfxO"}},{"cell_type":"code","source":["family_size_features = ['adult', 'hogar_adul', 'hogar_mayor', 'hogar_nin', 'hogar_total', 'r4h1', \n","                        'r4h2', 'r4h3', 'r4m1', 'r4m2', 'r4m3', 'r4t1', 'r4t2', 'r4t3', 'hhsize']\n","\n","new_feats = []\n","\n","for col in family_size_features :\n","  new_col_name = 'new_{}_per_{}'.format('v2a1',col)\n","  new_feats.append(new_col_name)\n","  df_train[new_col_name] = df_train['v2a1'] / df_train[col]\n","  df_test[new_col_name] = df_test['v2a1'] / df_test[col]"],"metadata":{"id":"ZVbsRngTcenK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for col in new_feats :\n","  df_train[col].replace([np.inf], np.nan, inplace=True)\n","  df_train[col].fillna(0, inplace=True)\n","\n","  df_test[col].replace([np.inf], np.nan, inplace=True)\n","  df_test[col].fillna(0, inplace=True)"],"metadata":{"id":"r1LD76-8c8tM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Room per family features"],"metadata":{"id":"gL3kCVK8dN_G"}},{"cell_type":"code","source":["new_feats = []\n","\n","for col in family_size_features :\n","  new_col_name = 'new_{}_per_{}'.format('rooms',col)\n","  new_feats.append(new_col_name)\n","\n","  df_train[new_col_name] = df_train['rooms'] / df_train[col]\n","  df_test[new_col_name] = df_test['rooms'] / df_test[col]\n","\n","\n","for col in new_feats :\n","  df_train[col].replace([np.inf], np.nan, inplace=True)\n","  df_train[col].fillna(0, inplace=True)\n","\n","  df_test[col].replace=([np.inf], np.nan, inplace=True)\n","  df_test[col].fillna(0, inplace=True)"],"metadata":{"id":"ekU73DgMdNGy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["BedRoom per faily features"],"metadata":{"id":"ZpWYHwZJd0YS"}},{"cell_type":"code","source":["new_feats = []\n","for col in family_size_features:\n","    new_col_name = 'new_{}_per_{}'.format('bedrooms', col)\n","    new_feats.append(new_col_name)\n","    df_train[new_col_name] = df_train['bedrooms'] / df_train[col]\n","    df_test[new_col_name] = df_test['bedrooms'] / df_test[col]\n","\n","for col in new_feats:\n","    df_train[col].replace([np.inf], np.nan, inplace=True)\n","    df_train[col].fillna(0, inplace=True)\n","    \n","    df_test[col].replace([np.inf], np.nan, inplace=True)\n","    df_test[col].fillna(0, inplace=True)"],"metadata":{"id":"N0qo6mO0dy91"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(df_train.shape, df_test.shape) \n","# To check the same number of features between train and test (target is there in train)"],"metadata":{"id":"3IJZ2mvJd4Vn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Tabulet per family features"],"metadata":{"id":"wf58b-Ktd_32"}},{"cell_type":"code","source":["new_feats = []\n","\n","for col in family_size_features :\n","  new_col_name  = 'new_{}_per_{}'.format('v18q1', col)\n","  new_feats.append(new_col_name)\n","\n","  df_train[new_col_name] = df_train ['v18q1'] / df_train[col]\n","  df_test[new_col_name] = df_test['v18q1'] / df_test[col]\n","\n","for col in new_feats :\n","  df_train[col].replace([np.inf], np.nan, inplace=True)\n","  df_train[col].fillna(0, inplace=True)\n","\n","  df_test[col].replace=([np.inf],np.nan, inplace=True)\n","  df_test[col].fillna(0, inplace=True)"],"metadata":{"id":"wlSeBXPCeBTX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["phone per family features"],"metadata":{"id":"EhYVHi5yeiMs"}},{"cell_type":"code","source":["new_feats = []\n","for col in family_size_features:\n","    new_col_name = 'new_{}_per_{}'.format('qmobilephone', col)\n","    new_feats.append(new_col_name)\n","    df_train[new_col_name] = df_train['qmobilephone'] / df_train[col]\n","    df_test[new_col_name] = df_test['qmobilephone'] / df_test[col]\n","\n","for col in new_feats:\n","    df_train[col].replace([np.inf], np.nan, inplace=True)\n","    df_train[col].fillna(0, inplace=True)\n","    \n","    df_test[col].replace([np.inf], np.nan, inplace=True)\n","    df_test[col].fillna(0, inplace=True)"],"metadata":{"id":"Daz4Gg9Xeggg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["rez_esc(years behond in school) per family features"],"metadata":{"id":"ew8YjCEFemj3"}},{"cell_type":"code","source":["new_feats = []\n","for col in family_size_features:\n","    new_col_name = 'new_{}_per_{}'.format('rez_esc', col)\n","    new_feats.append(new_col_name)\n","    df_train[new_col_name] = df_train['rez_esc'] / df_train[col]\n","    df_test[new_col_name] = df_test['rez_esc'] / df_test[col]\n","\n","for col in new_feats:\n","    df_train[col].replace([np.inf], np.nan, inplace=True)\n","    df_train[col].fillna(0, inplace=True)\n","    \n","    df_test[col].replace([np.inf], np.nan, inplace=True)\n","    df_test[col].fillna(0, inplace=True)"],"metadata":{"id":"Ty8OHiPuepCz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_train['rez_esc_age'] = df_train['rez_esc'] / df_train['age']\n","df_train['rez_esc_escolari'] = df_train['rez_esc'] / df_train['escolari']\n","\n","df_test['rez_esc_age'] = df_test['rez_esc'] / df_test['age']\n","df_test['rez_esc_escolari'] = df_test['rez_esc'] / df_test['escolari']"],"metadata":{"id":"j7irbUs4eqpy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Rich features\n","- i think the more richer, the larger number of phones and tablet"],"metadata":{"id":"-qvthnxiesxt"}},{"cell_type":"code","source":["df_train['tabulet_*_qmobilephone'] = df_train['v18q1']*df_train['qmobilephone']\n","\n","df_test['tabulet_*_qmobilephone'] = df_test['v18q1']*df_test['qmobilephone']"],"metadata":{"id":"zpVcUDF1esII"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- wall, roof, floor may be key factor.\n","- let's multiply each of them. Because they are binary cat features, so multification of each features generates new categorical features"],"metadata":{"id":"gxxibxuufNYo"}},{"cell_type":"code","source":["# wall and roof\n","\n","for col1 in ['epared1', 'epared2', 'epared3'] :\n","  for col2 in ['etecho1','etecho2','etecho3'] :\n","    new_col_name = 'new_{}x_{}'.format(col1, col2)\n","    df_train[new_col_name] = df_train[col] * df_train[col2]\n","    df_test[new_col_name] = df_test[col] * df_test[col2]\n","\n","\n","#wall and floor\n","for col1 in ['epared1', 'epared2', 'epared3']:\n","    for col2 in ['eviv1', 'eviv2', 'eviv3']:\n","        new_col_name = 'new_{}_x_{}'.format(col1, col2)\n","        df_train[new_col_name] = df_train[col1] * df_train[col2]\n","        df_test[new_col_name] = df_test[col1] * df_test[col2]\n","\n","# roof and floor\n","for col1 in ['etecho1', 'etecho2', 'etecho3']:\n","    for col2 in ['eviv1', 'eviv2', 'eviv3']:\n","        new_col_name = 'new_{}_x_{}'.format(col1, col2)\n","        df_train[new_col_name] = df_train[col1] * df_train[col2]\n","        df_test[new_col_name] = df_test[col1] * df_test[col2]"],"metadata":{"id":"M9TbsOjgfDgh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["combinating using three features"],"metadata":{"id":"1xheOH_HfyBk"}},{"cell_type":"code","source":["for col1 in ['epared1','epared2','epared3'] :\n","  for col2 in ['etecho1','etecho2','etecho3'] :\n","    for col3 in ['eviv1','eviv2','eviv3'] :\n","      new_col_name = 'new_{}_*_{}_*_{}'.format(col1, col2, col3)\n","\n","      df_train[new_col_name] = df_train[col1] * df_train[col2] * df_train[col3]\n","\n","      df_test[new_col_name] = df_test[col1] * df_test[col2] * df_test[col3]\n","\n","      "],"metadata":{"id":"ITC6_0Vifzm0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(df_train.shape, df_test.shape)"],"metadata":{"id":"6G2LImqmgMGW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- i want to mix electricity and energy features --> energy features"],"metadata":{"id":"mzLR9lYmgS5H"}},{"cell_type":"code","source":["for col1 in ['public','planpri','noelec','coopele'] :\n","  for col2 in ['energcocinar1','energcocinar2','energcocinar3','energcocinar4'] :\n","    new_col_name = 'new_{}_x_{}'.format(col1, col2)\n","    df_train[new_col_name] = df_train[col1] * df_train[col2]\n","    df_test[new_col_name] = df_test[col1]  * df_train[col2]"],"metadata":{"id":"bKNoua3kgOlH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["i want to mix toilet and rubbish disposal features --> other_infra features"],"metadata":{"id":"P98tEI29g5DV"}},{"cell_type":"code","source":["for col1 in ['sanitario1', 'sanitario2', 'sanitario3', 'sanitario5', 'sanitario6']:\n","    for col2 in ['elimbasu1', 'elimbasu2', 'elimbasu3', 'elimbasu4', 'elimbasu5', 'elimbasu6']:\n","        new_col_name = 'new_{}_x_{}'.format(col1, col2)\n","        df_train[new_col_name] = df_train[col1] * df_train[col2]\n","        df_test[new_col_name] = df_test[col1] * df_test[col2]"],"metadata":{"id":"l-QkO50xgpT1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["mix water provision features --> water features"],"metadata":{"id":"hwF57P_NhxM2"}},{"cell_type":"code","source":["for col1 in ['abastaguadentro', 'abastaguafuera', 'abastaguano']:\n","    for col2 in ['sanitario1', 'sanitario2', 'sanitario3', 'sanitario5', 'sanitario6']:\n","        new_col_name = 'new_{}_x_{}'.format(col1, col2)\n","        df_train[new_col_name] = df_train[col1] * df_train[col2]\n","        df_test[new_col_name] = df_test[col1] * df_test[col2]"],"metadata":{"id":"lCsDthdshwRn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(df_train.shape, df_test.shape)"],"metadata":{"id":"tyOphhbLhz9y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["mix education and area features --> education_zone_features"],"metadata":{"id":"RSHLVhoeh33L"}},{"cell_type":"code","source":["for col1 in ['area1', 'area2']:\n","    for col2 in ['instlevel1', 'instlevel2', 'instlevel3', 'instlevel4', 'instlevel5', 'instlevel6', 'instlevel7', 'instlevel8', 'instlevel9']:\n","        new_col_name = 'new_{}_x_{}'.format(col1, col2)\n","        df_train[new_col_name] = df_train[col1] * df_train[col2]\n","        df_test[new_col_name] = df_test[col1] * df_test[col2]"],"metadata":{"id":"mr44fLJxh2Tq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for col1 in ['lugar1', 'lugar2', 'lugar3', 'lugar4', 'lugar5', 'lugar6']:\n","    for col2 in ['instlevel1', 'instlevel2', 'instlevel3', 'instlevel4', 'instlevel5', 'instlevel6', 'instlevel7', 'instlevel8', 'instlevel9']:\n","        new_col_name = 'new_{}_x_{}'.format(col1, col2)\n","        df_train[new_col_name] = df_train[col1] * df_train[col2]\n","        df_test[new_col_name] = df_test[col1] * df_test[col2]"],"metadata":{"id":"A0C_83o3h9Xc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["multiply television / mobilephone / computer / tabulet / regrigerator --> electronic features"],"metadata":{"id":"Aiu_bANWiBQm"}},{"cell_type":"code","source":["df_train['electronics'] = df_train['computer'] * df_train['mobilephone'] * df_train['television'] * df_train['v18q'] * df_train['refrig']\n","df_test['electronics'] = df_test['computer'] * df_test['mobilephone'] * df_test['television'] * df_test['v18q'] * df_test['refrig']\n","\n","df_train['no_appliances'] = df_train['refrig'] + df_train['computer'] + df_train['television'] + df_train['mobilephone']\n","df_test['no_appliances'] = df_test['refrig'] + df_test['computer'] + df_test['television'] + df_test['mobilephone']"],"metadata":{"id":"t7ZCypHUiIPz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["mix all material of roof, floor, wall"],"metadata":{"id":"TvUSRRBpiRHS"}},{"cell_type":"code","source":["for col1 in ['paredblolad', 'paredzocalo', 'paredpreb', 'pareddes', 'paredmad', 'paredzinc', 'paredfibras', 'paredother']:\n","    for col2 in ['pisomoscer', 'pisocemento', 'pisoother', 'pisonatur', 'pisonotiene', 'pisomadera']:\n","      new_col_name = 'new_{}_*_{}'.format(col1,col2)\n","      df_train[new_col_name] = df_train[col1] * df_train[col2]\n","      df_test[new_col_name] = df_test[col1] * df_test[col2]\n","\n","\n","for col1 in ['pisomoscer', 'pisocemento','pisoother','pisonatur','pisonotiene','pisomadera'] :\n","  for col2 in ['techozinc','techoentrepiso','techocane','techootro'] :\n","    new_col_name = 'new_{}_*{}'.format(col1,col2)\n","    df_train[new_col_name] = df_train[col1] * df_train[col2]\n","    df_test[new_col_name] = df_test[col1] * df_test[col2]\n","\n","\n","for col1 in ['paredblolad', 'paredzocalo', 'paredpreb', 'pareddes', 'paredmad', 'paredzinc', 'paredfibras', 'paredother']:\n","    for col2 in ['techozinc', 'techoentrepiso', 'techocane', 'techootro']:\n","        new_col_name = 'new_{}_x_{}'.format(col1, col2)\n","        df_train[new_col_name] = df_train[col1] * df_train[col2]\n","        df_test[new_col_name] = df_test[col1] * df_test[col2]        \n","        \n","for col1 in ['paredblolad', 'paredzocalo', 'paredpreb', 'pareddes', 'paredmad', 'paredzinc', 'paredfibras', 'paredother']:\n","    for col2 in ['pisomoscer', 'pisocemento', 'pisoother', 'pisonatur', 'pisonotiene', 'pisomadera']:\n","        for col3 in ['techozinc', 'techoentrepiso', 'techocane', 'techootro']:\n","            new_col_name = 'new_{}_x_{}_x_{}'.format(col1, col2, col3)\n","            df_train[new_col_name] = df_train[col1] * df_train[col2] * df_train[col3]\n","            df_test[new_col_nam"],"metadata":{"id":"XphWFDIgiQka"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(df_train.shape, df_test.shape)"],"metadata":{"id":"PzageHwzjLkj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Remove feature with only one value"],"metadata":{"id":"Ir9HYeiyjSIU"}},{"cell_type":"code","source":["cols_with_only_one_value = []\n","for col in df_train.columns :\n","  if col == 'Target' :\n","    continue\n","  if df_train[col].value_counts().shape[0] == 1 or df_test[col].value_counts().shape[0] ==1 :\n","    print(col)\n","\n","    cols_with_only_one_value.append(col)"],"metadata":{"id":"sqeU6V9DjNsP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_train.drop(cols_with_only_one_value, axis=1, inpalce=True)\n","df_test = df_test.drop(cols_with_only_one_value, axis=1)"],"metadata":{"id":"tufwB4MRjkLy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Check whether both train and test have same features"],"metadata":{"id":"96zrZ0CRjzrM"}},{"cell_type":"code","source":["cols_train = np.array(sorted([col for col in df_train.columns if col != 'Targe']))\n","\n","cols_test = np.array(sorted(df_test.columns))"],"metadata":{"id":"bLhMnueqjzBk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["(cols_train == cols.test).sum() == len(cols_train)"],"metadata":{"id":"tZL3FFMFj9hI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["aggregation features\n","- in this competition, each samples are member of specific household. So let's aggregate based on ***'idhogar'*** values"],"metadata":{"id":"dE7m_8PTkKPl"}},{"cell_type":"markdown","source":["agg for family features"],"metadata":{"id":"CmSBM09rkQp-"}},{"cell_type":"code","source":["def max_min(x) :\n","  return x.max()-x.min()"],"metadata":{"id":"TtJIjZlTkFkn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["agg_train = pd.DataFrame()\n","agg_test = pd.DataFrame()\n","\n","for item in tqdm(family_size_features) :\n","  for i, function in enumerate(['mean','std','min','max','sum','count', max_min]) :\n","    group_train = df_train[item].groupby(df_train['idhogar']).agg(funcion)\n","\n","    group_test = df_test[item].groupby(df_train['idhogar']).agg(function)\n","\n","    if i==6 :\n","      new_col = item + '_new_' + 'max_min'\n","    else :\n","      new_col = item + '_new_' + function\n","    \n","    agg_train[new_col] = group_train\n","    agg_test[new_col] = group_test\n","\n","print('new agg train set has {} rows, and {} features'.format(agg_train.shape[0], agg_train.shape[1]))\n","\n","print('new agg test set has {} rows, and {} features'.format(agg_test.shape[0], agg_test.shape[1]))"],"metadata":{"id":"-K2nn1V1kUYR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["aggr_list = ['rez_esc', 'dis', 'male', 'female', \n","                  'estadocivil1', 'estadocivil2', 'estadocivil3', 'estadocivil4', 'estadocivil5', 'estadocivil6', 'estadocivil7', \n","                  'parentesco2', 'parentesco3', 'parentesco4', 'parentesco5', 'parentesco6', 'parentesco7', 'parentesco8', 'parentesco9', 'parentesco10', \n","                  'parentesco11', 'parentesco12',\n","                  'instlevel1', 'instlevel2', 'instlevel3', 'instlevel4', 'instlevel5', 'instlevel6', 'instlevel7', 'instlevel8', 'instlevel9',\n","                 'epared1', 'epared2', 'epared3', 'etecho1', 'etecho2', 'etecho3', 'eviv1', 'eviv2', 'eviv3', 'refrig', 'television', 'mobilephone',\n","            'area1', 'area2', 'v18q', 'edjef']"],"metadata":{"id":"67L0setWlVxN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for item in tqdm(aggr_list) :\n","  for function in ['count','sum'] :\n","    group_train = df_train[item].groupby(df_train['idhogar']).agg(function)\n","    group_test = df_test[item].groupby(df_test['idhogar']).agg(function)\n","\n","    new_col = item + '_new1_' + function\n","    agg_train[new_col] = group_train\n","    agg_test[new_col] = group_test\n","\n","print('neww aggregate train set ha s{} rows, and {} features.'.format(agg_train.shape[0], agg_train.shape[1]))\n","print('new aggregate test set has {} rows and {} features'.format(agg_test.shape[0], agg_test.shape[1]))"],"metadata":{"id":"p2iNCXjqmNMf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["aggr_list = ['escolari', 'age', 'escolari_age', 'dependency', 'bedrooms', 'overcrowding', 'rooms', 'qmobilephone', 'v18q1']\n","\n","for item in tqdm(aggr_list):\n","    for function in ['mean','std','min','max','sum', 'count', max_min]:\n","        group_train = df_train[item].groupby(df_train['idhogar']).agg(function)\n","        group_test = df_test[item].groupby(df_test['idhogar']).agg(function)\n","        if i == 6:\n","            new_col = item + '_new2_' + 'max_min'\n","        else:\n","            new_col = item + '_new2_' + function\n","        agg_train[new_col] = group_train\n","        agg_test[new_col] = group_test\n","\n","print('new aggregate train set has {} rows, and {} features'.format(agg_train.shape[0], agg_train.shape[1]))\n","print('new aggregate test set has {} rows, and {} features'.format(agg_test.shape[0], agg_test.shape[1]))"],"metadata":{"id":"hZZTlya8mwck"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["agg_test = agg_test.reset_index()\n","agg_train = agg_train.reset_index()\n","\n","train_agg = pd.merge(df_train, agg_train, on='idhogar')\n","test = pd.merge(df_test, agg_test, on='idhogar')\n","\n","#fill all na as 0\n","train_agg.fillna(0, inplace=True)\n","test.fillna(0, inplace=True)\n","\n","print('train shape : ', train_agg.shape, 'test_shape', test.shape)"],"metadata":{"id":"UYJer5o-nAtu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["aggr_list = ['rez_esc', 'dis', 'male', 'female', \n","                  'estadocivil1', 'estadocivil2', 'estadocivil3', 'estadocivil4', 'estadocivil5', 'estadocivil6', 'estadocivil7', \n","                  'parentesco2', 'parentesco3', 'parentesco4', 'parentesco5', 'parentesco6', 'parentesco7', 'parentesco8', 'parentesco9', 'parentesco10', \n","                  'parentesco11', 'parentesco12',\n","                  'instlevel1', 'instlevel2', 'instlevel3', 'instlevel4', 'instlevel5', 'instlevel6', 'instlevel7', 'instlevel8', 'instlevel9',\n","                 'epared1', 'epared2', 'epared3', 'etecho1', 'etecho2', 'etecho3', 'eviv1', 'eviv2', 'eviv3', 'refrig', 'television', 'mobilephone',\n","            'area1', 'area2', 'v18q', 'edjef']\n","  \n","\n","for lugar in ['lugar1', 'lugar2', 'lugar3', 'lugar4', 'lugar5', 'lugar6']:\n","    group_train = df_train[[lugar, 'idhogar'] + aggr_list].groupby([lugar, 'idhogar']).sum().reset_index()\n","    group_train.columns = [lugar, 'idhogar'] + ['new3_{}_idhogar_{}'.format(lugar, col) for col in group_train][2:]\n","\n","    group_test = df_test[[lugar, 'idhogar'] + aggr_list].groupby([lugar, 'idhogar']).sum().reset_index()\n","    group_test.columns = [lugar, 'idhogar'] + ['new3_{}_idhogar_{}'.format(lugar, col) for col in group_test][2:]\n","\n","    train_agg = pd.merge(train_agg, group_train, on=[lugar, 'idhogar'])\n","    test = pd.merge(test, group_test, on=[lugar, 'idhogar'])\n","    \n","print('train shape:', train_agg.shape, 'test shape:', test.shape)"],"metadata":{"id":"6Jk7mxE_nXGt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cols_nums = ['age', 'meaneduc', 'dependency', \n","             'hogar_nin', 'hogar_adul', 'hogar_mayor', 'hogar_total',\n","             'bedrooms', 'overcrowding']\n","\n","for function in tqdm(['mean','std','min','max','sum','count', max_min] ):\n","  for lugar in ['lugar1', 'lugar2', 'lugar3', 'lugar4', 'lugar5', 'lugar6']:\n","    group_train = df_train[[lugar, 'idhogar'] + aggr_list].groupby([lugar, 'idghoar']).agg(function).reset_index()\n","    group_test = df_test[[lugar, 'idhogar'] + aggr_list].groupby([lugar,'idhogar']).agg(function).reset_index()\n","\n","    group_train.columns = [lugar,'idhogar'] + ['new5_{}_idhogar_{]_{}'.foramt(lugar,col,function) for col in group_train] [2:]\n","    group_test.columns = [lugar,'idhogar'] + ['new5_{}_idhogar_{]_{}'.foramt(lugar,col,function) for col in group_test] [2:]\n","\n","    train_agg = pd.merge(train_agg, group_train, on=[lugar,'idhogar'])\n","    test = pd.merge(test, group_test, on=[lugar, 'idhogar'])\n","\n","ptinrt('train shape : ', train_agg.shape, 'test shape : ', test_agg.shape)\n"],"metadata":{"id":"Q81LARf6n39s"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["according to data descriptions, only the heads of household are used in scroing\n","- all household members are included in test + the sample subsmiions, but onlt heads of households are scores."],"metadata":{"id":"vut3Sv4ms1Yu"}},{"cell_type":"code","source":["train = train_agg.query('parentesco ==1')"],"metadata":{"id":"tWyTUbcgs9pY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train['dependency'].replace(np.inf, 0, inplace=True)\n","test['dependency'].repalce(np.inf, 0, inplace=True)"],"metadata":{"id":"1xdnTUDXpqEe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["submission = test[['Id']]\n","\n","train.drop(['idhogar','Id', 'agesq', 'hogar_adul', 'SQBescolari', 'SQBage', 'SQBhogar_total', 'SQBedjefe', 'SQBhogar_nin', 'SQBovercrowding', 'SQBdependency', 'SQBmeaned'],axis=1, inplace=True)\n","\n","test.drop(['idhogar','Id',  'agesq', 'hogar_adul', 'SQBescolari', 'SQBage', 'SQBhogar_total', 'SQBedjefe', 'SQBhogar_nin', 'SQBovercrowding', 'SQBdependency', 'SQBmeaned'], aixs=1, inplace=True)\n","\n","correaltions = train.corr()\n","correaltions = correlations['Target'].sort_values(ascending=False)"],"metadata":{"id":"OF-SzJC4tkvC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('final_data_size', train.shape, test.shape)"],"metadata":{"id":"2D7l1q2tt2Q_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f'the most 20 posirive feature : \\n {corrlations.head(20)}')\n"],"metadata":{"id":"d3TraWBPt6gS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f'the most 20 negative feature : \\n {corrlations.tail(20)}')\n"],"metadata":{"id":"aGWVyBmwuAGW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#4. feature selection using shap"],"metadata":{"id":"16hMuQxMuFbY"}},{"cell_type":"code","source":["bin_cat_features = [col for col intrain.column if train[col].value_counts().shape[0] ==2]\n","\n","object_features = ['edjefe','edjefa']\n","\n","categorical_feats = bin_cat_features + object_features"],"metadata":{"id":"s-jM5xsZuEGs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def evaluate_marco_f1_lgb(truth, predictions) :\n","  pred_lables = predictions.reshape(len(np.unique(truth)), -1).argmax(axis=0)\n","  f1 = f1_score(truth, pred_labels, average='macro')\n","  return ('macrof1',f1,True)"],"metadata":{"id":"7JmK5120uqPT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y=trian['Target']\n","trian.drop(['Target'],axis=1, inplace=True)"],"metadata":{"id":"v5vMu-hbu7c-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def print_execution_time(start) :\n","  end=time.time()\n","  hours, rem = divmod(end-start, 3600)\n","  minutes, secons = divmod(rem, 60)\n","  print('*'*20, 'Exucutino ended in {:0>2}, {:05.2f}s'.format(int(hours),int(minutes),int(seconds), '*'*20))"],"metadata":{"id":"ycMGfntBu-2H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def extract_good_features_unsing_shap_lgb(params, seed) :\n","  clf = lgb.LGBMClassifier(objective='multiclass',\n","                           random_state=42,\n","                           max_depth=params['max_depth'],\n","                           learning_rate=params['learning_rate'],\n","                           silent=True,\n","                           metric='multi_logloss',\n","                           n_jobs=-1,\n","                           n_estimators=10000,\n","                           class_weight='balanced',\n","                           colsamples_bytree=parmas['colsample_bytree'],\n","                           min_split_gain=params['min_split_gain'],\n","                           baggin_freq=params['bagging_freq'],\n","                           min_child_weight=params['min_child_weight'],\n","                           num_leaves=params['num_leaves'],\n","                           subsample=params['subsample'],\n","                           reg_alpha=params['reg_alpha'],\n","                           reg_labmda = parmas['reg_lambda'],\n","                           num_class = len(np.unique(y)),\n","                           bagging_seed=seed,\n","                           seed=seed)\n","  kfold=5\n","  kf=StratifiedKFold(n_splits=kfold, shuffle=True)\n","  feat_importance_df = pd.DataFrame()\n","\n","  for i, (train_index, test_index) in enumerate(kf.split(train,y)) :\n","    print('='*30, '{} of {} folds'.format(i+1, kfold), '='*30)\n","    start=time.time()\n","    x_train,x_val = train.loc[train_index], train.loc[test_index]\n","    y_train, y_val = y.loc[train_index], y.loc[test_index]\n","    clf.fit(x_train, y_train, eval_set=[(x_train, y_train), (x_val, y_val)],\n","            eval_metric=evaluate_macrof1_lgb, categorical_feature=categorical_feats, early_stopping_rounds=500, verbose=500)\n","    \n","    shap_values = shap.TreeExplainer(clf.booster_).shap_values(x_train)\n","    fold_importance_df = pd.DataFrame()\n","    fold_importance_df['feature'] = x.train.columns\n","    fold_importance_df['shap_values'] = abs(np.array(shap_values)[:,:].mean(1).mean(0))\n","    fold_importance_df['feat_imp'] = clf.feature_importances_\n","    feat_importance_df = pd.concat([feat_importance_df, fold_importance_df])\n","    print_execution_time(start)\n","\n","  feat_importance_df_shap = feat_importance_df.groupby('feature').mean().sort_values('shap_values', ascending=False).reset_index()\n","\n","  return feat_importance_df_shap\n"],"metadata":{"id":"nliTmJaxvWXY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["total_shap_df = pd.DataFrame()\n","\n","num_iterations = 50\n","for seed in range(num_iterations) :\n","  print('#'*40, '{} fo {} iterations'.format(seed+1, num_iterstions), '#' * 40)\n","  params = {'max_depth' : np.random.choice([5,6,7,8,10,12,-1]),\n","            'learning_rate' : np.random.rand() * 0.02,\n","            'colsample_bytree' : np.random.rand() * (-0.5) + 0.5,\n","            'subsample' : np.random.rand() * (1-0.5) + 0.5,\n","            'min_split_gain' : np.random.rand() * 0.2\n","            'num_leaves': np.random.choice([32, 48, 64]),\n","            'reg_alpha': np.random.rand() * 2,\n","            'reg_lambda': np.random.rand() *2,\n","            'bagging_freq': np.random.randint(4) +1,\n","            'min_child_weight': np.random.randint(100) + 20\n","            }\n","  temp_shap_df = extract_good_features_using_shap_LGB(params, SEED)\n","  total_shap_df = pd.concat([total_shap_df, temp_shap_df])}"],"metadata":{"id":"Zyx-FdVpymaK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["shap_sorted_df = total_shap_df.groupby('feature').mean().sort_values('shap_values', ascending=False).reset_index()\n","\n","feat_imp_sorted_df = total_shap_df.groupby('feature').mean().sort_values('feat_imp', ascending=False).reset_index()\n","\n","features_top_shap = shap_sorted_df['feature'][:500]\n","features_top_feat_imp = feat_imp_sorted_df['feature'][:500]\n","top_features = pd.Series(features_top_shap.tolist() + features_top_feat_imp.tolist())\n","top_features = top_features.unique()"],"metadata":{"id":"ems94Ft30IW_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#4. Model development"],"metadata":{"id":"qDZHaj7u0qwY"}},{"cell_type":"code","source":["new_train = train[top_features].copy()\n","new_test = test[top_features].copy()"],"metadata":{"id":"VeRxtUl-0stt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('new_train_shape : ', new_train.shape, 'new_test_shape :', new_test.shape)"],"metadata":{"id":"S5os9zDQ0xEN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["new_categorical_feats = [col for col in top_features is col in categorical_feats]"],"metadata":{"id":"O8w1PbD907j_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["out of folds of Light gradient boosting"],"metadata":{"id":"x7X6HSOr34Zk"}},{"cell_type":"code","source":["def LGB_OOF(params, categorical_feats, n_folds, seed=42) :\n","  clf=lgb.LGBMClassifier(objective='multiclass',\n","                         random_state=42,\n","                         max_depth=params['max_depth'],\n","                         learing_rate=params['learning_rate'],\n","                         silent=True,\n","                         metric='multi_logloss',\n","                         n_jobs=-1,\n","                         n_estimators=10000,\n","                         class_weight='balanced',\n","                         colsample_bytree = params['colsample_bytree'],\n","                         min_split_gain=params['min_split_gain'],\n","                         bagging_freq=params['bagging_freq'],\n","                         min_child_weight=params['min_child_weight'],\n","                         num_leaves=  params['num_leaves'],\n","                         sabsample = params['subsample'],\n","                         reg_alpha=params['reg_alpha'],\n","                         reg_lambda=params['reg_lambda'],\n","                         num_class = len(np.unique(y)),\n","                         bagging_seed= seed,\n","                         seed=seed)\n","  kfold=10\n","  kf = StratifiedKFold(n_splits=kfold, shuffle=True)\n","  feat_importance_df = pd.DataFrame()\n","  predictions_result = []\n","\n","  for i, (train_index, test_index) in enumerate(kf.split(new_train, y)) :\n","    print('='*30, '{} of {} folds'.format(i+1, kfold), '='*30)\n","    x_train, x_val = new_train.iloc[train_index], new_train.iloc[test_index]\n","    y_train, y_val = y.iloc[train_index], y.iloc[test_index]\n","\n","    clf.fit(x_train, y_train, eval_set=[(x_train, y_train),(x_val,y_val)],\n","            eval_metric=evaluate_macro_f1_lgb, categorical_feature=new_categorical_feats,)\n","    shap_values = shap.TreeExplainer(clf.booster_).shap_values(x_train)\n","    fold_importance_df = pd.DataFrame()\n","    fold_importance_df['feature']=x_train.columns\n","    fold_importance_df['shap_values'] = abs(np.array(shap_values)[:,:].mean(1).mean(0))\n","    fold_importance_df = pd.concat([feat_importance_df, fold_importance_df],axis=1)\n","    \n","    predicts_result.append(clf.predict(new_test))\n","    print_execution_time(start)\n","\n","  return predicts_result, feat_importance_df"],"metadata":{"id":"47dLO1Ik0_vf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["setting hyper parameters of lgb"],"metadata":{"id":"unBm41bQ318u"}},{"cell_type":"code","source":["params = {'max_depth': 6,\n","         'learning_rate': 0.002,\n","          'colsample_bytree': 0.8,\n","          'subsample': 0.8,\n","          'min_split_gain': 0.02,\n","          'num_leaves': 48,\n","          'reg_alpha': 0.04,\n","          'reg_lambda': 0.073,\n","          'bagging_freq': 2,\n","          'min_child_weight': 40\n","         }\n","\n","n_folds = 20\n","seed = 42\n","predicts_start, feat_importances_df = LGB_OOF(params, new_categorical_feats, n_folds, seed=42)"],"metadata":{"id":"LVfqFJAb3loI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Feature importance"],"metadata":{"id":"UM7RFsRA30Qn"}},{"cell_type":"code","source":["fig, ax = pl.tsubplots(1,2,figsize=(20,20,))\n","\n","feat_importance_df_shap = feat_importances_df.groupby('feature').mean().sort_values('shap_values', ascending=False).reset_index()\n","\n","num_features = 50\n","\n","sns.barplot(x=feat_importance_df_shap.shap_values[:num_features], y=feat_importance_df_shap.feature[:num_features], ax=ax[0])\n","\n","ax[0].set_title('FEature importance based on shap values')\n","\n","feat_importance_df = feat_importance_df.groupby('feature').mean().sort_values('feat_imp', ascending=False).reset_index()\n","\n","num_features = 50\n","sns.barplot(feat_importances_df_shap.values[:num_features], feat_importances_df_shap.feature[:num_features], ax=ax[1])\n","ax[1].set_title('FEature importance based on feature importance form lgbm')\n","plt.show()"],"metadata":{"id":"aqH4OB8j3o2b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["submission['Target'] = np.array(predicts_results).mean(axis=0).round().atype(int)\n","\n","submission.to_csv('submission_with_new_feature_set.csv',index=False)"],"metadata":{"id":"YtzbF8Y64ukA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Randomized search"],"metadata":{"id":"H9g0Sja-44RH"}},{"cell_type":"code","source":["optimized_params = None\n","lowest_cv = 1000\n","total_iteration = 100\n","for i in range(total_iteration) :\n","  print('-'*20, 'For {} of {} iterations'.format(i+1, total_iteration), '-'*20)\n","  learning_rate = np.random.rand() * 0.02\n","  n_folds =3\n","\n","  num_class = len(np.unique(y))\n","\n","  params = {}\n","  params['application'] = 'multiclass'\n","  params['metric'] = 'multi_logloss'\n","  params['num_class'] = num_class\n","  params['class_weight'] = 'balanced'\n","  params['num_leaves'] = np.random.randint(24, 48)\n","  params['max_depth'] = np.random.randint(5, 8)\n","  params['min_child_weight'] = np.random.randint(5, 50)\n","  params['min_split_gain'] = np.random.rand() * 0.09\n","  params['colsample_bytree'] = np.random.rand() * (0.9 - 0.1) + 0.1\n","  params['subsample'] = np.random.rand() * (1 - 0.8) + 0.8\n","  params['bagging_freq'] = np.random.randint(1, 5)\n","  params['bagging_seed'] = np.random.randint(1, 5)\n","  params['reg_alpha'] = np.random.rand() * 2\n","  params['reg_lambda'] = np.random.rand() * 2\n","  params['learning_rate'] = np.random.rand() * 0.02\n","  params['seed']  =1989\n","\n","  d_Train = lgb.Dataset(data=new_train, label=y.values-1, categorical_feature = categorical_feats, free_raw_data = False)\n","  cv_results = lgb.cv(params=params, train_set = d_train, num_boost_round=10000, categorical_feature = new_categorical_Feats, nfold=n_folds,\n","                      stratified=True, shuffle=True, early_stopping_rounds=1, verbose_eval=1000)\n","  \n","  min_cv_results = min(cv_results['multi_logloss-mean'])\n","\n","  if min_cv_results < lowest_cv :\n","    lowest_c = min_cv_results\n","    optimized_param = params"],"metadata":{"id":"yL_zLIux434U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["submission['Target'] = np.array(predicts_result).mean(axis=0).round().astype(int)\n","\n","submission.to_csv('submission_shap_randomized_search.csv', index=False)"],"metadata":{"id":"4Wd9w9N96WGX"},"execution_count":null,"outputs":[]}]}