{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNkR5tTvGr+OyChHuZW6KNP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["LGBM with random split for early stopping\n","\n","- the lightGBM model hav ebeen replaced with xgboost and the code has been updated accordingly.\n","- i am also firring VotingClassifiers of RandomForests and ensembling the results of the XGBs with the RFs.\n","- some additional features have been added.\n","- some features which were previously dropped have been retained.\n","- some of the code has been reorganized.\n","- Rather than splitting the data once and using the validation data for the LGBM earyl stopping, I split the dat aduring the training so the entire training set can be trained on."],"metadata":{"id":"tXK0ppM9Xvt0"}},{"cell_type":"markdown","source":["in this kernel :\n","\n","1. training on the heads of households only\n","2. balance class frequencies\n","3. this kernel uses macro F1 score to early stopping in training\n","4. OHE if reversed into label encoding, as it is easier to digest for a tree model.\n","5. idhogar is NOT used in training.\n","6. There are aggregations done within households and new features are hand-crafted\n","7. A voting classifier is used to average over several LGBM models\n"],"metadata":{"id":"Bc_q0r13Yo0d"}},{"cell_type":"code","source":[],"metadata":{"id":"tvPp488uPmpd"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":5,"metadata":{"id":"zYjchJVvWv7e","executionInfo":{"status":"ok","timestamp":1669253409516,"user_tz":-540,"elapsed":521,"user":{"displayName":"김시윤","userId":"10463797081079810917"}}},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","plt.style.use('fivethirtyeight')\n","\n","import lightgbm as lgb\n","import xgboost as xgb\n","from sklearn.metrics import f1_score\n","import joblib\n","from joblib import Parallel, delayed\n","from sklearn.base import clone\n","from sklearn.ensemble import VotingClassifier, ExtraTreesClassifier, RandomForestClassifier\n","from sklearn. utils import class_weight\n","#if 'balanced', class weights will be given by n_smaples / (n_classes * np,bincount(y))\n","\n","\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","source":["from sklearn.preprocessing import LabelEncoder\n","\n","#this only transofrms the idhogar filed, the other things this functino used to do are done elsewhere\n","def encode_data(df) :\n","  df['idhogar'] = LabelEncoder().fit_transofrm(df['idhogar'])\n","\n","\n","#plot feature importance for sklearn decision trees\n","def feature_importance(forest, x_train, display_results = True) :\n","  ranked_list = []\n","  zero_features = []\n","\n","  importances = forest.feature_importances_\n","\n","  indices = np.argsort(importances)[::-1]\n","\n","  if display_results :\n","    #print the feature ranking\n","    print('FEature ranking')\n","\n","    for f in range(x_train.shape[1]) :\n","      if display_results :\n","        print('%d. feature %d (%f)' % (f+1, indices[f],importances[indices[f]] + \" _ \" + x_train.columns[indices[f]]))\n","        ranked_list.append( x_train.columns[indices[f]] )\n","\n","        if importances[indices[f]] == 0.0 :\n","          zero_features.append(x_train.columns[indices[f]])\n","\n","  return ranked_list, zero_features\n","  "],"metadata":{"id":"SngqqMQBZxPP","executionInfo":{"status":"ok","timestamp":1669253696812,"user_tz":-540,"elapsed":357,"user":{"displayName":"김시윤","userId":"10463797081079810917"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["def do_features(df) :\n","  feats_div = [('children_fraction', 'r4t1', 'r4t3'), \n","                 ('working_man_fraction', 'r4h2', 'r4t3'),\n","                 ('all_man_fraction', 'r4h3', 'r4t3'),\n","                 ('human_density', 'tamviv', 'rooms'),\n","                 ('human_bed_density', 'tamviv', 'bedrooms'),\n","                 ('rent_per_person', 'v2a1', 'r4t3'),\n","                 ('rent_per_room', 'v2a1', 'rooms'),\n","                 ('mobile_density', 'qmobilephone', 'r4t3'),\n","                 ('tablet_density', 'v18q1', 'r4t3'),\n","                 ('mobile_adult_density', 'qmobilephone', 'r4t2'),\n","                 ('tablet_adult_density', 'v18q1', 'r4t2'),\n","                ]\n","  feats_sub = [('people_not_living', 'tamhog', 'tamviv'),\n","                 ('people_weird_stat', 'tamhog', 'r4t3')]\n","\n","  for f_new, f1, f2 in feats_div :\n","    df['fe_' + f_new] = (df[f1] / df[f2]).astype(np.float32)\n","\n","  for f_new, f1, f2 in feats_sub :\n","    df['fe_' + f_new] = (df[f1] / df[f2]).astype(np.float32)\n","\n","\n","    #aggregation rules over hosehold\n","    aggs_num = {'age' : ['min','max','mean'],\n","                'escolari' : ['min','max','mean']}\n","\n","    aggs_cat = {'dis' : ['mean']}\n","  \n","  for s_ in ['estadicivil','parentesco','instlevel'] :\n","    for f_ in [f_ for f_ in df.columns if f_.startswith(s_)] :\n","      aggs_cat[f_] = ['mean','count']\n","\n","  \n","  #aggregation over household\n","  for name_, df_ in [('18', df.query('age >= 18'))] :\n","    df_agg = df_.groupby('idhogar').agg({**aggs_num, **aggs_cat}).astype(np.float32)\n","\n","    df_agg.columns = pd.Index(['agg' + name_ + '_' + e[0] + '_' + e[1].upper() for e in df_agg.columns.tolist()])\n","\n","    df= df.join(df_agg, how='left', on='idhogar')\n","    del df_agg\n","\n","  #drop id's\n","  df.drop(['Id'], axis=1, inplace=True)\n","\n","  return df"],"metadata":{"id":"yCZAEZ7_bZDr","executionInfo":{"status":"ok","timestamp":1669253807106,"user_tz":-540,"elapsed":328,"user":{"displayName":"김시윤","userId":"10463797081079810917"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["def conver_OHE2Le(df) :\n","  tmp_df = df.copy(deep=True)\n","  for s_ in ['pared','piso','techo','abastagua','sanitario','energcocinar','elimbase','epared','etecho','eviv','estadicivil','parentesco','instlevle','lugar','tipovivi','manual_elec'] :\n","    if'manual_' not in s_:\n","      cols_s_ = [f_ for f_ in df.columns if f_.startswith(s_)]\n","    elif 'elec' in s_ :\n","      cols_s_ = ['public','planpri','noelec','coopele']\n","      sum_ohe = tmp_df[cols_s_].sum(axis=1).unique()\n","      #deal with those ohe, where there is a sum over columns ==0\n","\n","    \n","    if 0 in sum_ohe :\n","      print('the ohe in {} is incomplete. a new column will be added before label encoding'.format(s_))\n","\n","      #dummy column name to be added\n","      col_dummy = s_ + '_dummy'\n","      #add the column to the dataframe\n","      tmp_df[col_dummy] = (tmp_df[cols_s_].sum(axis=1) ==0).astype(np.int8)\n","      #add the name to the list of columns to be label-encoded\n","      cols_s_.append(col_dummy)\n","\n","      # proof-check, that now the category is complete\n","      sum_ohe = tmp_df[cols_s_].sum(axis=1).unique()\n","      if -0 in sum_ohe :\n","        print('The category completeion did not work')\n","\n","    tmp_cat = tmp_df[cols_s_].idxmax(axis=1)\n","    tmp_df[s_ + '_LE'] = LabelEncoder().fit_transform(tmp_cat).astype(np.int16)\n","    if 'parentesco1' in cols_s_ :\n","      cols_s_.remove('parentesco1')\n","      tmp_df.drop(cols_s_, axis=1, inplace=True)\n","  return tmp_df"],"metadata":{"id":"xDDiYwFzeYHs","executionInfo":{"status":"ok","timestamp":1669253814270,"user_tz":-540,"elapsed":431,"user":{"displayName":"김시윤","userId":"10463797081079810917"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["Read in the dat aand clean it up"],"metadata":{"id":"VZwgSHQ8g6SU"}},{"cell_type":"code","source":["train=pd.read_csv('../input/train.csv')\n","test=pd.read_csv('../input/test.csv')\n","\n","test_ids = test.Id"],"metadata":{"id":"tTP6SxbYgr0w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def process_df(df_):\n","  #encode the idhogar\n","  encode_data(df_)\n","\n","  #create aggregate features\n","  return do_features(df_)\n","\n","train=process_df(train)\n","test=process_df(test)"],"metadata":{"id":"nIZNzD90hBSb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["clean up some missing data and convert objects to numeric."],"metadata":{"id":"Icogt78HhLo8"}},{"cell_type":"code","source":["#some dependencies are na, fill those with the square root of the square\n","\n","\n","train['dependency'] =np.sqrt(train['SQBdependency'])\n","test['dependency'] =np.sqrt(test['SQBdependency'])\n","\n","#fill 'no's for education with 0s\n","\n","train.loc[train['edjefa'] == 'no', 'edjefa'] =0\n","train.loc[train['edjefe'] == 'no', 'edjefe'] =0\n","test.loc[test['edjefa'] == 'no', 'edjefa'] =0\n","test.loc[test['edjefa'] == 'no', 'edjefa'] =0\n","\n","#if eduatino is 'yes' and person is head of households, fill with escolari\n","\n","train.loc[(train['edjefa'] == 'yes') & (train['parnetesci1'] ==1), 'edjefa'] = train.loc[(train['edjefa']=='yes') & (train.loc['parentesco1'] ==1), 'escolari']\n","train.loc[(train['edjefe'] == 'yes') & (train['parnetesci1'] ==1), 'edjefe'] = train.loc[(train['edjefe']=='yes') & (train.loc['parentesco1'] ==1), 'escolari']\n","test.loc[(test['edjefa'] == 'yes') & (test['parnetesci1'] ==1), 'edjefa'] = test.loc[(test['edjefa']=='yes') & (test.loc['parentesco1'] ==1), 'escolari']\n","test.loc[(test['edjefe'] == 'yes') & (test['parnetesci1'] ==1), 'edjefe'] = test.loc[(test['edjefe']=='yes') & (test.loc['parentesco1'] ==1), 'escolari']\n","\n","\n","#convert to int for our models\n","train['edjefe'] = train['edjefe'].astype('int')\n","train['edjefa'] = train['edjefa'].astype('int')\n","test['edjefe'] = test['edjefe'].astype('int')\n","test['edjefa'] = test['edjefa'].astype('int')\n","\n","#create feature with max education of either head of household\n","train['edjef'] = np.max(train[['edjefe','edjefa']], axis=1)\n","test['edjef'] = np.max(test[['edjefe','edjefa']], axis=1)\n","\n","#fill some nas\n","train['v2a1'] = train['v2a1'].fillna(0)\n","test['v2a1'] =test['v2a1'].fillna(0)\n","\n","train['v18q1'] = train['v18q1'].fillna(0)\n","test['v18q1'] =test['v18q1'].fillna(0)\n","\n","train['rez_esc'] = train['rez_esc'].fillna(0)\n","test['rez_esc'] = test['rez_esc'].fillna(0)\n","\n","train.loc[train.meaneduc.isnul(), 'meaneduc'] =0\n","test.loc[test.meaneduc.isnul(), 'meaneduc'] =0\n","\n","train.loc[train['SQBmeaned'.isnull(), 'SQBmeaned']] = 0\n","test.loc[test['SQBmeaned'.isnull(), 'SQBmeaned']] = 0\n","\n","#fix some inconssistencies int he data\n","train.loc[(train.v14a ==  1) & (train.sanitario1 ==  1) & (train.abastaguano == 0), \"v14a\"] = 0\n","train.loc[(train.v14a ==  1) & (train.sanitario1 ==  1) & (train.abastaguano == 0), \"sanitario1\"] = 0\n","\n","test.loc[(test.v14a ==  1) & (test.sanitario1 ==  1) & (test.abastaguano == 0), \"v14a\"] = 0\n","test.loc[(test.v14a ==  1) & (test.sanitario1 ==  1) & (test.abastaguano == 0), \"sanitario1\"] = 0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":245},"id":"UIjSEmYphKfg","executionInfo":{"status":"error","timestamp":1669253865976,"user_tz":-540,"elapsed":415,"user":{"displayName":"김시윤","userId":"10463797081079810917"}},"outputId":"b3f60347-e60f-4227-c1d7-75b21a8c66de"},"execution_count":14,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-a3509ad14ec1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dependency'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'SQBdependency'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dependency'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'SQBdependency'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"]}]},{"cell_type":"code","source":["def train_test_apply_func(train_, test_, func_) :\n","  test_['Target'] = 0\n","  xx = pd.concat([train_, test_])\n","\n","  xx_func = func_(xx)\n","  train_ = xx.func.iloc[: train_.shape[0], :]\n","  test_ = xx.func.iloc[: train_.shape[0], :].drop('Target', axis=1)\n","\n","  del xx, xx_func\n","  return train_, test_"],"metadata":{"id":"CIt1wjF9kHjK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#convert the one hot fields into label encoded\n","\n","train, test = train_test_apply_func(train, test, convert_OHE2LE)"],"metadata":{"id":"1_lt9_EukbXJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Geo aggregates"],"metadata":{"id":"iVfsuFUlktc_"}},{"cell_type":"code","source":["cols_2_ohe = ['eviv_Le', 'etecho_Le','epared_LE','elimbasu_LE', 'energcocinar_Le','sanirario_LE','manual_elec_Le', 'pared_LE']\n","cols_nums = ['age','meaneduc','dependency','hogar_nin','hogar_adul' 'hogar_mayor','hogar_total','bedrooms','overcrowding']\n","\n","def convert_geo2aggs(df_) :\n","  tmp_df = pd.concat([df_[(['lugar_Le','idhogar'] + cols_nums)], pd.get_dummies(df_[cols_2_ohe],columns=cols_2_ohe)], axis=1)\n","\n","  geo_agg = tmp_df.groupby(['lugar_LE','idhogar']).mean().groupby('lugar_LE').mean().astype(np.float32)\n","  get_agg.columns = pd.Index(['geo_' + e for e in geo.agg.columns.tolist()])\n","\n","  del tmp_df\n","  return df_.join(geo_agg, how='left', on='lugar_LE')\n","\n","#add some aggregates by geography\n","train, test = train_test_apply_func(train, test, convert_geo2aggs)\n"],"metadata":{"id":"oKkSvKN2knR4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#add the number of people over 18 in each household\n","\n","train['num_over_18'] = 0\n","train['num_over_18'] = train[train.age >= 18].groupby('idhogar').transform(\"count\")\n","train['num_over_18'] = train.groupby(\"idhogar\")[\"num_over_18\"].transform(\"max\")\n","train['num_over_18'] = train['num_over_18'].fillna(0)\n","\n","test['num_over_18'] = 0\n","test['num_over_18'] = test[test.age >= 18].groupby('idhogar').transform(\"count\")\n","test['num_over_18'] = test.groupby(\"idhogar\")[\"num_over_18\"].transform(\"max\")\n","test['num_over_18'] = test['num_over_18'].fillna(0)\n","\n","#add some extra features, these were taken from another kernel\n","\n","def exract_feature(df) :\n","  df['bedrooms_to_rooms'] = df['bedroom'] / df['rooms']\n","  ..."],"metadata":{"id":"vkR9RoukbuIn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#drop duplicated columns\n","needless_cols = ['r4t3','tamhog','tamviv','hhsize','v18q','v14a','agesq','mobilephone','female']\n","\n","instlevel_cols = [s for s in train.columns.tolist() if 'instlevel' in s]\n","\n","needless_cols.extend(instlevel_cols)\n","\n","train=train.drop(needless_cols, axis=1)\n","test=test.drop(needless_cols, axis=1)"],"metadata":{"id":"uB3cqPa2SeyH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Split data\n","\n","we split the data by household to avoid leakage, since rows belonging to the same houshold usually have the same target. SInce we filter the data to only include heads of household this isn't technically necessary, but it provedes an easy way to use the netire training data sey if we want to do that."],"metadata":{"id":"Z8L-WD5jS25q"}},{"cell_type":"code","source":["def split_data(train,y,sample_weigh=None, households=None,test_percentage=0.2,seed=None) :\n","  train2=train.copy()\n","\n","  #pick some random households to use for the test data\n","  cv_hhs = np.random.choice(households, size=int(len(households) * test_percentage), replace=False)\n","\n","  #select households which are in the random selection\n","  cv_idx = np.isin(households, cv_hhs)\n","  x_test = train2[cv_idx]\n","  y_test = y[cv_idx]\n","\n","  x_train=train2[~cv_idx]\n","  y_train=y[~cv_idx]\n","\n","  if sample_weight is not None :\n","    y_train_weights = sample_weight[~cv_idx]\n","    return x_train, y_train, x_test, y_test, y_train_weights\n","\n","  \n","  return x_train, y_train,x_test,y_test\n"],"metadata":{"id":"vJWVMBabTHH-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x = train.query('parnetesco1==1')\n","# x=train.copy()\n","\n","\n","#pull out and drop the target variable\n","\n","y=x['Target']-1\n","x=x.drop(['Target'],axis=1)\n","\n","np.random.seed(seed=None)\n","\n","train2=x.copy()\n","\n","train_hhs=train2.idhogar\n","\n","households = train2.idhogar.unique()\n","\n","cv_hhs = np.random.choice(households, size=int(len(households) * 0.15), replace=False)\n","\n","cv_idx = np.isin(train2.idhogar, cv_hhs)\n","\n","\n","x_test = train2[cv_idx]\n","y_test = y[cv_idx]\n","\n","x_train = train2[~cv_idx]\n","y_train = y[~cv_idx]\n","\n","#train on entire dataset\n","x_train = train2\n","y_train = y\n","\n","train_households=  x_train.idhogar"],"metadata":{"id":"ksJmJnDdT1XR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#figure out the class weights for training with unbalanced classes\n","y_train_weights = class_weight.compute_sample_weight('balanced',y_train, indices=None)"],"metadata":{"id":"M3Dy7FOpUfQX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#drop some feature which aren't used by the LGBM or have very low importance\n","extra_drop_features = [\n"," 'agg18_estadocivil1_MEAN',\n"," 'agg18_estadocivil6_COUNT',\n"," 'agg18_estadocivil7_COUNT',\n"," 'agg18_parentesco10_COUNT',\n"," 'agg18_parentesco11_COUNT',\n"," 'agg18_parentesco12_COUNT',\n"," 'agg18_parentesco1_COUNT',\n"," 'agg18_parentesco2_COUNT',\n"," 'agg18_parentesco3_COUNT',\n"," 'agg18_parentesco4_COUNT',\n"," 'agg18_parentesco5_COUNT',\n"," 'agg18_parentesco6_COUNT',\n"," 'agg18_parentesco7_COUNT',\n"," 'agg18_parentesco8_COUNT',\n"," 'agg18_parentesco9_COUNT',\n"," 'geo_elimbasu_LE_4',\n"," 'geo_energcocinar_LE_1',\n"," 'geo_energcocinar_LE_2',\n"," 'geo_epared_LE_0',\n"," 'geo_hogar_mayor',\n"," 'geo_manual_elec_LE_2',\n"," 'geo_pared_LE_3',\n"," 'geo_pared_LE_4',\n"," 'geo_pared_LE_5',\n"," 'geo_pared_LE_6',\n"," 'num_over_18',\n"," 'parentesco_LE',\n"," 'rez_esc']"],"metadata":{"id":"UfaCSqIFS184"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["xgb_drop_cols = extra_drop_features + ['idhogar','parentesco1']"],"metadata":{"id":"jVloiMxQVKyr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Fit a voting classifier\n","\n","Define a derived VotingClassifier class to be able to pass fit_params for early stopping. Vote based on LGBM models with early stopping based on macro f1 and decaying learning rate."],"metadata":{"id":"S1EGrnzmVOtT"}},{"cell_type":"code","source":["#if you need to set optimal paramters, use automl like lazypredict.\n","\n","opt_parameters1 = {'max_depth' : 35,\n","                  'eta' : 0.1,\n","                  'silent' : 0,\n","                  'objective' : 'multi:softmax',\n","                  'min_child_weight' : 1,\n","                  'num_class' :'4',\n","                  'gamma' : 2, 'colsample_bylevel' : 0.9, 'subsample' :0.84, 'colsample_bytree' : 0.88, 'reg_labda':0.4}\n","\n","opt_parameters2 = {'max_depth' : 35,\n","                  'eta' : 0.1,\n","                  'silent' : 0,\n","                  'objective' : 'multi:softmax',\n","                  'min_child_weight' : 1,\n","                  'num_class' :'4',\n","                  'gamma' : 2, 'colsample_bylevel' : 0.9, 'subsample' :0.84, 'colsample_bytree' : 0.88, 'reg_labda':0.4}\n","\n","\n","def evaluate_macroF1_lgb(predictions, truth) :\n","  pred_labels = predictions.argmax(axis=1)\n","  truth=truth.get_label()\n","  f1 = f1_score(truth, pred_labels, average='macro')\n","  return ('macroF1',1-f1)\n","\n","fit_params = {'early_stopping_rounds' : 500, 'eval_metric' : evaluate_macroF1_lgb,\n","              'eval_set' : [(x_train,y_train), (x_test,y_test)],\n","              'verbose':500}\n","\n","def learning_rate_power_0997(current_iter) :\n","  base_learning_rate = 0.1\n","  min_sampling_rate = 0.02\n","  lr = base_learning_rate * np.power(.995, current_iter)\n","  return max(lr, min_sampling_rate)\n","\n","fit_params['verbose'] = 50"],"metadata":{"id":"K-m_DglhVYjr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["np.random.seed(100)\n","\n","def _parallel_fit_estimator(estimator1, x, y, sample_weight = None, threshold=True, **fit_params) :\n","  estimator = clone(estimator1)\n","\n","  #randomly split the data so we have a test set for early stopping\n","  if sample_weight is not None :\n","    x_train,y_train,x_test,y_test,y_train_weight = split_data(x,y,sample_weight, households= train_households)\n","\n","  else :\n","    x_train,y_train,x_test,y_test = split_data(x,y,None,households=train_households)\n","\n","    #update the fit params with our new split\n","    fit_params['eval_set'] = [(x_test,y_test)]\n","\n","    #fir the estimator\n","  if sample_weight is not None :\n","    if isinstance(estimator1, ExtraTreesClassifier) or isinstance(estimator1, RandomForestClassifier) :\n","      estimator.fit(x_train,y_train)\n","    else :\n","       _ = estimator.fit(x_train, y_train, sample_weight=y_train_weight, **fit_params)\n","\n","  else :\n","    if isinstance(estimator1, ExtraTreesClassifier) or isinstance(estimator1, RandomForestClassifier) :\n","      estimator.fit(x_train,y_train)\n","    else :\n","      _ = estimator.fit(x_train,y_train, **fit_params)\n","    \n","  if not isinstance(estimator1, ExtraTreesClassifier) and not isinstance(estimator1, RandomForestClassifier) and not isinstance(estimator1, xgb.XGBClassiifer) :\n","    best_cv_round = np.argma(estimator.evals_result_['validation_0']['mlogloss'])\n","    best_cv = np.max(estimator.evals_result_['validation_0']['mlogloss'])\n","    best_train = estimator.evals_result_['train']['macroF1']['best_cv_round']\n","  else :\n","    best_train = f1_score(y_train, estimator.predict(x_train), average='macro')\n","    best_cv = f1_score(y_test, estimator.predict(x_test), average='macro')\n","    print('train f1 :', best_train)\n","    print('test f1 :', best_cv)\n","\n","  #reject some estimators based on their performance on train and test sets\n","  if threshold :\n","    #if the vlaid score is very high, we'll allow a little more leeway with the train scroes\n","    if ((best_cv > 0.37) & (best_train > 0.75)) or ((best_cv>0.44) & (best_train > 0.65)) :\n","      return estimator\n","\n","    #else recurse until we get a better one\n","    else :\n","      print('unacceptable. try again')\n","      return _parallel_fit_estimator(estimaotr1, x,y,sample_weight=sample_wiegh, **fit_params)\n","  else :\n","    return estimator\n","\n"],"metadata":{"id":"PhFeZO8RWzD7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["make votingclassifier with LGBM"],"metadata":{"id":"TMUgkkyUZkZM"}},{"cell_type":"code","source":["class VotingClassifierLGBM(VotingClassifier) :\n","  '''this implements the fit method of the VotingClassifier propagating fit_params'''\n","  def fit(self, x,y,sample_weight=None, threshold=True, **fit_params) :\n","    if isinstance(y,np.ndarray) and len(y.shape) > 1 and y.shape[1] > 1 :\n","      raise NotImplementedError('Multilabel and multi-output' 'classification is not supported')\n","    \n","    if self.voting not in ('soft','hard') :\n","      raise ValueError(\"voting must be 'soft' or 'hard' ; got(voting=%r)\" % self.voting)\n","    \n","    if self.estimators is None or len(self.estimators) == 0 :\n","      raise AttributeError(\"Invalid estimators attribute, 'estimators'\")\n","\n","    names,clfs = zip(*self.estimators)\n","    self._validate_names(names)\n","\n","    n_isnone = np.sum([clf is None for _, clf in self.estimators])\n","    if n_isnone == len(self.estimators) :\n","      raise ValueError(\"all estimators are none. at least one is\"'required to be a classifier')\n","    \n","    self.le_ = LabelEncoder().fit(y)\n","    self.classes_ = self.le_.classes_\n","    self.estimators_ = []\n","\n","    transformed_y = self.le_.transform(y)\n","\n","    self.estimators_ = Parallel(n_job=self.n_jobs)(delayed(_parallel_fit_estimator)(clone(clf),x,transformed_y,sample_weight, threshold=threshold, **fit_params)\n","    for clf in clfs if clf is not None)\n","\n","    return self"],"metadata":{"id":"SnlSMjLsZiNC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["clfs = []\n","for i in range(15) :\n","  clf = xgb.XGBClassifer(random_state=42, n_estimators=300, learning_rate =0.15, n_jobs=-1, **opt_parameters)\n","\n","  clfs.append(('xgb{}'.format(i), clf))\n","\n","vc = VotingClassifierLGBM(clfs, voting='soft')\n","del(clfs)\n","\n","#train the final model with leaning rate decay\n","\n","_ = vc.fit(x_train.drop(xgb_drop_cols, axis=1), y_train, sample_weight=y_train_weights, threshold=False, **fit_params)\n","\n","clf_final = vc.estimators_[0]"],"metadata":{"id":"u3nsUWeTbMs_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#params 4 - 400 early stop and 15 est, l1 used features-weighted\n","\n","global_score = f1_score(y_test, clf_final.predict(x_test.drop(xgb_drop_cols, axis=1)), average='macro')\n","vc.voting='soft'\n","global_score_soft = f1_score(y_test, vc.predict(x_test.drop(xgb_drop_cols, axis=1)), average='macro')\n","\n","vc.voting='hard'\n","global_score_hard = f1_score(y_test, vc.predict(x_test.drop(xgb_drop_cols, axis=1)), average='macro')\n","\n","print('validation score of a single LGBM classiifer : {:.4f}'.format(global_score))\n","\n","print('validation score of a votingclassifier on 3 lgbms with soft voring strategy : {:.4f}'.format(global_score_soft))\n","\n","print('validation score of a votingclassifier on 3 lgbms with hard voring strategy : {:.4f}'.format(global_score_hard))\n","\n"],"metadata":{"id":"ZF24MO0ecFl1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#see which features are not used by ANY models\n","useless_features = []\n","drop_features = set()\n","counter=0\n","\n","for est in vc.estimators_ :\n","  ranked_features, unused_features = feature_importance(est, x_train.drop(xgb_drop_cols, axis=1), display_results = False)\n","  useless_features.append(unused_features)\n","  if counter==0 :\n","    drop_features = set(unused_features)\n","  else :\n","    drop_features = drop_features.intersection(set(unused_features))\n","    counter+=1\n","\n","drop_features\n"],"metadata":{"id":"7hh658IKc6WZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ranked_features = feature_importance(clf_final, x_train.drop(xgb_drop_cols, axis=1))"],"metadata":{"id":"gDQq3ByFddJy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["RF"],"metadata":{"id":"MySboZGRdk8Y"}},{"cell_type":"code","source":["#result cols from ranked_Features\n","et_drop_cols = ['agg18_age_MAX', 'agg18_age_MEAN', 'agg18_age_MIN', 'agg18_dis_MEAN',\n","       'agg18_escolari_MAX', 'agg18_escolari_MEAN', 'agg18_escolari_MIN',\n","       'agg18_estadocivil1_COUNT', 'agg18_estadocivil1_MEAN',\n","       'agg18_estadocivil2_COUNT', 'agg18_estadocivil2_MEAN',\n","       'agg18_estadocivil3_COUNT', 'agg18_estadocivil3_MEAN',\n","       'agg18_estadocivil4_COUNT', 'agg18_estadocivil4_MEAN',\n","       'agg18_estadocivil5_COUNT', 'agg18_estadocivil5_MEAN',\n","       'agg18_estadocivil6_COUNT', 'agg18_estadocivil6_MEAN',\n","       'agg18_estadocivil7_COUNT', 'agg18_estadocivil7_MEAN',\n","       'agg18_parentesco10_COUNT', 'agg18_parentesco10_MEAN',\n","       'agg18_parentesco11_COUNT', 'agg18_parentesco11_MEAN',\n","       'agg18_parentesco12_COUNT', 'agg18_parentesco12_MEAN',\n","       'agg18_parentesco1_COUNT', 'agg18_parentesco1_MEAN',\n","       'agg18_parentesco2_COUNT', 'agg18_parentesco2_MEAN',\n","       'agg18_parentesco3_COUNT', 'agg18_parentesco3_MEAN',\n","       'agg18_parentesco4_COUNT', 'agg18_parentesco4_MEAN',\n","       'agg18_parentesco5_COUNT', 'agg18_parentesco5_MEAN',\n","       'agg18_parentesco6_COUNT', 'agg18_parentesco6_MEAN',\n","       'agg18_parentesco7_COUNT', 'agg18_parentesco7_MEAN',\n","       'agg18_parentesco8_COUNT', 'agg18_parentesco8_MEAN',\n","       'agg18_parentesco9_COUNT', 'agg18_parentesco9_MEAN']\n","\n","et_drop_cols.extend(['idhogar','parentesco1','fe_rent_per_person','fe_rent_per_room','fe_tablet_adult_density','fe_tablet_density'])"],"metadata":{"id":"OccFLWjHdjWn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#so the same thing for some extra trees classifiers\n","\n","ets = []\n","\n","for i in range(10) :\n","  rf = RandomForestClassifier(max_depth=None, random_state=42, n_jobs=-1, n_estimators=700, min_impurity)decrease=1e-3, min_samples_leaf = 2, verbose=0, class_weight='balanced')\n","  ets.append('rf{}'.format(i),rf)\n","\n","vc2 = VotingClassifier(ets, voing='soft')\n","_ = vc2.fit(x_train.drop(et_drop_cols, axis=1), y_train, threshold=False)"],"metadata":{"id":"Pp3Dsl6Gd9Sf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# w/ threshold, extra drop cols\n","vc2.voting = 'soft'\n","\n","global_rf_score_soft= f1_score(y_test, vc2.predict(x_test.drop(et_drop_cols, axis=1)), average='macro')\n","\n","vc2.votinh='hard'\n","global_rf_score_hard= f1_score(y_test, vc2.predict(x_test.drop(et_drop_cols, axis=1)), average='macro')\n","\n","print('validation score of a votingclassifier on 3 lgbms with soft voting strategy : {:.4f}'.format(global_rf_score_soft))\n","print('validation score of a votingclassifier on 3 lgbms with hard voting strategy : {:.4f}'.format(global_rf_score_hard))"],"metadata":{"id":"raGHF-jAecpj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#see which features are not used by any models\n","\n","\n","useless_features = []\n","drop_features = set()\n","counter = 0\n","\n","for est in vc2.estimators_ :\n","  ranked_features, unused_features = feature_importance(est, x_train.drop(et_drop_cols, axis=1), display_results = False)\n","  useless_features.append(unused_features)\n","\n","  if counter == 0 :\n","    drop_features = set(unused_features)\n","  else :\n","    drop_features = drop_features.intersection(set(unused_features))\n","    counter +=1\n","\n","drop_features_vc2"],"metadata":{"id":"votPW87Qe3se"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def combine_voters(data, weights=[0.5,0.5]) :\n","  #do soft voting with both classifiers\n","  vc.voting = 'soft'\n","  vc1_probs = vc.predict_proba(data.drop(xgb_drop_dols, axis=1))\n","  vc2.voting='soft'\n","  vc2_probs = vc2.predict_proba(data.drop(xgb_drop_cols, axis=1))\n","\n","  final_vote = (vc1_probs * weights[0] + (vc2_probs * weights[1]))\n","  predictions = np.argmax(final_vote, axis=1)\n","\n","  return predictions"],"metadata":{"id":"SIqXVYaZhMfg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["combo_preds = combine_voters(x_test, weights=[0.5,0.5])\n","\n","global_combo_score_soft = f1_score(y_test, combo_preds, average='macro')\n","global_combo_socre_soft"],"metadata":{"id":"Ar5AnlWKhm2I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#if change weights :\n","combo_preds = combine_voters(x_test, weights=[0.5,0.5])\n","\n","global_combo_score_soft = f1_score(y_test, combo_preds, average='macro')\n","global_combo_socre_soft"],"metadata":{"id":"ueLTGwC6hvF9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Prepare submission"],"metadata":{"id":"QIOZBYb5h2vu"}},{"cell_type":"code","source":["y_sub = pd.DataFrame()\n","y_sub['Id'] = test.ids"],"metadata":{"id":"ve38taBNh4AT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vc.voting='soft'\n","\n","y_sub_lgb = y_sum.copy(deep=True)\n","y_sub_lgb['Target'] = vc.predict(test.drop(xgb_drop_cols, axis=1)) +1\n","\n","vc2.voting='soft'\n","y_sub_lgb = y_sub.copy(deep=True)\n","y_sub_rf['Target'] = vc2.predict(test.drop(et_drop_cols, axis=1))+1\n","\n","y_sub_ens = y_sub.copy(deep=True)\n","y_sub_ens['Target'] = combine_voters(test)+1"],"metadata":{"id":"OObMnZAj40DW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from datetime import datetime\n","now = datetime.now()\n","\n","sub_file_lgb = 'submission_soft_xgb_{:.4f}_{}.csv'.format(gloabal_score_soft, str(now.strftime('%Y-%M-%d-%h-%m')))\n","\n","sub_file_rf = 'submission_soft_rf_{:4.f}_{}.csv'.format(global_rf_score_soft, str(now.strftime('%y-%m-%d-%h-%m')))\n","\n","sub_file_ens = 'submission_ens_{:.4f}_{}.csv'format(global_rf_score_soft, str(now.strftime('%y-%m-%d-%h-%m')))\n","\n","y_sub_lgb.to_csv(sub_file_lgb, index=False)\n","y_sub_rf.to_csv(sub_file_lgb, index=False)\n","y_sub_ens.to_csv(sub_file_lgb, index=False)\n"],"metadata":{"id":"huny0mvJ5P5P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"2XGachaa59uC"},"execution_count":null,"outputs":[]}]}