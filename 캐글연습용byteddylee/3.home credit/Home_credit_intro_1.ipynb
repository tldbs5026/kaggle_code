{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPB7wk/xsoE/ZTgLfoRhqvL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#Introdiction\n","\n","The objective of this competion is to use historical loadn application data to predict whether or not an applicatn will be ale o replay a loan. This is a standard supervised classification task :\n","\n","- Supervised : The labels ar eincluded in the training data and teh goal is to train a  model to learn to predict the labels fro the features.\n","- Classification : The label is a binary viriable, 0(will repay loan on time) or 1(will have difficulty repaying loan)\n","\n"],"metadata":{"id":"vo3Os2ICNBHd"}},{"cell_type":"markdown","source":["#Data\n","\n","- application_train/application_test : identified by the feature ***SK_Id_CURR***.\n"," The training application data comes with the target indicating 0  the loan was repaid or 1 : the loan was not repaid.\n","\n"," - bureau : client's previous credits from other financial institutions.\n","\n"," - bureau_balance : monthly data about the previous credits in bureau.\n","\n"," - previous_application : previous applications for loans at home credit of clients who have loans in the application data.\n","\n"," - POS_CASH_BALACE : monthly data about previous point of sale or cash loans\n","\n"," - credit_card_balance : monthly data about previous credit cards clients have had with home credit.\n","\n"," - installments_payment : payment history for previous loans at home credit.\n","\n"," "],"metadata":{"id":"9N87ZnOmN_JM"}},{"cell_type":"markdown","source":["#Metric : ROC AUC\n","\n","A single line on the graph that the curve for a single mode, and movement along a line indicates chaningn the threshild used for calssifying a positive instance, The threshold starts a t 0 in the upper right to and goes to 1 in the lower left.\n","\n","\n","The Area Under the Curve(AUC) explains itself by its name. It is simply the areunder the ROC curve. This metric is between 0 and 1 with a better model scoring higher. Model that simply guesses at random will have an ROC AUC of 0.5.\n","\n"],"metadata":{"id":"9Zfs5UeTPIfm"}},{"cell_type":"markdown","source":["Follow-up notebooks\n","\n","- Manual Feature Engineering 1\n","- Manual Feature Engineering 2\n","- Introduction to Automated FEature Engineering\n","- Advanced Automated Feature Engineering\n","- Feature Sleection\n","- Intro to Model Tuning : Grid and Random Search\n","- Automated Model Tuning\n","- Model Tuning Results"],"metadata":{"id":"ir7sikvuQLi4"}},{"cell_type":"markdown","source":["#Imports"],"metadata":{"id":"2u0RN1IvQeaF"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","\n","from sklearn.preprocessing import LabelEncoder\n","\n","import os\n","\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns"],"metadata":{"id":"-nvrxGe-Qdn4","executionInfo":{"status":"ok","timestamp":1667374453337,"user_tz":-540,"elapsed":9,"user":{"displayName":"김시윤","userId":"10463797081079810917"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["##Read in Data\n","\n","first, we can list all the available data fiels. There are a total of 9 files : 1 main file for training (with target) 1 main file for testing(without the target), 1 example submission file, and 6 other files containing additional information about each loan."],"metadata":{"id":"0byvkAqaQuLk"}},{"cell_type":"code","source":["print(os.listdir('../input/'))"],"metadata":{"id":"IJdzDZMFQ8tV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Training data\n","\n","app_train = pd.read_Csv('../input/application_train.csv')\n","print('Training data shape :', app_train.shape)\n","\n","app_train.head()"],"metadata":{"id":"lgY6mu-JRAsK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Testing data features\n","\n","app_test= pd.read_csv('../input/application_test.csv')\n","print('Testing data shape :', app_test.shape)\n","app_test.head()"],"metadata":{"id":"0viX4tlSRKce"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#EDA\n","\n","EDA is an open ended process where we calculate statistics and make figures to find trends, anomalies, patterns, or relationships within the data. The goal of EDA is to learn what our data can tell us. It genereally starts out with a high level overview, then narrows in to specific areas as we find intriguing areas of the data. The findings may be interesting in their own righrm or they can used to inform our modeling choices, such as by helping us decide which features to use."],"metadata":{"id":"pt_y9ieRRTPW"}},{"cell_type":"markdown","source":["Examine the Distribution of the Target Column\n","\n","The target is what we are asked to predict : either a 0 for the loan was repaid on time, or  a 1indicating the client had payment difficulties. We can first examine the number of loans falling into each category."],"metadata":{"id":"VTxC_cU2RqmI"}},{"cell_type":"code","source":["app_train['TARGET'].value_counts()"],"metadata":{"id":"V0-PIQMlR5-0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["app_train['TARGET'].astype(int).plot.hist()"],"metadata":{"id":"ayMDLrmARqLE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Examine Missing values"],"metadata":{"id":"7uhbit4nSGrv"}},{"cell_type":"code","source":["def missing_values_table(df) :\n","  mis_val = df.isnull().sum()\n","\n","  mis_val_percent = 100*df.isnull().sum() / len(df)\n","\n","  mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n","\n","  #rename the col\n","  mis_val_table_ren_col = mis_val_table.rename(columns={0 : 'Missing values', 1 : '% of total values'})\n","\n","  #Sort the table by percentage of missing descending\n","  mis_val_table_ren_columns = mis_val_table_ren_col[mis_val_table_ren_col.iloc[:,1] !=0].sort_values('% of total values', ascending=False).round(1)\n","\n","  #Print some summary information\n","  print('Your selected dataframe has ' + str(df.shape[1]) + 'columns.\\n' + 'There are ' + str(mis_val_table_ren_columns.shape[0]) + 'columns that have missing values')\n","\n","  #Return the dataframe with missing information\n","  return mis_val_table_ren_columns"],"metadata":{"id":"zdwyyQA9SICR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Missing values statistics\n","\n","missing_values = missing_values_table(app_train)\n","missing_values.head(20)"],"metadata":{"id":"6MkgQlE4TniU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Qhwn it comes time to build our machine learning models, we will have to fill in these missing vlaues. In later work, we will use models such as XGBoost that can handle missing values with no need for imputation.\n","\n","Another option would be to drop columns with a high percentage of missing values, although it is impossible to know ahead of time i these columns will be hilpful to our model. Therefore, we will keep all of the columns for now."],"metadata":{"id":"F9jZoDP1T9E1"}},{"cell_type":"markdown","source":["##Column Types\n","\n","Let's look at the #cols of each data type. int and float are numeric variables. objs cols contain str and are categorical features."],"metadata":{"id":"glpH82z-USVB"}},{"cell_type":"code","source":["app_train.dtypes.value_counts()"],"metadata":{"id":"bwntCk-eUehQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["app_train.select_dtypes('object').apply(pd.Series.nunique,axis=0 )\n","\n","#nunique : Return number of unique elements in the object."],"metadata":{"id":"ju8T27QQUiWk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Most of the categorical variables have a relatibely small number of unique entries.\n","\n","We will need to find a way to deal with these categorical variables."],"metadata":{"id":"6PENlLpvVXzs"}},{"cell_type":"markdown","source":["##Encoding Categorical Variables\n","\n","Before we go any further, we need to deal with pesky categorical variables. A machine learning model unfortunately cannot deal with categorical variables(such as LGBM). Therefore, we have to find a way to encode these variables as numbers bofore hadling them off to the model. There are two main wayy to carry out this process\n","\n","1. Label encoding : assign each unique category ina categorical variable with an integer., No new columns are created. \n","\n","2. One-hot encoding : create a new col for each unique category in a categorical variable. Each observation recieves a 1 in the column for its corresponding category and a 0 i all other new columns."],"metadata":{"id":"JSWWxHQNVgJn"}},{"cell_type":"markdown","source":["The problem with label encoding is that it gives the categories an arbitrary ordering. The value assigned to each of the categories in rando and does not reflect any ingerent aspect of the category. it will may cause a confusion btw programmer and data scientists.  Therefore, when we perform label encoding, the model might use the relative value of the feature ti assign weights which is not what we want. // for more than 2 unique categories, one-hot-encoding is the safe option.\n","\n","The onyl downside to one-hot-encoding is that #features can explode with categorical variables with many categories. To deal with this, we can perform one-hot encoding followed by PCa or other dimensionality reduction methods"],"metadata":{"id":"xNew_Hk-WEwz"}},{"cell_type":"markdown","source":["###Label Endocing and one-hot encoding\n","\n","LEt's implemet the policy described above : for any categorical variables with 2 unique categories, we will use label encoding, and for any categorical variable with more than 2 unique categories, we will use one-hot encoding."],"metadata":{"id":"Y0DegX7sXGfX"}},{"cell_type":"code","source":["#create a label encoder object\n","\n","le = LabelEncoder()\n","\n","le_count=0\n","\n","#Iterate through the columns\n","\n","for col in app_train :\n","  if app_train[col].dtype == 'object' :\n","    #if 2 or fewer unique categories\n","    if len(list(app_train[col].unique())) <= 2 :\n","      le.fit(app_train[col])\n","\n","      #Transform both training and testing data\n","      app_train[col] = le.transform(app_train[col])\n","      app_test[col] = le.transform(app_test[col])\n","\n","      #Keep track of how many columns were label encoded\n","      le_count+=1\n","\n","print(' {} columns were label encoded.'.format(le_count))"],"metadata":{"id":"2sOFFBLBXYXV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#one-hot encoding of categorical variables\n","\n","app_train = pd.get_dummies(app_train)\n","app_test = pd.get_dummies(app_test)\n","\n","print('Training FEature shape : ', app_train.shape)\n","print('Testing Feature shape : ', app_test.shape)\n","\n"],"metadata":{"id":"Pe4lWxcJX-bi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Aligning Training and Testing Data\n","\n","There need to be the same features (cols) in both the training and testing data.\n","\n","To ***remove the columns in the training data that are not in the testing data***, we need to align the dataframes.\n","\n","First, we extract the target column from the training data(bcuz this si not in the testing data but we need to keep this information). When we do the align we must make sure to set axis=1 to align the df based on the columns and not on the rows"],"metadata":{"id":"ffvM9o10YLJg"}},{"cell_type":"code","source":["train_labels = app_Train['TARGET']\n","#Align the training and testing data, keep only columns present in both df\n","\n","app_train, app_test = app_train.align(app_test, join = 'inner', axis=1)\n","\n","#Add the target back in\n","app_train['TARGET'] = train_labels\n","\n","print('Training feature shape :', app_train.shape)\n","print('Testing feature shape :' , app_test.shape)\n"],"metadata":{"id":"ugTIEfYQYypJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Back to Exploratory Data Analysis\n","\n","####Anomalies"],"metadata":{"id":"EojS8uR2ZP7L"}},{"cell_type":"code","source":["(app_train['DAYS_BIRTH'] / -365).describe()"],"metadata":{"id":"6Oq6toOkYKIB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["app_train['DAYS_EMPLOYED'].describe()"],"metadata":{"id":"Dm05IGCAZpxj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["app_train['DAYS_EMPLOYED'].plot.hist(title='Days Employment Histogram')\n","\n","plt.xlabel('Days Employment')"],"metadata":{"id":"PIImcuL2ZuJO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["anom = app_train[app_train['DAYS_EMPLOYED'] == 365243]\n","non_anom = app_train[app_train['DAYS_EMPLOYED'] != 365243]\n","print('The Non-anomalies default on %.2f%% of loans'% (100*non_anom['TARGET'].mean()))\n","\n","print('The anomalies default on %.2f%% of loans' % (100*anom['TARGET'].mean()))\n","\n","print('There are %d anomalous days of employment' % (len(anom)))"],"metadata":{"id":"gynOpF7kaA6G"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["One of the safest approaches is just to set the anomalies to a missing value and then have them filled in (using imputation) vefore machine learning. In this case, since all the anomalies have the exact same value, we want to fill them in with the same value in case all of these loans share something in common. \n","\n","In this solution, we will fill in the anomalous values with np.NaN and then create a new boolean cols indicating whether or not the value was anomalous."],"metadata":{"id":"LoDF_8vDazjl"}},{"cell_type":"code","source":["#Create an anomalous flag column\n","\n","app_train['DAYS_EMPLOYED_ANOM'] = app_train['DAYS_EMPLOYED'] == 365243\n","\n","#Replace the anomalous values with nan\n","\n","app_train['DAYS_EMPLOYED'].replace({365243 : np.NaN}, inplace=True)\n","\n","app_train['DAYS_EMPLOYED'].plot.hist(title='Days Employment Histogram')\n","plt.xlabel('Days Employment')"],"metadata":{"id":"zVcHx02qbR_F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Create an anomalous flag column\n","\n","app_test['DAYS_EMPLOYED_ANOM'] = app_test['DAYS_EMPLOYED'] == 365243\n","\n","#Replace the anomalous values with nan\n","\n","app_test['DAYS_EMPLOYED'].replace({365243 : np.NaN}, inplace=True)\n","\n","app_test['DAYS_EMPLOYED'].plot.hist(title='Days Employment Histogram')\n","plt.xlabel('Days Employment')"],"metadata":{"id":"7vLWXPONaheG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Correlations\n","\n"],"metadata":{"id":"NwBEsB2Tb137"}},{"cell_type":"code","source":["# find correlations with the target and sort\n","\n","correlations = app_train.corr()['TARGET'].sort_values()\n","\n","#Display corrrelations\n","\n","print('Most Posirive Correlation : \\n', correlations.tail())\n","print('\\n Most Negative Correlation : ', correlations.head())\n","\n","# sns.heatmap(correlations, annot=True, fmt='2.f', square=True blah blabh blah~~)"],"metadata":{"id":"VeX1eG6Qb77T"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Effect of Age on Repayment"],"metadata":{"id":"i5eo9KgncXRe"}},{"cell_type":"code","source":["#find the correlation of the positive days since birth and target\n","\n","app_train['DAYS_BIRTH'] = abs(app_train['DAYS_BIRTH'])\n","\n","app_train['DAYS_BIRTH'].corr(app_train['TARGET'])"],"metadata":{"id":"iUI0tslDcWdI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Set the style of plots\n","\n","plt.style.use('fivethirtyeight')\n","\n","#plot the distribution of ages in years\n","plt.hist((app_train['DAYS_BIRTH' / 365), edgecolor='k', bins=25])\n","plt.title('Age of Client')\n","plt.xlabel('Age(years')\n","plt.ylabel('Count')\n"],"metadata":{"id":"0_ggEYg4cja3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(10,8))\n","\n","sns.kdeplot(app_train.loc[app_train['TARGET']==0, 'DAYS_BIRTH'] /365, label = 'target==0')\n","sns.kdeplot(app_train.loc[app_train['TARGET'] ==1, 'DAYS_BIRTH'] / 365, label = 'target==0')\n","\n","plt.xlabel('Age')\n","plt.ylabel('Distribution')\n","plt.title('Distribusion of Ages')"],"metadata":{"id":"53tAQt2rdGtt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# AGe information into a separate dataframe\n","\n","age_data = app.train[ ['TARGET','DAYS_BIRTH']]\n","age_data['YEARS_BIRTH'] = age_data['DAYS_BIRTH'] / 365\n","\n","#bin the age data\n","age_data['YESRS_BINNED'] = pd.qcut(age_data['YEARS_BRITH'], bins=np.linspace(2070,num=11))\n","\n","age_data.head()"],"metadata":{"id":"racczvoKdjNc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Group by the bin and calculate averages\n","\n","age_groups = age_data.groupby('YEARS_BINNED').mean()\n","\n","age_groups"],"metadata":{"id":"jriUhJniJ_6n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(8,8))\n","\n","plt.bar(age_groups.index.astype(str), 100*age_groups['TARGET'])\n","\n","plt.xticks(rotation=75)\n","plt.xlabel('Age Group(yrs')\n","plt.ylabel('Failure to Repay')\n","plt.title('Failure to Repay by age group')"],"metadata":{"id":"DBphXJRFKM6_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Exterior Sources\n","\n","3 variables with the strongest neg corr with the target are 'ext_souce_1','ext_source_2','ext_source_3'"],"metadata":{"id":"NPIy6uWAKn0Q"}},{"cell_type":"code","source":["# Extract the EXT_SOURCE variables and show correlations\n","\n","ext_data = app_train[ [ 'TARGET','EXT_SOURCE_1','EXT_SOURCE_2','EXT_SOURCE_3','DAYS_BIRTH']]\n","\n","ext_data_corrs= ext_data.corr()\n","ext_data_corrs"],"metadata":{"id":"mlgnTwpxK_9-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(8,6))\n","\n","sns.heatmap(ext_data_corrs, cmap = plt.cm.RdYlBu_r, vmin=-0.2, annot=True, vamx=0.6)\n","\n","plt.title('Correlation Heatmap')"],"metadata":{"id":"X4fKKocpLQN1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(10,12))\n","\n","#iterate throug the source\n","\n","for i , source in enumerate(['EXT_SOURCE_1','EXT_SOURCE_2','EXT_SOURCE_3']) :\n","  plt.subplot(3,1,i)\n","\n","  sns.kdeplot(app_train.loc[app_train['TARGET'] ==0, source], label='target==0')\n","  sns.kdeplot(app_train.loc[app_train['TARGET'] ==1, source], label='target==1')\n","\n","  plt.xlabel('%s' % source)\n","  plt.ylabel('Density')\n","  plt.title('Distribution of %s by Target values' % source)\n","\n","plt.tight_layout(h_pad=2.5)"],"metadata":{"id":"uu_kSJbJLbuj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Pairs Plot"],"metadata":{"id":"N7vfHblcMKX3"}},{"cell_type":"code","source":["# copy the data for plotting\n","\n","plot_data = ext_data.drop(['DAYS_BRITH']).copy()\n","\n","#add in the age of the client in years\n","plot_data['YEARS_BIRTH'] = age_Data['YEARS_BIRTH']\n","\n","#drop na values and limit to first 100k rows\n","\n","plot_data = plot_data.dropna().loc[:100000, :]\n","\n","#function to calculate correlation coefficient between two clumns\n","def corr_func(x,y, **kwargs) :\n","  r=np.corrcoef(x,y)[0][1]\n","  ax = plt.gca()\n","  ax.annotate('r = {:.2f'.format(r), xy=(.2,.8), xycoords=ax.transAxes, size=20)\n","\n","#Create the parigrid object\n","grid= sns.Pairplot(plot_data, sie=3, diag_sharey=False, hue='TARGET', \n","                   vars= [x for x in list(plot_data.columns) if x != 'TARGET']) \n","\n","#UPPER IS A SCATTER PLOT\n","grid.map_upper(plt.scatter, alpha=0.2)\n","\n","#Diagonal is a histogram\n","grid.map_diag(sns.kdeplot)\n","\n","#Bottom is density plot\n","grid.map_lower(sns.kdeplot, cmap=plt.cm.OrRd_r)\n","\n","plt.subtitle('Ext source and age features pairs plot', size=32, y=1.05)"],"metadata":{"id":"PdJJoT8TN27y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In this plots, the red indicates loans that were not repaid and the blue are loans that are paid. we can see the different relationships within the data, There dose appear to be a moderate positive linear relationship between the EXT_SOUCE_1 and the DAYS_BIRTH, indicating that this feature may take into account the age of the client."],"metadata":{"id":"H7snTY4hPOoW"}},{"cell_type":"markdown","source":["#Feature Engineering\n","\n","most parts as the winning models, at least for structured data, all tento to be variants on gradient boosting\n","\n","\n","- applied machine learning is basically feature engineering\n","\n","\n"],"metadata":{"id":"qBHgTyl6PhFz"}},{"cell_type":"markdown","source":["Polynomial Features\n","\n","in polynomial features, we can create variables such as ext_source_1^2 and ext_source_2^2.\n","\n","or ext_source_1 x ext_source_2 /\n","\n","ext_source_1 x ext_source_2^2\n","\n","\n","The combination of multiple individual variables are called ***interaction terms*** bcuz they capture the interactions between variables. In other words, while two variables by hemselves may not have a strong influence on the target, ***combining them tohether into a single interaction vatiable might show a relationship with the target***."],"metadata":{"id":"jBUsF5QjQPY7"}},{"cell_type":"code","source":["poly_features = app_train [['EXT_SOURCE_1','EXT_SOURCE_2','EXT_SOURCE_3','DAYS_BIRTH','TARGET']]\n","\n","poly_features_test = app_test [['EXT_SOURCE_1','EXT_SOURCE_2','EXT_SOURCE_3','DAYS_BIRTH']]\n","\n","#imputer for handling missing values\n","\n","#from sklearn.preprocessing import Imputer\n","from sklearn.imput import SimpleImputer\n","\n","imputer = SimpleImputer(strategy='median')\n","\n","poly_target = poly_features['TARGET']\n","\n","poly_features = poly_features.drop(['TARGET'])\n","\n","#Need to impute missing values\n","\n","poly_features = SimpleImputer.fit_transform(poly_features)\n","poly_features_test = SimpleImputer.fit_transform(poly_features_test)\n","\n","from sklearn.preprocessing import PolynomialFeatures\n","\n","#Create the polynomial object with specified degree\n","\n","poly_transformer = PolynomialFeatures(degree=3)"],"metadata":{"id":"_EfS5WXaRcve"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Train the polynomial features\n","\n","poly_transformer.fit(poly_features)\n","\n","#Transform the features\n","poly_features = poly_transformer.transform(poly_features)\n","poly_features_test = poly_transformer.transform(poly_features_test)\n","\n","print('Polynomial Features shape : ', poly_features.shape)"],"metadata":{"id":"W4xzrmi2SNyc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This creates a considerable #new features. To get the names we have to use the polynomial features get_feature_names method."],"metadata":{"id":"8bezltg1Sh6L"}},{"cell_type":"code","source":["poly_transformer.get_feature_names(input_features = ['EXT_SOURCE_1','EXT_SOURCE_2','EXT_SOURCE_3','DAYS_BIRTH'])[:15]\n","\n","#15개의 최대조합 생성"],"metadata":{"id":"uznTWWfeSoWi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["There are 35 features with individual features raised to powers up to degree 3 and interaction terms. Now, we can see whether any of these new features are correalted with the target."],"metadata":{"id":"rFP958iHS9-5"}},{"cell_type":"code","source":["#Create a dataframe of the features\n","\n","poly_features = pd.DataFrame(poly_features, columns = poly_transformer.get_feature_names(['EXT_SOURCE_1','EXT_SOURCE_2','EXT_SOURCE_3','DAYS_BIRTH']))\n","\n","#Add in the target\n","poly_features['TARGET'] = poly_target\n","\n","#Find the correlations with the target\n","poly_corrs = poly_features.corr()['TARGET'].sort_values()\n","\n","#Display most negtive andn most positive\n","print(poly_corrs.head())\n","ptiny(poly_corrs.tail())"],"metadata":{"id":"Cif-QNR8TGIQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Several of the new variables have a greater correlation with the target than the original features. When we build ML models, we can try with and without these features to determine if they actually help the model learn.\n","\n","We will add these features to a copy of the training and testing data and then evaluate models with and without the features. Many time in ML, the only way to know if an approach will work is to try it out"],"metadata":{"id":"a1MQAZkoT7E_"}},{"cell_type":"code","source":["#put test featues into df\n","\n","poly_features_test = pd.DataFrame(poly_features_test, columns=poly_transformer.get_feature_names(['EXT_SOURCE_1','EXT_SOURCE_2','EXT_SOURCE_3', 'DAYS_BIRTH']))\n","\n","#Merge polynomial features into training df\n","poly_features['SK_ID_CURR'] = app_train['SK_ID_CURR']\n","app_train_poly = app_train.merge(poly_features, on ='SK_ID_CURR', how='left')\n","\n","#Merge polynomial features into testing df\n","poly_features['SK_ID_CURR'] = app_test['SK_ID_CURR']\n","app_test_poly = app_test.merge(poly_features_test, on='SK_ID_CURR', how='left')\n","\n","#Align the df\n","app_train_poly, app_test_poly = app_train.align(app_test_poly, join='inner', axis=1)\n","\n","#Print out the new shapes\n","print('Training data with polynomial features shape :', app_train_poly.shape)\n","print('test data with polynomial features shape :', app_test_poly.shape)"],"metadata":{"id":"nTvOp9olUQPH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Domain Knowledge Features\n","\n","we could call this 'attempts at applying limited financial knowledge'. In this frame of mind, we cna make a couple features that attempt to capture what we think may be important for telling whether a client will default on a loan.\n","\n","- CREDIT_INCOM_PERCENT : % of the credit amount relative to a client's income\n","- ANNUITY_INCOME_PERCENT : % of the loadn annuity\n","- CREDIT_TERM : len of the payment in month\n","- DAYS_EMPLOYED_PERCENT : % of the days employed relative to the client's age"],"metadata":{"id":"7meK_id1T6-K"}},{"cell_type":"code","source":["app_train_domain = app_train.copy()\n","app_test_domain=app_test.copy()\n","\n","app_train_domain['CREDIT_INCOME_PERCENT'] = app_train_domain['AMT_CREDIT'] / app_train_domain['AMT_INCOME_TOTAL']\n","app_train_domain['ANNUITY_INCOME_PERCEN'] = app_train_domain['AMT_ANNUITY'] / app_train_domain['AMT_INCOME_TOTAL']\n","app_train_domain['CREDIT_TERM'] = app_train.domain['AMT_ANNUITY'] / app_train_domain['AMT_CREDIT']\n","app_train_domain['DAYS_EMPLOYED_PERCENT']=app_train_domain['DAYS_EMPLYED'] / app_train_domain['DAYS_BIRTH']"],"metadata":{"id":"EuAd02t3WFhD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["app_test_domain['CREDIT_INCOME_PERCENT'] = app_test_domain['AMT_CREDIT'] / app_test_domain['AMT_INCOME_TOTAL']\n","app_test_domain['ANNUITY_INCOME_PERCEN'] = app_test_domain['AMT_ANNUITY'] / app_test_domain['AMT_INCOME_TOTAL']\n","app_test_domain['CREDIT_TERM'] = app_test.domain['AMT_ANNUITY'] / app_test_domain['AMT_CREDIT']\n","app_test_domain['DAYS_EMPLOYED_PERCENT']=app_test_domain['DAYS_EMPLYED'] / app_test_domain['DAYS_BIRTH']"],"metadata":{"id":"IV3_m0SIXTQ9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Visualize new variables"],"metadata":{"id":"qqv9YDSuXgo5"}},{"cell_type":"code","source":["plt.figure(figsize(12,20))\n","\n","for i, feature in enumerate(['CREDIT_INCOME_PERCENT','ANNUITY_INCOME_PERCENT','CREDIT_TERM','DAYS_EMPLOYED_PERCENT']) :\n","  plt.subplot(4,1,i+1)\n","  #plot that repaid loans\n","\n","  sns.kdeplot(app_train_domain.loc[app_train_domain['TARGET'] ==0, feature], label = 'target==0')\n","  sns.kdeplot(app_train_domain.loc[app_train_domain['TARGET'] ==1, feature], label = 'target==1')\n","\n","  #Label the plots\n","  plt.title('Distribution of %s by Target values' % feature)\n","  plt.xlabel('%s' % feature)\n","  ply.ylabel('Density')\n","\n","plt.tight_layout(h_pad=2.5)"],"metadata":{"id":"2qoVVQtjT1pD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Baseline\n","\n","For a naive baseline, we could guess the same value for all examples on the testing set. We are asked to predict the probability of not repaying the loan, so if we are entirely unsure, we would guess .5 for all observations on the test set.\n","\n","This will get us a Reciever Operating Characteristic Area under the Cuve (AUCROC) of .5 in the comptition\n","\n","Since we already know what score we are going to get, we don't really need to make a naive baseline guess."],"metadata":{"id":"mQ5SQI2BYKxZ"}},{"cell_type":"markdown","source":["##Logistic Regression Implementation\n","\n","To get a baseline, we will use all of the features after encoding the categorical vaiables. We will preprocess the data by filling in the missing values and normalizing the range of the features (feature scaling). The following code performs both of these preprocessing steps."],"metadata":{"id":"eGXDy5GPvKED"}},{"cell_type":"code","source":["from sklearn.preprocessing import MinMaxScaler, Imputer\n","\n","#drop the target from the training data\n","if 'TARGET' in app_train :\n","  trian=app_train.drop(['TARGET'])\n","\n","else :\n","  train=app_train.copy()\n","\n","\n","#Feature names\n","features = list(train.columns)\n","\n","test = app_test.copy()\n","\n","#MEdian imputation of missing values\n","imputer = SimpleImputer(strategy='median')\n","\n","#Scale each feature to 0-1\n","scaler=MinMaxScaler(feature_range=(0,1))\n","\n","#fit on the training data\n","imputer.fit(train)\n","\n","#Transform both training and testing data\n","train=imputer.transform(train)\n","test=imputer.transform(test)\n","\n","#Repeat with the scaler\n","scaler.fit(train)\n","train=scaler.transform(train)\n","test=scaler.transform(test)\n","\n","print('Training data shape : ', train.shape)\n","print('Test data shape : ', test.shape)\n"],"metadata":{"id":"dcaYu8-6vjDp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We will use LogisticRegression from sklearn for our first model. The only change we will make from the default model settings is to lower the regularization parameter, C, which controls the amount of overitting (a lower value should decrease overfitting). This will get us slightly better results than the default LogisticRegression, but it still will set a low bar for any future models.\n","\n","Here we use the familiar Scikit-Learn modeling syntax : we first create the model, then we train the model using."],"metadata":{"id":"kYYRVLm7wVCy"}},{"cell_type":"code","source":["from sklearn.linear_model import LogisticRegression\n","\n","log_reg = LogisticRegression(C=0.0001)\n","\n","log_reg.fit(train, train_labels)"],"metadata":{"id":"Q5-9W0Cjw6ay"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#make predictions\n","#make sure to select the second column only\n","log_reg_pred = log_reg.predict_proba(test)[:,1]"],"metadata":{"id":"ggwvZaWgxEtL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The predictions must be in the format shown in the sample_submission.csv file, where there are only two columns : SK_ID_CURR and TARGET We will create a df in this format from the test set and the predictions called submit."],"metadata":{"id":"v1xMaR_dxLvK"}},{"cell_type":"code","source":["#Submission df\n","submit = app_test[ ['SK_ID_CURR']]\n","submit['TARGET']=log_reg_pred\n","\n","submit.head()"],"metadata":{"id":"zwewU1HGxKST"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The predictions represent a probability between 0 and 1 that the loan will not be repaid. If we were using these predictions to classify applicatns, we couls set a probablity threshold for determining that a loan is risky."],"metadata":{"id":"noRMzdd9xd5N"}},{"cell_type":"code","source":["#Save the submission to a csv file\n","\n","submit.to_csv('log_reg_baseline.csv', index=False)"],"metadata":{"id":"cHOmH3l_xo-G"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Improved Model : Random Forest"],"metadata":{"id":"5EJlrxwCx-P2"}},{"cell_type":"code","source":["from sklearn.ensemble import RandomForestClassifier\n","\n","#make the random forest classifier\n","random_forest = RandomForestClassifier(n_estimators=100, random_state=42, verbose=1, n_jobs=-1)"],"metadata":{"id":"kK7qlol6yBQf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Train on the training data\n","\n","random_forest.fit(train, train_labels)\n","\n","#Extract feature importances\n","feature_importance_values=random_forest.feature_importances_\n","feature_importances = pd.DataFrame({'feature' : features, 'importrnacec':feature_importance_values})\n","\n","#make predictions on the test data\n","predictions=random_forest.predict_proba(test)[:,1]\n"],"metadata":{"id":"mwzY9hDiyPEP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#make a submission dataframe\n","submit = app_test[['SK_ID_CURR']]\n","submit['TARGET']=predictions\n","\n","submit.to_csv('random_forest_baseline.csv', index=False)"],"metadata":{"id":"NzPkNByPymYF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Make Predictions using Engineered Features"],"metadata":{"id":"vVKDZP3EyxpB"}},{"cell_type":"code","source":["poly_features_names =list(app_train_poly.columns)\n","\n","#Impute the polynomial features\n","imputer=SimpleImputer(strategy='median')\n","\n","poly_features = imputer.fit_transform(app_train_poly)\n","poly_features_test = imputer.transform(app_test_poly)\n","\n","#Scale the polynomial features\n","scaler = MinMaxScaler(feature_range=(0,1))\n","\n","poly_features = scaler.fit_transform(poly_features)\n","poly_features_test = scaler.transform(poly_features_test)\n","\n","random_forest_poly = RandomForestClassifier(n_estimators=100,random_state=42, verbose=1, n_jobs=-1)\n","\n"],"metadata":{"id":"fWGu-yNjy0iU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Train on the training data\n","random_forest_poly.fit(poly_features, train_labels)\n","\n","#make predictions on the test data\n","predictions = random_forest_poly.predict_proba(poly_features_test)[:,1]"],"metadata":{"id":"E2LXj5-0zgwO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["submit=app_test[ ['SK_ID_CURR']]\n","submit['TARGET'] = predictions\n","\n","submit.to_csv('random_forest_baseline_engineered.csv', index=False)"],"metadata":{"id":"MmoN2N2Gztsp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Testing Domain Features"],"metadata":{"id":"PSYROyNpz18c"}},{"cell_type":"code","source":["app_train_domain = app_train_domain.drop('TARGET')\n","\n","domain_features_names = list(app_train_domain.columns)\n","\n","imputer= SimpleImputer(strategy='median')\n","\n","domain_features = imputer.fit_transform(app_train_domain)\n","domain_features_test=imputer.transform(app_test_domain)\n","\n","#Scale the domainnomial features\n","scaler = MinMaxScaler(feature_range=(0,1))\n","\n","domain_features = scaler.fit_transform(domain_features)\n","domain_features_test = scaler.transform(domain_features_test)\n","\n","random_forest_domain = RandomForestClassifier(n_estimators = 100, random_state = 50, verbose = 1, n_jobs = -1)\n","\n","#train on the training data\n","random_forest_domain.fit(domain_features, train_labels)\n","\n","#Extract feature importances\n","feature_importance_values_domain = random_forest_domain.feature_importances_\n","feature_importrances_domain = pd.DataFrame({'feature':domain_features_names,'importance' : feature_importances_values_domain})\n","\n","#make predictions on the test data\n","predictions = random_forest_domain.predict_proba(domain_features_test)[:,1]"],"metadata":{"id":"DowUeMl3z1My"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Make a submission dataframe\n","submit = app_test[['SK_ID_CURR']]\n","submit['TARGET'] = predictions\n","\n","# Save the submission dataframe\n","submit.to_csv('random_forest_baseline_domain.csv', index = False)"],"metadata":{"id":"ax5PVrQX0--O"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Model Interpretation : Feature importances"],"metadata":{"id":"QuW62-pc1BA_"}},{"cell_type":"code","source":["def plot_feature_importances(df) :\n","  #sort features according to importance\n","  df=df.sort_values('importance', ascending=False).reset_index()\n","\n","  #Normalize the feature importances to add up to one\n","  df['importance_normalized'] = df['importance']/df['importnace'].sum()\n","\n","  #make a horizontal bar chart of feature importances\n","  plt.figure(figsize=(10,6))\n","  ax=plt.subplot()\n","\n","  ax.barh(list(reversed(list(df.index[:15]))),\n","               df['importnace_normalized'].head(15),\n","               align = 'center' edgecolor = 'k' )\n","  \n","  #Set the yticks and labels\n","  ax.set_yticks(list(reversed(list(df.index[:15]))))\n","  ax.set_yticklabels(df['feature'].head(15))\n","\n","  #plot labeling\n","  plt.xlabel('Normalized Importance')\n","  plt.title('Feature importrances')\n","  plt.show()\n","\n","  return df"],"metadata":{"id":"KriPwNly1DTE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["feature_importances_sorted = plot_feature_importances(feature_importnaces)"],"metadata":{"id":"6NNswvjn2FE7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["feature_importances_domain_sorted = plot_feature_importances(feature_importances_domain)"],"metadata":{"id":"pDm9F6fj2LLc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Conclusion\n","\n","\n","In short, we followed the general outline of a ML Project :\n","\n","1. Understand the problem and the data\n","2. Data cleaning and formatting (this was mostly done for us)\n","3. EDA\n","4. Baseline Model\n","5. Improved model\n","6. Model Interpretation (just a little)"],"metadata":{"id":"tkA1glNu2TFJ"}},{"cell_type":"markdown","source":["#LGBM"],"metadata":{"id":"r4lzJBRa3Tjg"}},{"cell_type":"code","source":["from sklearn.model_selection import KFold\n","from sklearn.metric import roc_auc_score\n","import lightgbm as lgb\n","import gc\n","\n","def model(features, test_features, encoding = 'ohe', n_folds=5) : \n","  '''train and test a light gradient boosting model using cross validation.\n","\n","  Parameters \n","  1. encoding (str, default = 'ohe')\n","  : method for encoding categorical variables. Either 'ohe' for one-hot encoding or 'le' for integer label\n","  ------\n","  return\n","  1. submission(pd.DataFrame)\n","  : df with 'SK_ID_CURR' ad 'TARGET proba that predicted by the model.\n","  2. feature_improtances\n","  : df with the feature importances from the model\n","  3. valid_metrics\n","  : df with training and validation metrics for each fold and overall\n","  '''\n","\n","  #Extract the ids\n","  train_ids = features['SK_ID_CURR']\n","  test_ids = test_features['SK_ID_CURR']\n","\n","  #Extract the labels for training\n","  labels = features['TARGET']\n","\n","  #Remove the ids and target\n","  features = features.drop(['SK_ID_CURR', 'TARGET'])\n","  test_features = test_features.drop(['SK_ID_CURR'])\n","\n","  #ohe\n","  if encoding = 'ohe':\n","    features = pd.get_dummies(features)\n","    test_features = pd.get_dummies(test_features)\n","\n","    #align the df by the columns\n","    features, test_features = features.alien(test_features, join='inner', axis=1)\n","\n","    #No categorical indices to records\n","    cat_indices = 'auto'\n","  #integer label encoding\n","  elif encoling ==' le':\n","    label_encoder = LabelEncoder()\n","\n","    #List for storing categorical indices\n","    cat_indices =[]\n","\n","    #Iterate through each column\n","    for i, col in enumerate(features) :\n","      if features[col].dtype == 'object' :\n","        #Map the categorical features to integers\n","        features[col] = label_encoder.fit_transform(np.array(features[col].astype(str)).reshape(-1,) )\n","\n","        #record the categorical indices\n","        cat_indices.append(i)\n","\n","  #Catch error if label encoding scheme is not valie\n","  else :\n","    raise ValueError('Encoding must be either 'ohe' or 'le'')\n","  \n","  print('Training data shape : ', features.shape )\n","  print('Test data shape : ', test_features.shape )\n","\n","\n","  feature_names = list(features.columns)\n","\n","  #Convert to np arrays\n","\n","  features = np.array(features)\n","  test_features = np.array(test_features)\n","\n","  #Create the kfold object\n","\n","  k_fold=KFold(n_splits =n_folds, shuffle=False, random_state=42)\n","\n","  #empty array for feature importances\n","  feature_importance_values = np.zeroes(len(feature_names))\n","\n","  #empty array for test predictions\n","  test_predictions = np.zeroes(test_features.shape[0])\n","\n","  #empty array for out of fold validation predictions\n","  out_of_fold = np.zeroes(features.shape[0])\n","\n","  #list for recording validation and training scores\n","  valid_scores=[]\n","  train_scores=[]\n","\n","  #Iterate through each fold\n","  for train_indices, valid_indices in k_fold.split(features) :\n","\n","    #Training data for the fold\n","    train_features, train_labels = features[train_indices], labels[train_indices]\n","    #Validation data for the fold\n","    valid_features, valid_labels = features[valid_indcies], labels[valid_indices]\n","\n","    #Create the model\n","    model = lgb.LGBMClassifier(n_estimators=1000, objective='binary', class_weight='balanced', learning_rate=0.1,\n","                               reg_alpha=0.1, reg_lambda=0.1, subsample=0.8, n_jobs=-1, random_state=42)\n","    \n","    #Train the model\n","    model.fit(train_features, train_labels, eval_metric='auc',\n","              eval_set=[(valid_features, valid_labels), (train_features, train_labels)],\n","              eval_names = ['valid','train'],\n","              categorical_feature = cat_indices,\n","              early_stopping_rounds=100, verbose=200)\n","    best_iteration = model.best_iteration_\n","\n","    #Record the feature importances\n","    feature_importrnace_values += model.feature_importances_ / k_fold.n_splits\n","\n","    #Make predictions\n","    test_predictions += model.predict_proba(test_features, num_iternation = best_iterration)[:,1]/k_fold.n_splits\n","\n","    #Record the best score\n","    valid_score=model.best_score_['valid']['auc']\n","    train_score = model.best_score_['train']['auc']\n","\n","    valid_score.append(valid_score)\n","    train_score.appned(train_score)\n","\n","    #Clean up memory\n","    gc.enable()\n","    del model, train_feature, valid_features\n","    gc.collect()\n","\n","    #make the submission df\n","  submission = pd.DataFrame({'SK_ID_CURR':test_ids, 'TARGET':test_predictions})\n","\n","    #make the feature importance dataframe\n","  feature_importances = pd.DataFrame({'feature' : feature_names, 'importance' : feature_importance_values})\n","\n","    #Overall validation score\n","  valid_auc = roc_auc_score(labels, out_of_fold)\n","\n","\n","    #Add the overall scores to the metrics\n","  valid_scores.append(valid_auc)\n","  train_scores.append(np.mean(train_scores))\n","\n","    #Needed for creating dataframe of validation scores\n","  fold_names = list(range(n_folds))\n","  folds_names.append('overall')\n","\n","    #Dataframe of validation scores\n","  metrics = pd.DataFrame({'fold' : fold_names,\n","                            'train' :  train_scores,\n","                            'valid' : valid_scores})\n","\n","  return submission, feature_importrnaces, metrics\n","            "],"metadata":{"id":"mss97Uhl3YV5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["submission, fi, metrics = model(app_train, app_test)\n","print('baseline metrics')\n","print(metrics)"],"metadata":{"id":"Ktu0E7Hu9TaU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fi_sorted = plot_feature_importances(fi)"],"metadata":{"id":"E9q-FFKY9ZpZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["submission.to_csv('baseline_lgb.csv', index=False)"],"metadata":{"id":"3TdDv7_99b-J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["app_train_domain['TARGET'] = train_labels\n","\n","#Test the domain knowledge features\n","submission_domain, fi_domain, metrics_domain = model(app_train_domain, app_test_domain)\n","\n","print('Baseline with domain knowledge features metrics')\n","print('metrics_domain')"],"metadata":{"id":"AeILL2j49gGl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fi_sorted = plot_feature_importances(fi_domain)"],"metadata":{"id":"6KmCdHoi9tEo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["submission_domain.to_csv('baseline_lgb_domain_features.csv', index=False)"],"metadata":{"id":"9NgiGS_e9w_x"},"execution_count":null,"outputs":[]}]}