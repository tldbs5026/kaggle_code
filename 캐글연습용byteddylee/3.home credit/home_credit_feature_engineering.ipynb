{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNJXrhpkgRWYD5trlUrjHrM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#Introduction : Manual Feature Engineering\n","\n","in this notebook, we will explore making features by hand for the home credit default risk competetion.\n","\n","in order to beeter this score, we will have to include more informaiton from the other dataframes.\n","\n","- bereau : information abour client's previous loans with other financial institutions reported to home credit.\n","\n","- bureau_balance : monthly informaiton about the previous loans. Each month has its own row.\n"],"metadata":{"id":"ToxKfrRepuDA"}},{"cell_type":"code","source":["# pandas and numpy for data manipulation\n","\n","import pandas as pd\n","import numpy as np\n","\n","#matplotlib and seaborn for plotting\n","import matlotlib.pyplot as plt\n","import seaborn as sns\n","%matplotlib inline\n","\n","#Suppress warnings from pandas\n","import warnings\n","warnings.filterwarings('ignore')\n","\n","plt.style.use('fivethirtyeight')"],"metadata":{"id":"DvD8LSnTqeB8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Example : Counts fo a client's previous loans\n","\n","To illustrate the general precess of manual feature engineering, we will first simply get the count of a client's previous loans at other financial institutions.\n","\n","This requires a number of pandas operations we will make heavy use of throughout the notebook :\n","\n","- groupby : group a df by a cols. in this case we will groupby the unique client, the sk_id_curr\n","- agg : perform a calculation on the grouped data such as taking the mean of cols. We can either all the function directly.\n","- merge : match the aggregated statictics to the appropriate client. We need to merge the original training data with the calculated stats on the sk_id_cur cols which will insert NAN in any cell for which the client does not have the corresponding statistic\n","\n","We also use the rename function quite a bit specifying the cols to be renamed as a dict. This is useful in order to keep track of the new variables we create."],"metadata":{"id":"eon9t6aiqwY9"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"AI36DX8_ecXv"},"outputs":[],"source":["#REad in bureau\n","\n","bureau = pd.read_csv('../input/bureau.csv')\n","\n","bureau.head()"]},{"cell_type":"code","source":["#Groupby the client id, count the number of previous loans, and rename the column\n","bureau=bureau.columns.str.lower()\n","previous_loan_counts = bureau.groupby('sk_id_curr' , as_index=False)['sk_id_curr'].count().rename(columns={'sk_id_bureau' : 'previous_loan_counts'})\n","\n","previous_loan_counts.head()"],"metadata":{"id":"UwcgyTNzrqR0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#join to the training df\n","train = pd.read_csv('../input/appication_train.csv')\n","train = train.merge(previous_loan_counts, on='sk_id_curr', how='left')\n","\n","#fill the missing values with 0\n","\n","train['previous_loan_count'] = train['previous_loan_count'].fillna(0)\n","train.head()"],"metadata":{"id":"CX_O4RfNsB5q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Assessing Usefulness of new Variable with r value\n","\n","To determine if the new variable is useful, we can calculate the Pearson correlation coeffient btw this variable and the target.\n","\n","We can also visually inspect a relationship with the target using the Kernel Density Estimakte(KDE) plot."],"metadata":{"id":"zc4bvpYKsULv"}},{"cell_type":"markdown","source":["KDE"],"metadata":{"id":"B6zrlYkZsnK_"}},{"cell_type":"code","source":["#Plots the distribution of a variable colored by value of the target\n","def kde_target(var_name, df)  :\n","\n","  #Calculate the correaltino coefficient btw the new variable and the target\n","  corr = df['target'].corr(df[var_name])\n","\n","  #calcylate medians for repaid vs not repaid\n","  avg_repaid = df.ix[df['target'] ==0, var_name].median()\n","  avg_not_repaid = df.ix[df['target'] ==1, var_name].median()\n","\n","  plt.figure(figsize=(12,6))\n","\n","  #plot the distribution for target ==0 and target==1\n","\n","  sns.kdeplot(df.ix[df['target'] ==0, var_name], label = 'target==0')\n","  sns.kdeplot(df.ix[df['target']==1, var_name], label = 'target ==1')\n","\n","\n","  #label the plot\n","  plt.xlabel(var_name)\n","  plt.ylabel('density')\n","  plt.title('%s distribution'% var_name)\n","  plt.legend()\n","\n","  #print out the correlation\n","  print('The correlation btw %s and the target is %0.4f' %(var_name, corr))\n","  print('median value for loan that was not repaid = %0.4f' %avg_not_repaid)\n","  print('median value for loan that was repaid = %0.4f'% avg_repaid)\n","  "],"metadata":{"id":"GT817qrysq3-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can test this function using the ext_source_3 variable which we found to be one of the most important variables according to a random forest and gradient boosting machine."],"metadata":{"id":"ZJA70DUbttQf"}},{"cell_type":"code","source":["kde_target('ext_source_3', train)"],"metadata":{"id":"sx5z4xPPt4-Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["kde_target('previous_loan_counts', train)"],"metadata":{"id":"EwQgxIhit858"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Aggregating Numeric Columns"],"metadata":{"id":"JReE6kt0t_wN"}},{"cell_type":"code","source":["#Group by the client id, calculate aggregation statistics\n","\n","bureau_agg = bureau.drop('sk_id_bureau', axis=1).groupby('sk_id_curr', as_index=False).agg(['count','mean','max','min','sum']).reset_index()\n","\n","bureau_agg.head()"],"metadata":{"id":"P_K14GMEt-8f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#List of col names\n","columns = ['sk_id_curr']\n","\n","#Iterate through the variables names\n","for var in bureau_agg.columns.levels[0] :\n","  #Skip the id name\n","  if var != 'sk_id_curr' :\n","    #Iterate through the stat names\n","    for stat in bureau_agg.columns.levels[1][:-1] :\n","      #make a new column name for the variable and stat\n","      columns.append('bureau_%s_%s' %(var, stat))"],"metadata":{"id":"XWwK13ewuXkO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Assign the list of columns names a sthe df column names\n","bureau_agg.columns = columns\n","bureau_agg.head()"],"metadata":{"id":"IKp6_2vqusyk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Merge with the training data\n","train = train.merge(bureau_agg, on='sk_id_curr', how='left')\n","train.head()"],"metadata":{"id":"ddinK3Mgu0aF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Correlations of Aggregated Values with Target"],"metadata":{"id":"7Ee67shVu9zh"}},{"cell_type":"markdown","source":["We can calculate the correlation of all new values with the target. Again, we can use these as an approximation of the variables which may be important for modeling."],"metadata":{"id":"VZOracjfvB4i"}},{"cell_type":"code","source":["#list of new corrlations\n","new_corrs = []\n","\n","#Iterate through the columns\n","for col in columns :\n","  #Calculate correlation with the target\n","  corr = train['Target'].corr(train[col])\n","\n","  #Append the list as a tuple\n","\n","  new_corrs.append((col, corr))\n","  "],"metadata":{"id":"QTo28vyDvH1y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["IIn the code below, we sort the correlations by the magnitude (abs) using the sorted function. WE also make use of an anonymous lambda function"],"metadata":{"id":"v_Pi1AwWvZNi"}},{"cell_type":"code","source":["#sort the correlations by the abs values\n","#make sure to reverse to put the largest vlaues at the front of list\n","new_corrs = sorted(new_corrs, key= lambda x : abs(x[1]), reverse=True)\n","\n","new_corrs[:15]"],"metadata":{"id":"mHzCmNpYvo8_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["None of the new variables have a significant correlation with the TARGET. we can look at the kde plot of the highest correlated variable, bureau_days_credit_mean, with the target in terms of absolute magnitude correlation."],"metadata":{"id":"oRHyFddav5af"}},{"cell_type":"code","source":["lde_target('bureau_days_credit_mean', train)"],"metadata":{"id":"dxU_jRqQwD3f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The Multiple Comparisons problem"],"metadata":{"id":"kSZrDDm3wILQ"}},{"cell_type":"markdown","source":["##Function for Numeric Aggregations\n","\n","Let's encapsulate all of the previous work into a function. This will allow us to compute aggregate stats for numeric olumns accross any df. We will reuse this functino when we want to apply the same operations for other df."],"metadata":{"id":"G3Cox4mfwPPv"}},{"cell_type":"code","source":["def agg_numeric(df, group_var, df_name) :\n","  '''Aggregates the numeric values in a df. This can be used to create features for each instance of the grouping variable.\n","\n","  parameters :\n","  - df : the df to calculate the statistics on\n","  - group_var : the variable by whih to group df\n","  - df_name :the variable used to rename\n","\n","  return\n","  --------\n","  agg\n","  '''\n","  #Remove id variables other than grouping variable\n","  for col in df :\n","    if col != group_var and 'sk_id' in col :\n","      df=df.drop(columns=col)\n","\n","    group_ids = df[group_var]\n","    numeric_df = df.select_dtypes('number')\n","    numeric_df[group_var] = group_ids\n","\n","    #Group by the specified variable and calculate the statistics\n","    agg = numeric_df.groupby(group_var).agg(['count','mean','max','min','sum']).reset_index()\n","\n","    #Need to create new column names\n","    columns = [group_var]\n","\n","    #Iterate through the variables names\n","    for var in agg.columns.levels[0] :\n","      #Skip the gruping variable\n","      if var!= group_var :\n","        for stat in agg.columns.levels[1][:-1] :\n","          #make a new cols names for the variable and stat\n","          columns.append('%s %s %s' %(df_name, var, stat))\n","\n","    agg.columns = columns\n","    return agg"],"metadata":{"id":"FrpaEYrMwbT_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bureau_agg_new = agg_numeric(bureau.drop(columns= ['sk_id_bureau']), group_var = 'sk_id_curr', df_name = 'bureau')\n","bureau_agg_new.head()"],"metadata":{"id":"bWKYEg8MxjTP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Function to calculate corr with te target for a df\n","\n","def target_corrs(df) :\n","\n","  corrs= []\n","\n","  for col in df.columns :\n","    print(col)\n","    if col != 'target' :\n","      corr = df['target'].corr(df[col])\n","\n","      corrs.append(col, corr)\n","\n","  corrs = sorted(corrs, key=lambda x : abs(x[1]), reverse=True)\n","  #abs(x[1] : corrs는 append로 col(colname)과 corr(corr_value)를 받기 때문에 sort는 상관계수를 바탕으로 하는 것이 맞음)\n","  return corrs"],"metadata":{"id":"B-kMQ8aM6oec"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["categorical = pd.get_dummies(bureau.selec_dtypes('object'))\n","categorical['sk_id_curr'] = bureau['sk_id_curr']\n","categorical.head()"],"metadata":{"id":"mCxW-VNiPxwy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["categorical_grouped = categorical.groupby('sk_id_curr').agg['sum','mean']\n","categorical_grouped.head()"],"metadata":{"id":"XnLWr-IwP51l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["categorical_grouped.columns.levels[0][:10]\n","#levels[0] is row, levels[1] is cols"],"metadata":{"id":"CBhCp4PbQDqS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["categorical_grouped.columns.levels[1]"],"metadata":{"id":"v6NkWHyPRF-x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["group_var = 'sk_id_curr'\n","\n","#need to create new column names\n","columns = []\n","\n","#iterate through the variabel names\n","for var in categorical_grouped.columns.levels[0] :\n","  #skip the grouping variable\n","  if var != group_var :\n","    for stat in ['count','count_norm'] :\n","      #make a new column name for the varaible and stat\n","      columns.append( '%s %s' % (var,stat))\n","\n","#rename the columns\n","categorical_grouped.columns = columns\n","categorical_grouped.head()"],"metadata":{"id":"X-IN1MiCRZXo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train = train.merge(categorical_grouped, left_on = 'sk_id_curr', right_on = True, how = 'left')\n","\n","train.head()"],"metadata":{"id":"243yTZFsTj_-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train.shape()"],"metadata":{"id":"eHOHgrGDT0A4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train.iloc[:10, 123:]"],"metadata":{"id":"7Jmt5UCUT2Aa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Function to handle categorical variables"],"metadata":{"id":"8YvhwpqzT7zz"}},{"cell_type":"markdown","source":["to make the code more efficient, we can now write a function to handle he categorical variables for us. This will take the same form as the agg_numeric function in that it accepts a df and a grouping variable. Then it will calclate the counts and normalized counts of each category for all cateogrical variables in the df."],"metadata":{"id":"ZoaYp1qpUCZt"}},{"cell_type":"code","source":["def count_categorical(df, group_var, df_name) :\n","  ''' parameters\n","  - df : df to calculate the value counts for\n","  - group_var : the variable by which to group the df. for each unique value of this variable, the fianl df will have one row\n","  - df_name : variable added to the fron of column names to keep track of columns\n","  return\n","  -------\n","  categorical : a df with counts and normalized counts of each unique category in every categorical variable with one row for every unique value of the 'group_var'\n","  '''\n","\n","  #select the categorical columns\n","  categorical = pd.get_dummies(df.select_dtypes('object'))\n","\n","  #make sure to put the identifying id on the column\n","  categorical[group_var] = df[group_var]\n","\n","  #groupby the group var and calculate the sum and mean\n","  categorical = categorical.groupby(group_var).agg(['sum','mean'])\n","\n","  column_names = []\n","\n","  #iterate through the columns in level 0 :\n","  for var in categorical.columns.levels[0] :\n","    for stat in ['count','count_norm'] :\n","      #make a new column name\n","      column_names.append('%s %s %s' % (df_name,var,stat))\n","\n","\n","  categorical.columns = column_names\n","\n","  return categorical"],"metadata":{"id":"eGupomdOUPzN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bureau_counts = count_categorical(bureau, group_var = 'sk_id_curr', df_name = 'bureau')\n","brreau_count.head()"],"metadata":{"id":"5IZlnFazVl1o"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Applying Operations to another dataframe\n","\n","We will now turn to the bureau balance df. This df had monthly information about **each clinet's previous loans with other financial institutions**.\n","\n","Instead of grouping this df by the sk_id_curr which is the client id, we will first gorup the df by the sk_id-bureau which is the id o f the previous loan. This will give us ***one row*** of the df for each loan. Then, we can group by the sk_di_curr and calculate the aggregations across the loans of each client. The final result will be a df with one row for each client, with stats calculated for their loans."],"metadata":{"id":"g2AIKqI0Vv2j"}},{"cell_type":"code","source":["bureau_balance = pd.read_csv('../input/bureau_balance.cs')\n","\n","bureau_balance.head()"],"metadata":{"id":"KdvIulsfWPSP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bureau_balance_counts = count_categorical(bureau_balance, group_var = 'sk_id_bureau' df_name = 'bureau_balance')\n","bureau_balance_count.head()"],"metadata":{"id":"tRz189TgWu-w"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["now we can handle the one numeric column. the months_balance column has the 'months of balance relative to application date.' this might not necessarily be that important as a numeric variable, and in future work we might want to consider this as a time variable. For now, we cna just calculate the same aggregation statistics as previously."],"metadata":{"id":"v_tGn3BIXRzU"}},{"cell_type":"code","source":["#calculate value count tatistics for each sk_id_curr\n","bureau_balance_agg = agg_numeric(bureau_balance, group_var = 'sk_id_bureau', df_name = 'bureau_balance')\n","\n","bureau_balance_agg.head()"],"metadata":{"id":"L8_AxfIWXmDr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The above dataframes have the calculations done on each loan. Now we need to aggregate these for each client. We cna do this by merging the dataframes together first and then since all the variables are numeric, we just need to aggregate the statistics again, thsi timegrouping by the sk_id_curr"],"metadata":{"id":"aM4thP0vX6Zu"}},{"cell_type":"code","source":["#df grouped by the loan\n","bureau_by_loan = bureau_balance_agg.merge(bureau_balance_counts, right_index= True, left_on='sk_id_bureau', how='outer')\n","\n","#merge to include the sk_id_bureau\n","bureau_by_loan = bureau_by_loan.merge(bureau[['sk_id_bureau','sk_id_curr']], on='sk_id_bureau', how='left')\n","\n","bureau_by_loan.head()"],"metadata":{"id":"fVorD0esYJoU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bureau_balance_by_client = agg_numeric(bureau_by_loan.drop('sk_id_bureau', axis=1)), group_var = 'sk_id_curr', df_name = 'client')\n","bureau_balance_by_client.head()"],"metadata":{"id":"T1f5KVhIYyUe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["To recap, for the bureau_balance df we :\n","1. Calculate numeric stats grouping by each loan\n","2. Made value counts of each categorical variable goruping by loan\n","3. Merged the stats and the value counts on the loans\n","4. Calculated numeric stats for the resulting dataframe goruping by the client id\n","\n","The final resulting df has one row for each client, with statistics calculate dfor all of their loans with monthly balance inforamtion.\n","\n","Some of these variables are a little confusing, so let's try to explain a few :\n","\n","- client_bureau_balance_months_balance_mean_mean : \n","\n","for each loan calculate the mean value of months_balance. Then for each client, calculate the mean of this value for all of their loans.\n","- Client_bureau_balance_status_x_count_norm_sum ⁉\n","For each loan, Calculate #occurences of status==x diveded by #total status values for the loan. Then for each client, add up the values for each loan."],"metadata":{"id":"DdD-_byvZC-G"}},{"cell_type":"markdown","source":["#Putting the Functions Together\n","\n","\n"],"metadata":{"id":"JOnX0qDYZ_AR"}},{"cell_type":"markdown","source":["We now have all the pieces in place to take the infromation from the previous loans at other institutions and the monthly payments inforamtion abour these loans and put them into the main training df. Let's do a reset of all the variables and then use the functions we built to do this from the ground up. This demonstrate the benefit of using functinos for repeatable workflows"],"metadata":{"id":"x2G-_JQqaD0i"}},{"cell_type":"code","source":["#Free up memory by deleting old objects\n","import gc\n","gc.enable()\n","\n","def train, bureau, bureau_balance, bureau_agg, bureau_agg_new, bureau_balance_agg, bureau_balance_counts, bureau_by_loan, bureau_balance_by_client, bureau_counts\n","gc.collec()"],"metadata":{"id":"52kIm4mXaX1c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#read in new ciplies of all the df\n","train.pd_read_csv('../input/application_train.csv')\n","bureau = pd.read_csv('../input/bureau.csv')\n","bureau_balance = pd.read_csv('../input/bureau_balance.csv')\n"],"metadata":{"id":"lR0NdEcMbK-W"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Counts of Bureau DF"],"metadata":{"id":"G9jVtTw7bXjV"}},{"cell_type":"code","source":["bureau_counts = count_categorical(bureau, group_var ='sk_id_curr', df_name = 'bureau')\n","bureau_counts.head()"],"metadata":{"id":"47MNCOiJbXC4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Aggregated stats if bureau dataframe"],"metadata":{"id":"-brGEC45bkQp"}},{"cell_type":"code","source":["bureau_agg = agg_numeric(bureau.drop('sk_id_curr',axis=1), group_var='sk_id_curr', df_name = 'bureau')\n","bureau_agg.head()"],"metadata":{"id":"eby5NLRkboEr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Value counts of Bureau Balance df by loan"],"metadata":{"id":"PcYqfmevbw-v"}},{"cell_type":"code","source":["bureau_balance_counts = count_categorical(bureau_balance, group_var='sk_id_bureau', df_name = 'bureau_balance')\n","bureau_balance_counts.head()"],"metadata":{"id":"OHx21rnYbzbw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Aggregated stats of Bureau Balance df by client"],"metadata":{"id":"2V_sbLaOb8lL"}},{"cell_type":"code","source":["#df grouped by the loan\n","bureau_by_loan = bureau_balance_agg.merge(bureau_balance_counts, right_index=True, left_on='sk_id_bureau', how='outer')\n","\n","#merge to include the sk_id_curr\n","bureau_by_loadn = bureau[ ['sk_id_curr', 'sk_id_bureau']].merge(bureau_by_loan, on='sk_id_bureau',how='left')\n","\n","#Aggregate the stats for each client\n","bureau_balance_by_client = agg_numeric(bureau_by_loan.drop('sk_id_bureau', axis=1), group_var='sk_id_curr', df_name='client')\n","\n"],"metadata":{"id":"M_ETveQLcBSt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Insert Computed Features into Training Data"],"metadata":{"id":"COYt99j6fcBL"}},{"cell_type":"code","source":["original_features = list(train.columns)\n","print('original number of features : ', len(original_features))"],"metadata":{"id":"AtG_3n3mfbV4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#merge with the value counts of bureau\n","train = train.merge(bureau_counts, on = 'sk_id_curr', how='left')\n","\n","#merge with the stats of bureau\n","train = train.merge(bureau_agg, on='sk_id_curr', how='left')\n","\n","#merge with the monthly information grouped by client\n","train = train.mege(burea_balance_by_client, on='sk_id_curr', how='left') "],"metadata":{"id":"sv-2gGNbflcV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["new_features = list(train.columns)\n","print('# features using previous loans from other institutions data : ', len(new_features))"],"metadata":{"id":"dxc29Wm1umfg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Feature Engineering Outcomes\n","\n","After all that work, now we want to take a look at the variables we ahve created. We can look at the **percentage of missing values**, the **correlations **fo variables with the target, and also the correlation of variables with the other variables. The correlations btw variables acan show if we have collinear variable, that is, variables that are highly correlated with one another. Often, we want to remove one in a pair of collinear variables because having both variables would be redundant. We cna also use the percentage of missing values to remove features with a subtrantial majority of values that are not present. Feature selection will be an important focus going forward, because reducing the number of fetures can help the model learn during training and also generalize better to the testing data. The 'curse of dimensionality' is the name given to the issues caused by having too many features ( too high of a dimension). As the number of variables increases, the number of datapoints needed to learn the relationship between these variables and the target value increases exponentially."],"metadata":{"id":"8Fxatzc8uuF0"}},{"cell_type":"markdown","source":["##Missing values\n","\n","An important consideration is the missing values in the df. Columns with too many missing values might have to be dropped."],"metadata":{"id":"3j2gxwNVyRBW"}},{"cell_type":"code","source":["# Function to calculate missing values by column # Funct\n","\n","def missing_values_table(df) :\n","  mis_val = df.isnull().sum()\n","\n","  mis_val_percent = (mis_val / len(df)) * 100\n","\n","  mis_val_table = pd.concat([mis_val, mis_val_percent], aixs=1)\n","\n","  #Rename the columns\n","  mis_val_table_ren_columns = mis_val_table.rename({0 : 'Missing Values', 1 : '% of Total values'})\n","\n","  #sort the table by percentage of missing descending\n","  mis_val_table_ren_columns = mis_val_table_ren_columns[mis_val_table_ren_column.iloc[:,1] !=0].sort_values('% of total values', ascending=False).round(1)\n","\n","  #print some summary information\n","  print ('Your selected df has' + str(df.shape[1] + 'columns' + 'There are' + str(mis_val_table_ren_columns.shape[0] + 'columns that have missing values'))\n","\n","  return mis_val_table_ren_columns"],"metadata":{"id":"VVs6RJxSyXxc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["missing_train = missing_values_trable(train)\n","\n","missing_train"],"metadata":{"id":"pUDjFu_8zUBC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["missing_train_vars = list(missing_train.index[missing_train['% of total values'] > 90])\n","\n","len(missing_train_vars)"],"metadata":{"id":"wpq0fLm6zZLq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Calculate information for testing data"],"metadata":{"id":"izP4RBwYzigp"}},{"cell_type":"code","source":["test = pd.read_csv('../input/application_test.csv')\n","\n","#merge with the value counts of bureau\n","test = test.merge(bureau_counts, on = 'sk_id_curr', how='left')\n","\n","#merge with the stats of bureau\n","test = test.merge(bureau_agg, on='sk_id_curr', how='left')\n","\n","#merge with the value counts of bureau balance\n","test = test.merge(bureau_balance_by_client, on='sk_id_curr', how='left')"],"metadata":{"id":"Y-LOymYKzj5D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('shape of testing data : ', test.shape)"],"metadata":{"id":"6O3zPJ-Fz7x3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_labels = train['target']\n","\n","train, test = train.align(test, join='inner', axis=1)\n","\n","train['target'] = train_labels"],"metadata":{"id":"k8Tg9EIG0ALT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["missing_test = missing_values_table(test)\n","missing_test.head()"],"metadata":{"id":"Vp3e2fku0J07"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["missing_test_vars = list(missing_test_vars.index[missing_test['% of total values'] >90 ])\n","\n","len(missing_test_vars)"],"metadata":{"id":"3uJKF-fO0Peq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["missing_columns = list(set(missing_test_vars + missing_train_vars))\n","print('There are %d columns with more than 90%% missing in either the train or testing data.' % len(missing_columns))"],"metadata":{"id":"GjwkKPKE0XEm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#drop the missing columns\n","train = train.drop('missing_columns', axis=1)\n","test = test.drop('missing_columns', axis=1)"],"metadata":{"id":"Aofum3rs0iZR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Correlations\n","\n"],"metadata":{"id":"LAhl3dFW04ex"}},{"cell_type":"code","source":["corrs = train.corr()"],"metadata":{"id":"on5vHZuM06nN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["corrs = corrs.sort_values('target', ascending=False)\n","\n","#Ten most positive correaltions\n","pd.DataFrame(corrs['target'].head(10))"],"metadata":{"id":"YbOATdt708PE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Tem most negative correlations\n","pd.DataFrame(corrs[['target'].tail(10)])"],"metadata":{"id":"G2fhlKi11EDh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The highest correlated variable with the target, is a variable we created. however, just because the variable is correlated does not mean that it will be useful, and we have to rememver that if we generate hundreds of new variables, some are going to be correlated with the target simply because of random noise.\n","\n","Viewing the correlations skeptically, it does appear that several of the newly created variables may be useful. To assess the 'usefullness' of variables, we will look at the feature importances rerturned by the model."],"metadata":{"id":"K0uJg9gT1NAv"}},{"cell_type":"code","source":["kde_target(var_name = 'client_bureau_balance_counts_mean', df=train)"],"metadata":{"id":"WskDr7Hi1mWS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This variable represents the average number of monthly records per loan for each client. Based on the distribution, clients witha greater number of average monthly records per loan were more likely to repay their loans with home credit."],"metadata":{"id":"kd5grtMp1v3b"}},{"cell_type":"code","source":["kde_target(var_name='bureau_CREDIT_ACTIVE_Active_count_norm', df=train)"],"metadata":{"id":"z6taQt6w19op"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###collinear Variables\n","\n","We can calculate not only the correlations of the variables with the target, but also the correlation of each variable with every other variable. This will allow us to see if there are highly collinear variables that should perhaps be removed from the data."],"metadata":{"id":"LoA_N7X32e8y"}},{"cell_type":"code","source":["#set the threshold\n","threshold= 0.8\n","\n","above_threshold_vars= {}\n","\n","for col in corrs :\n","  above_threshold_vars[col] = list(corrs.index[corrs[col] > threshold])"],"metadata":{"id":"qSJR54po2sNY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#track columns to remove and columns already examined\n","cols_to_remove = []\n","cols_seen = []\n","cols_to_remove_pair=[]\n","\n","#iterate through columns and correalted columns\n","for key, value in above_threshold_vars.items() :\n","  cols_seen.append(key)\n","  for x in value :\n","    if x == key :\n","      next\n","    else:\n","      #only want to remove one in a pair\n","      if x not in cols_seen :\n","        cols_to_remove.append()\n","        cols_to_remove_pair.append(key)\n","\n","cols_to_remove = list(set(cols_to_remove))\n","\n","print('# columns to remove', len(cols_to_remove))"],"metadata":{"id":"UPhkrOrd26OS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_corrs_removed = train.drop('cols_to_remove', axis=1)\n","test_corrs_removed = test.drop('cols_to_remove', axis=1)\n","\n"],"metadata":{"id":"MqDmt3Lk3bvp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_corrs_removed.to_csv('train_bureau_corrs_removed.csv', index=False)\n","test_corrs_removed.to_csv('test_bureau_corrs_removed.csv', index=False)"],"metadata":{"id":"cAkDgRZN3kCO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Modeling\n","\n","To actually test the performance of these new datasets, we will try using them for machinelearning! \n","\n","For all datasets, use the model shown below\n","- control : only the data in the application files.\n","- test one : the data in the application files with all of the data recorded from the bureau and bureau_balance fiels\n","- test two : the data in the application files with all of the data recorded from the bureau and bureau_balance files with highly correlated variables removed."],"metadata":{"id":"KRSLqJpE3ryV"}},{"cell_type":"code","source":["import lightgbm as lgb\n","from sklearn.model_selection import KFold\n","from sklearn.metrics import roc_auc_score\n","from sklearn.preprocessing import LabelEncoder\n","\n","import gc\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline"],"metadata":{"id":"PNgnDrOR4J8O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def model(features, test_features, encoding='ohe', n_folds=5):\n","  '''Train and test a light gradient boosting model using cross validation.'''\n","  #Extract the ids\n","  train_ids = features['sk_id_curr']\n","  test_ids = test_features['sk_id_curr']\n","\n","  #Extract the labels for training\n","  labels = train['target']\n","\n","  #Remove the ids and target\n","  features = features.drop(['sk_id_curr','target'])\n","  test_features = test_features.drop(['sk_id_curr'])\n","\n","  #one hot encoding\n","  if encoding == 'ohe' :\n","    features = pd.get_dummies(features)\n","    test_features = pd.get_dummies(test_features)\n","\n","    #Align the df by the columns\n","    features, test_features = features.align(test_features, join='inner', axis=1)\n","\n","    #No categorical indices to records\n","    cat_indices = 'auto'\n","  elif encoding =='le' :\n","    label_encoder = LabelEncoder()\n","\n","    #List for storing categorical indices\n","    cat_indices = []\n","\n","    #Iterate through each column\n","    for i, col in enumerate(features) :\n","      if features[col].dtype =='object' :\n","        #map the categorical features to integers\n","        features[col] = label_encoder.fit_transform(np.array(features[col].astype(str)).reshape((-1,)))\n","\n","        #record the categorical indices\n","        cat_indices.append(i)\n","\n","      else :\n","        raise ValueError('Encoding must be either 'ohe' or 'le' ')\n","\n","  print('Training data shape' : features.shape)\n","  print('test data shape' : test_features.shape)\n","\n","  #Convert to np arrays\n","  features = np.array(features)\n","  test_features = np.array(test_features)\n","\n","  #Create the kfold object\n","  k_fold = KFOLD(n_splits=n_folds, shuffle=False, random_state = 42)\n","\n","  #empty array for feature importances\n","  feature_importance_values = np.zeros(len(features_names))\n","\n","  #empty array for test prediction\n","  test_predictions = np.zeros(len(test_features.shape[0]))\n","\n","  #empty array for out of fold validation predictions\n","  out_of_fold = np.zeros(features.shape[0])\n","\n","  #Lists for recording validation and training scores\n","  valid_scores=[]\n","  train_scores=[]\n","\n","  for train_indices, valid_indices in k_fold.split(features) :\n","\n","    #Training data for the fold\n","    train_features, train_labels = features[train_indices], labels[traub_indices]\n","    #Validation data for the fold\n","    valid_features, valid_labels = features[valid_indices], labels[valid_indices]\n","\n","    #Create the model\n","    model = lgb.LGBMClassifier(n_estimators=100, objective = 'binary', class_weight = 'balanced', learning_rate = 0.05, reg_alpha = 0.1, reg_lambda = 0.1, random_state=42)\n","\n","    #train the model\n","    model.fit(train_features, train_labels, eval_metric ='auc', eval_set=[(train_features, train_labels),(valid_features, valid_labels)], eval_name=['train','valid'], categorical_feature = cat_indices, early_stopping_rounds = 100, verbose = 100)\n","\n","    #Record the best iteratino\n","    best_iteration = model.best_iteration_\n","\n","    #Records the feature importances\n","    feature_importance_values += model.feature_importances_ / k_fold.n_splits\n","\n","    #Make predictions\n","    test_predictions +=model.predict_proba(test_features, num_iteration = best_iteration)[:,1] / k_fold.n_splits\n","\n","    #Records the out of fold predictions\n","    out_of_fold[valid_indices] = model.predict_proba(valid_features, num_iteration = best_iteration)[:,1]\n","\n","    #Record the best score\n","    valid_score = model.best_score_['valid']['auc']\n","    train_score = model.best_score_['train']['auc']\n","\n","    valid_scores.append(valid_score)\n","    train_scores.append(train_score)\n","\n","    #clean up memory\n","    gc.enable()\n","\n","    del model, train_features, valid_features\n","    gc.collect()\n","\n","    #make the submission df\n","    submission = pd.DataFrame({'sk_id_curr' : test_ids, 'target' : test_predictions})\n","\n","    #make the feature importance df\n","    feature_importances = pd.DataFrame({'feature' : feature_names, 'importance' : feature_importance_values})\n","\n","    #overall validation score\n","    valid_auc = roc_auc_score(labels, out_of_fold)\n","\n","    #add the overall scores to the metrics\n","    valid_scores.append(valid_auc)\n","    train_scores.append(np.mean(train_scores))\n","\n","    #needed for creating df of validation scores\n","    fold_names = list(range(n_folds))\n","    fold_names.append('overall')\n","\n","\n","    #df of validation scores\n","    metrics = pd.DataFrame({'fold' : fold_names,\n","                            'train' : train_scores,\n","                            'valid' : valid_scores})\n","    \n","    return submission, feature_importances, metrics"],"metadata":{"id":"bMgdVaK04V5A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plot_feature_importances(df) :\n","  df = df.sort_values('importance', ascending=False).reset_index()\n","\n","  df['importance_normalized'] = df['importance'] / df['importance'].sum()\n","\n","  #make a barh of feature importances\n","  plt.figure(figsize=(10,6))\n","  ax= plt.subplot()\n","\n","  ax.barh(list(reversed(list(df.index[:15]))),\n","          df['importance_normalized'].head(15),\n","          align = 'center', edgecolor = 'k')\n","  \n","  #Set the yticsk and labels\n","  ax.set_yticks(list(reversed(list(df.index[:15]))))\n","  ax.set_yticklabels(df['feature'].head(15))\n","\n","  #plot labeling\n","  plt.xlabel('normalized importance')\n","  plt.title(' feature importance')\n","  plt.show()\n","\n","  return df"],"metadata":{"id":"1RstQS-39lYr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Control\n","\n","The first step in any experiment is establishing a control. For this we will use the function defined above(that implements a Gradient Boosting machine model) and the single main data souce(application)"],"metadata":{"id":"w24muzqL-Vh7"}},{"cell_type":"code","source":["train_control = pd.read_csv('../input/application_train.csv')\n","test_control = pd.read_csv('../input/application_test.csv')"],"metadata":{"id":"HM9z-upt-gbe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["submission, fi , metrics = model(train_control, test_control)"],"metadata":{"id":"Lao1iCYH-ncs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["metrics"],"metadata":{"id":"Z6m8E07k-rrQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fi_sorted = plot_feature_importances(fi)"],"metadata":{"id":"7gIv_Urd-s3s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["submission.to_csv('control.csv', index=False)"],"metadata":{"id":"roBcz0Tq-v9G"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Test One"],"metadata":{"id":"81DEUUnD-zg4"}},{"cell_type":"code","source":["submission_raw, fi_raw, metrics_raw = model(train, test)"],"metadata":{"id":"tAuiT4tt-y2A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["metrics_raw"],"metadata":{"id":"TpQmNQmz-3qI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fi_raw_sorted = plot_feature_importances(fi_raw)"],"metadata":{"id":"CH2unSJW-49r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["top_100 = list(fi_raw_sotred['feature'])[:100]\n","new_features = [x for x in top_100 if x not in list(fi['feature'])]\n","\n","print('%% of top 100 features created from the bureau data = %d.00' %len(new_features))"],"metadata":{"id":"eGUrmFOb-8Xe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["submission_raw.t"],"metadata":{"id":"H59N8_im_MB1"},"execution_count":null,"outputs":[]}]}