{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNlZGTsyNssszSHFkSgKWgr"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#Introduction\n","\n","1. load the data\n","2. data categorization\n","3. modeling"],"metadata":{"id":"fWg95mvNJzLy"}},{"cell_type":"markdown","source":["#1. load the data"],"metadata":{"id":"p74bemJXJ5iz"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"uffCBN5eJwzQ"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import os\n","from pathlib import Path\n","import json\n","\n","from datetime import timedelta\n","from tqdm.notebook import tqdm\n","from collection import Counter\n","from heapq import nlargest\n","\n","#그림 그리기\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","%matplotlib inline\n","sns.set_theme()\n","\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","source":["data_path = Path(\"/kaggle/input/otto-recommender-system\")\n","\n","train= data_path/'train.jsonl'\n","test= data_path/'test.jsonl'\n","\n","sample_sub = Path(\"/kaggle/input/otto-recommender-system/sample_submission.csv\")"],"metadata":{"id":"ETngYJ2RJ8Ki"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open(train,'r') as f :\n","  print(f'total lines in train data is : {len(f.readlines())}')\n","  "],"metadata":{"id":"QecQqj1PJ83q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["chunk session for train"],"metadata":{"id":"1ghPZ3l_AdFh"}},{"cell_type":"code","source":["def read_jsonl(target: str) -> pd.DataFrame():\n","    sessions = pd.DataFrame()\n","    chunks = pd.read_json(data_path / f'{target}.jsonl', lines=True, chunksize=100_000)\n","\n","    for e, chunk in enumerate(chunks):\n","        event_dict = {\n","            'session': [],\n","            'aid': [],\n","            'ts': [],\n","            'type': [],\n","        }\n","        if e < 2:\n","            for session, events in zip(chunk['session'].tolist(), chunk['events'].tolist()):\n","                for event in events:\n","                    event_dict['session'].append(session)\n","                    event_dict['aid'].append(event['aid'])\n","                    event_dict['ts'].append(event['ts'])\n","                    event_dict['type'].append(event['type'])\n","            chunk_session = pd.DataFrame(event_dict)\n","            sessions = pd.concat([sessions, chunk_session])\n","        else:\n","            break\n","    return sessions.reset_index(drop=True)\n","\n","train_sessions = read_jsonl('train')\n","test_sessions = read_jsonl('test')"],"metadata":{"id":"nxbi3rZfKMqm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"rWeepD5yKeSS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#2. categorization"],"metadata":{"id":"uP1D2qFlLjr5"}},{"cell_type":"code","source":["#extract info from each session and make it to the df\n","\n","action_counts_list, article_id_list, session_length_ts_list, session_length_action_list = [[] for i in range(4)]\n","\n","overall_action_counts = {}\n","overall_article_id_counts = {}\n","\n","for i, row in tqdm(sample_df_train_df.iterrows(), total=len(sample_df_train_df)) :\n","  actions = row['events']\n","\n","  action_counts = {}\n","  article_id_counts = {}\n","\n","  action in actions :\n","  action_counts[action['type']] = action_counts.get(action['type'],0) +1\n","  article_id_counts[action['aid']] = article_id_counts.get(action['aid'],0) +1\n","\n","  overall_action_counts[action['type']] = overall_action_counts.get(action['type'],0) +1\n","  overall_article_id_counts[action['aid']] = overall_article_id_counts.get(actio['aid'],0) +1\n","\n","  session_length_time = action['ts'][-1] - action['ts'][0]\n","\n","  #add to list\n","  action_counts_list.append(action_counts)\n","  article_id_list.append(article_id_counts)\n","  session_length_ts_list.append(session_length_time)\n","  session_length_action_list.append(len(actions))\n","\n","sample_train_df['action_counts'] = action_counts_list\n","sample_train_df['article_id_list'] =article_id_list\n","sample_train_df['session_length_time'] =session_length_time_list\n","sample_train_df['session_length_hours'] = sample_train_df['session_length_time']*2.77778e-7\n","sample_train_df['session_length_action'] = session_length_action_list"],"metadata":{"id":"iyr8D8cQKtjt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#3. word2vec"],"metadata":{"id":"FloSkPk1LnV3"}},{"cell_type":"code","source":[],"metadata":{"id":"kM0kx9ch_inv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from nvtabular import *\n","from merlin.schema.tags import Tags\n","import polars as pl\n","import xgboost as xgv\n","\n","from merlin.core.utils import Distributed\n","from merlin.models.xgb import XGBoost\n","from nvtabular.ops import AddTags"],"metadata":{"id":"fus46BhKLoF7"},"execution_count":null,"outputs":[]}]}