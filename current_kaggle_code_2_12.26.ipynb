{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMCVc3aMKHAC4MeOjk6hgSW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["1. predict submission1 with word2vec\n","2. predict submission2 with DeepFM \n","3. combine recommender 20 items in the submission1 and submission2\n"],"metadata":{"id":"evd3F-Lnkfrx"}},{"cell_type":"markdown","source":["# 1.load lib and data"],"metadata":{"id":"fY2MnM3tkhHi"}},{"cell_type":"markdown","source":["due to memory problem, i'll use parquet file by @RADEK OSMULSKI "],"metadata":{"id":"qmS8Q7FmkjGj"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"8AWbCSQhkbdb"},"outputs":[],"source":["!pip install polars\n","!pip install pickle5"]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import os\n","from pathlib import Path\n","import json\n","import polars as pl\n","\n","from datetime import timedelta\n","from tqdm.notebook import tqdm\n","from collections import Counter\n","from heapq import nlargest\n","\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","%matplotlib inline\n","sns.set_theme()\n","\n","import warnings\n","warnings.filterwarnings('ignore')\n"],"metadata":{"id":"TMBitxsIklbZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# #paths\n","\n","# data_path = Path(\"/kaggle/input/otto-recommender-system\")\n","# train_df = data_path/'train.jsonl'\n","# test_df = data_path/'test.jsonl'\n","# sample_sub_path = Path('/kaggle/input/otto-recommender-system/sample_submission.csv')\n"],"metadata":{"id":"C2b-y4bSkm4O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train = pd.read_parquet('../input/otto-full-optimized-memory-footprint/train.parquet')\n","test = pd.read_parquet('../input/otto-full-optimized-memory-footprint/test.parquet')\n","\n"],"metadata":{"id":"oq39hhQkkm72"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pickle5 as pickle\n","\n","with open('../input/otto-full-optimized-memory-footprint/id2type.pkl', \"rb\") as fh:\n","    id2type = pickle.load(fh)\n","with open('../input/otto-full-optimized-memory-footprint/type2id.pkl', \"rb\") as fh:\n","    type2id = pickle.load(fh)"],"metadata":{"id":"uSBTSz12km_K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_type_as_string = train.iloc[:10000000].type.map(lambda i: id2type[i])\n","type_as_string.head()\n","\n","test_type_as_string = test.iloc[:5015409].type.map(lambda i : id2type[i])\n"],"metadata":{"id":"o68qta2jknCh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train['type_id'] = train_type_as_string\n","test['type_id'] = test_type_as_string"],"metadata":{"id":"lQuYzaJ2knFY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train = train[:len(train_type_as_string)]\n","test = test[:len(test_type_as_string)]"],"metadata":{"id":"2ClulGMrknH8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train['type_id'].value_counts()"],"metadata":{"id":"Lp35fZMRknKh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#let's make test data"],"metadata":{"id":"43uvpPrvk0HR"}},{"cell_type":"code","source":["train_sample = train[:1000000]\n","test_sample = test[:1000000]"],"metadata":{"id":"nn4Vy93gknNA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 1-1.json to DF\n","\n","reference of read data :\n","\n","https://www.kaggle.com/code/columbia2131/otto-read-a-chunk-of-jsonl-to-manageable-df?scriptVersionId=109793998 & https://www.kaggle.com/code/takaito/otto-word2vec-tutorial\n","\n","- due to memory problem, i will use parquet file for data analyze\n","- how much faster parquet than pandas? \n","- check here https://dadosdadosdados.wordpress.com/2015/12/30/benchmarking-csv-vs-parquet/ "],"metadata":{"id":"ZQPbwj4Ik3JA"}},{"cell_type":"code","source":["# def read_jsonl(target: str) -> pd.DataFrame():\n","#     sessions = pd.DataFrame()\n","#     chunks = pd.read_json(data_path / f'{target}.jsonl', lines=True, chunksize=1000000)\n","\n","#     for e, chunk in enumerate(chunks):\n","#         event_dict = {\n","#             'session': [],\n","#             'aid': [],\n","#             'ts': [],\n","#             'type': [],\n","#         }\n","#         if e < 2:\n","#             for session, events in zip(chunk['session'].tolist(), chunk['events'].tolist()):\n","#                 for event in events:\n","#                     event_dict['session'].append(session)\n","#                     event_dict['aid'].append(event['aid'])\n","#                     event_dict['ts'].append(event['ts'])\n","#                     event_dict['type'].append(event['type'])\n","#             chunk_session = pd.DataFrame(event_dict)\n","#             sessions = pd.concat([sessions, chunk_session])\n","#         else:\n","#             break\n","#     return sessions.reset_index(drop=True)\n","# train_sessions = read_jsonl('train') \n","# test_sessions = read_jsonl('test') "],"metadata":{"id":"Ro4CHvYZknPj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# type2id = {'clicks' :0, \"carts\" : 1, 'orders' : 2}\n","\n","# train_sessions['type2id'] = train_sessions['type'].apply(lambda x : type2id[x])\n","# test_sessions['type2id'] = test_sessions['type'].apply(lambda x : type2id[x])"],"metadata":{"id":"TtgoNs3hknSH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 1-2.calculating the scores used for co-visitation matrix\n","- this variable could be use for lgbm or xgboost\n","\n","reference : https://www.kaggle.com/code/radek1/co-visitation-matrix-simplified-imprvd-logic, https://www.kaggle.com/code/alberteinsten/cudf-pandas-proof-of-concept-lgbm-ranker"],"metadata":{"id":"ccjFVWP9k-CR"}},{"cell_type":"code","source":["# def add_session_length(df) :\n","#     df['session_length'] = train_sessions.groupby('session')['ts'].transform('count')\n","#     return df\n","\n","# def add_action_num_reverse_chrono(df):\n","#     df['action_num_reverse_chrono'] = df.session_length - df.groupby('session').cumcount() - 1\n","#     return df\n","\n","# def add_log_recency_score(df):\n","#     linear_interpolation = 0.1 + ((1-0.1) / (df['session_length']-1)) * (df['session_length']-df['action_num_reverse_chrono']-1)\n","#     df['log_recency_score'] = (2 ** linear_interpolation - 1).fillna(1.0)\n","#     return df\n","\n","# def add_type_weighted_log_recency_score(df):\n","#     type_weights = {0:1, 1:6, 2:3}\n","#     df['type_weighted_log_recency_score'] = df['log_recency_score'] / df['type2id'].map(type_weights)\n","#     return df\n","\n","# def apply(df, pipeline) :\n","#     for f in pipeline :\n","#         df = f(df)\n","    \n","#     return df\n"],"metadata":{"id":"g9oUQvxKknUl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# pipeline = [add_session_length, add_action_num_reverse_chrono, add_log_recency_score, add_type_weighted_log_recency_score]\n","# apply(train_sessions, pipeline)\n","# apply(test_sessions, pipeline)"],"metadata":{"id":"_lniomTpknXE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# train_sessions['gt']=0\n","# test_sessions['gt'] =1"],"metadata":{"id":"HCbZg61hknZp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# train = train_sessions.merge(test_sessions, on=['session','ts','type', 'aid','type2id','session_length','action_num_reverse_chrono','log_recency_score','type_weighted_log_recency_score'], how='left')\n","\n","# train['gt'] = train['gt'].fillna(0)\n","# train = train.sort_values('session').reset_index(drop=True)"],"metadata":{"id":"saZ4UvhlkncK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["raw_corpus = []\n","for session, group_df in tqdm(train_sample.groupby(['session'])):\n","    raw_corpus.append(list(group_df['aid'].astype(str)))\n","\n","for session, group_df in tqdm(test_sample.groupby(['session'])):\n","    raw_corpus.append(list(group_df['aid'].astype(str)))"],"metadata":{"id":"TJ5oDwYrknef"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(raw_corpus)"],"metadata":{"id":"JotGzCL0lHqa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 2.Word2Vec\n","\n","documentation : https://radimrehurek.com/gensim/models/word2vec.html\n","reference : https://www.kaggle.com/code/takaito/otto-word2vec-tutorial"],"metadata":{"id":"QfxaqzU5lJgP"}},{"cell_type":"markdown","source":["## create raw_corpus to train dataset of Word2vec\n","\n","referece : https://www.kaggle.com/code/takaito/otto-word2vec-tutorial"],"metadata":{"id":"2-e1YXiolLjI"}},{"cell_type":"code","source":["raw_corpus = []\n","for session, group_df in tqdm(train_sample.groupby(['session'])):\n","    raw_corpus.append(list(group_df['aid'].astype(str)))\n","\n","for session, group_df in tqdm(test_sample.groupby(['session'])):\n","    raw_corpus.append(list(group_df['aid'].astype(str)))"],"metadata":{"id":"b7DL3GsPlHtc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(raw_corpus)"],"metadata":{"id":"qQuyswmhlHwI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["import library for Word2Vec"],"metadata":{"id":"Ib4LdEivlQe_"}},{"cell_type":"code","source":["import hashlib\n","import os\n","from gensim.models import Word2Vec\n","from gensim.models import KeyedVectors\n","os.environ[\"PYTHONHASHSEED\"] = str(42)\n","def hashfxn(x):\n","    return int(hashlib.md5(str(x).encode()).hexdigest(), 16)"],"metadata":{"id":"0iclBZMllHye"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Train w2v model\n","\n","vector_size : Dimensionality of the word vectors\n","\n","window : Maxmum distance\n","\n","min_count : Ignores all words with total frequency lower than this\n","\n","sg : 1 for skip / otherwise cbow\n","\n","i tried min_count 3, number 59625 not predicted in word2vec\n","when tried min_count 2, number 1024433 not predicted in word2vec\n","\n","** with skip gram, it reduces error(prediction with Word2vec under 20) 5% to 2%."],"metadata":{"id":"URu1SgwrlUsX"}},{"cell_type":"code","source":["w2vec =Word2Vec(sentences=raw_corpus, \n","                vector_size=100, window=5, \n","                min_count=1, sg=1, workers=-1, \n","                seed=42, hashfxn=hashfxn)"],"metadata":{"id":"vJKMrlp8lYSx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["w2vec.save(\"word2vec.model\")"],"metadata":{"id":"8As0s8dLlay3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = Word2Vec.load(\"word2vec.model\")"],"metadata":{"id":"l-99B8gJla1f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Through AnnoyIndex function, we can measure similarity and find neighbors between train representation of our aid to create a submission.\n","\n"],"metadata":{"id":"yQzI-Tj2lc35"}},{"cell_type":"markdown","source":["Instance Attibute\n","AnnoyIndex(n_features, metric)\n","\n","Instance Method\n","add_item(i,v)\n","- add item to be indexed\n","build(n_trees, n_jobs)"],"metadata":{"id":"CFl1w-qjlgMZ"}},{"cell_type":"code","source":["%%time\n","\n","from annoy import AnnoyIndex\n","\n","aid2idx = {aid: i for i, aid in enumerate(w2vec.wv.index_to_key)}\n","index = AnnoyIndex(100, 'euclidean')\n","\n","for aid, idx in aid2idx.items():\n","    index.add_item(idx, w2vec.wv.vectors[idx])\n","    \n","index.build(10,n_jobs=-1)"],"metadata":{"id":"93gQ8flOljP_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# check how key and values matched in it.\n","aid2idx"],"metadata":{"id":"PdQG_JJJljS8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_aids = test_sample.groupby('session')['aid'].apply(list)\n","test_types = test_sample.groupby('session')['type'].apply(list)"],"metadata":{"id":"wC0pa0OtljVr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_aids"],"metadata":{"id":"Wb_-eXqWljYS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_types"],"metadata":{"id":"e8DixFy8ljaM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["this format referenced by @radek.\n","https://www.kaggle.com/code/radek1/word2vec-how-to-training-and-submission\n","\n","defaultdict\n"],"metadata":{"id":"hyIbX5kxlsPF"}},{"cell_type":"code","source":["from collections import defaultdict\n","\n","labels = []\n","type_weight_multipliers = {0: 1, 1: 6, 2: 3}\n","\n","for AIDs, types in zip(test_aids,test_types):\n","#     print(AIDs,types)\n","    if len(AIDs) >= 20:\n","        # if we have enough aids (over equals 20) we don't need to look for candidates! we just use the old logic\n","        weights=np.logspace(0.1,1,len(AIDs),base=2, endpoint=True)-1\n","        aids_temp=defaultdict(lambda: 0)\n","        for aid,w,t in zip(AIDs,weights,types): \n","            aids_temp[aid]+= w * type_weight_multipliers[t]\n","            \n","        sorted_aids=[k for k, v in sorted(aids_temp.items(), key=lambda item: -item[1])]\n","        labels.append(sorted_aids[:20])\n","    else:\n","        # here we don't have 20 aids to output -- we will use word2vec embeddings to generate candidates!\n","        AIDs = list(dict.fromkeys(AIDs[::-1]))\n","        print(AIDs)\n","        # let's grab the most recent aid\n","        most_recent_aid = AIDs[0]\n","        print(most_recent_aid)\n","        # and look for some neighbors!\n","        nns = [w2vec.wv.index_to_key[i] for i in index.get_nns_by_item(aid2idx[most_recent_aid], 21)[1:]]\n","                        \n","        labels.append((AIDs+nns)[:20])"],"metadata":{"id":"oqTjZ_CBltWJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# check the labelling is done well\n","\n","cnt =0\n","for i in range(len(labels)) :\n","    if len(labels[i]) < 20 :\n","        cnt+=1\n","\n","(cnt /len(labels) )*100"],"metadata":{"id":"Pe8RnROWlvXw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["the error is 2.26%, with CBOW, error was 5.xx%"],"metadata":{"id":"iylNb-_OlyKQ"}},{"cell_type":"markdown","source":["let's make submission files for word2vec"],"metadata":{"id":"r1dU2qCclzl-"}},{"cell_type":"code","source":["session_types = ['clicks','carts','orders']\n","labels_as_strings = [' '.join([str(l) for l in lls]) for lls in labels]\n","\n","predictions = pd.DataFrame(data={'session_type': test_aids.index, 'labels': labels_as_strings, 's_type' : 0})\n","\n","prediction_dfs = []\n","\n","type2id = {'clicks' :0, \"carts\" : 1, 'orders' : 2}\n","\n","\n","for st in session_types:\n","    prediction = predictions.copy()\n","    prediction.session_type = prediction.session_type.astype('str') + f'_{st}'\n","    prediction.s_type = st\n","    prediction.s_type = prediction.s_type.apply(lambda x : type2id[x])\n","    prediction_dfs.append(prediction)\n","    \n","w2v_submission = pd.concat(prediction_dfs).reset_index(drop=True)\n","w2v_submission"],"metadata":{"id":"flZchwvsl0i6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["w2v_submission = w2v_submission.sort_values(['session_type', 's_type'], ascending=True)\n","w2v_submission"],"metadata":{"id":"QF7Pc7XSl1xI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 3. FM\n","\n","references are here, but since i'm korean, there are some docs written in korean.\n","\n","https://deepctr-doc.readthedocs.io/en/latest/Features.html#sparsefeat\n","\n","https://deepctr-doc.readthedocs.io/en/latest/Examples.html\n","\n","https://projectlog-eraser.tistory.com/41\n","\n","https://arxiv.org/pdf/1703.04247v1.pdf\n","\n","https://aplab.tistory.com/entry/%EC%B6%94%EC%B2%9C-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98-%ED%83%90%EC%83%89-Deep-FM-%EC%95%8C%EC%95%84%EB%B3%B4%EA%B8%B0"],"metadata":{"id":"t_P65y37l3Lt"}},{"cell_type":"markdown","source":["sparse : categorical value\n","dense : numerical value\n"],"metadata":{"id":"b0E4hwV0l8oP"}},{"cell_type":"markdown","source":["## 3-1. vectorize train and test data"],"metadata":{"id":"U7k5yh_hmBaL"}},{"cell_type":"code","source":["# train_sessions.head()\n","# test_sessions['aid']=test_sessions['aid'].astype(int)\n","\n","# train_sessions.to_parquet('train.parquet')\n","# test_sessions.to_parquet('test.parquet')\n","\n","# train=pl.read_parquet('train.parquet')\n","# test=pl.read_parquet('test.parquet')"],"metadata":{"id":"wDWD29vjmIMh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%time\n","train_pairs = (pl.concat([train, test])\n","    .groupby('session').agg([\n","        pl.col('aid'),\n","        pl.col('aid').shift(-1).alias('aid_next')\n","    ])\n","    .explode(['aid', 'aid_next'])\n","    .drop_nulls()\n",")[['aid', 'aid_next']]"],"metadata":{"id":"HtXzbUttmMur"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_pairs.shape[0] /100000"],"metadata":{"id":"75HOOKlHmOAT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["cardinality that need to create the embedding layer"],"metadata":{"id":"aG21kIwJmPLd"}},{"cell_type":"code","source":["cardinality_aids = max(train_pairs['aid'].max(),train_pairs['aid_next'].max())\n","cardinality_aids"],"metadata":{"id":"OViCqyvbmQGG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from torch import nn\n","from torch.utils.data import Dataset, DataLoader\n","\n","class ClickDataset(Dataset) :\n","    def __init__(self,pairs) :\n","        self.aid1 = pairs['aid'].to_numpy()\n","        self.aid2 = pairs['aid_next'].to_numpy()\n","    def __getitem__(self, idx) :\n","        aid1 = self.aid1[idx]\n","        aid2 = self.aid2[idx]\n","        return [aid1,aid2]\n","    def __len__(self) :\n","        return len(self.aid1)\n","    \n","train_ds = ClickDataset(train_pairs[:len(train_sessions)])\n","test_ds = ClickDataset(train_pairs[len(train_sessions):])"],"metadata":{"id":"s5VYm5RamRDH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["check how quickly we can iterate over a single epoch with a batch size 65535."],"metadata":{"id":"JxWB5s49mS05"}},{"cell_type":"code","source":["train_ds = ClickDataset(train_pairs)\n","train_dl_pytorch = DataLoader(train_ds, 65535, True, num_workers=2)"],"metadata":{"id":"ddTMn0QRmSpx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%time\n","\n","for batch in train_dl_pytorch :\n","    aid1,aid2 = batch[0], batch[1]"],"metadata":{"id":"mPzorsUNmU0r"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Load merlin"],"metadata":{"id":"Y74l0DzJmWSz"}},{"cell_type":"code","source":["!pip install merlin-dataloader==0.0.2"],"metadata":{"id":"fWYuc05RmU-M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["to using merlin-dataloader, create train_pairs and valid_pairs as parquet file."],"metadata":{"id":"_Ycqf1ckmbJW"}},{"cell_type":"code","source":["from merlin.loader.torch import Loader \n","from merlin.io import Dataset"],"metadata":{"id":"GIZC8_CPmcN0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_pairs[:len(train_sessions)].to_pandas().to_parquet('train_pairs.parquet')\n","train_pairs[len(train_sessions):].to_pandas().to_parquet('valid_pairs.parquet')"],"metadata":{"id":"3Jqsrl51mco5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from merlin.io import Dataset\n","\n","train_ds = Dataset('train_pairs.parquet')\n","train_dl_merlin = Loader(train_ds, 65536, True)"],"metadata":{"id":"hd-6Yo8pmcxy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%time\n","\n","for batch, _ in train_dl_merlin:\n","    aid1, aid2 = batch['aid'], batch['aid_next']"],"metadata":{"id":"WRiIxEX5mfFF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["train matrix factorization model\n","\n","documentation\n","- https://discuss.pytorch.org/t/indexing-embeddings-matrix-factorization/57268"],"metadata":{"id":"z7TTjwBumgm9"}},{"cell_type":"code","source":["class MatrixFactorization(nn.Module) :\n","    def __init__(self, n_aids, n_factors) :\n","        super().__init__()\n","        self.aid_factors = nn.Embedding(n_aids, n_factors, sparse=True)\n","        \n","    def forward(self, aid1,aid2) :\n","        aid1 = self.aid_factors(aid1)\n","        aid2 = self.aid_factors(aid2)\n","        return (aid1*aid2).sum(dim=1)\n","\n","class AverageMeter(object) :\n","    def __init__(self, name, fmt=':f') :\n","        self.name = name\n","        self.fmt =fmt\n","        self.reset()\n","        \n","    def reset(self) :\n","        self.avg=0\n","        self.val=0\n","        self.sum=0\n","        self.count = 0\n","    def update(self, val, n=1) :\n","        self.val = val\n","        self.sum +=val*n\n","        self.count +=n\n","        self.avg = self.sum / self.count\n","        \n","    def __str__(self) :\n","        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n","        return fmtstr.format(**self.__dict__)\n","    \n","valid_ds = Dataset('valid_pairs.parquet')\n","valid_dl_merlin = Loader(valid_ds, 65536, True)"],"metadata":{"id":"2sPeujLLmfOO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.optim import SparseAdam\n","import torch\n","\n","num_epochs = 1\n","lr = 0.1\n","\n","model = MatrixFactorization(cardinality_aids+1, 32)\n","optimizer = SparseAdam(model.parameters(), lr=lr)\n","criterion = nn.BCEWithLogitsLoss()"],"metadata":{"id":"UsT8K2CFmfWx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%time\n","\n","for epoch in range(num_epochs):\n","    for batch, _ in train_dl_merlin:\n","        model.train().cuda()\n","        losses = AverageMeter('Loss', ':.4e')\n","            \n","        aid1, aid2 = batch['aid'], batch['aid_next']\n","        output_pos = model(aid1, aid2)\n","        output_neg = model(aid1, aid2[torch.randperm(aid2.shape[0])])\n","        \n","        output = torch.cat([output_pos, output_neg])\n","        targets = torch.cat([torch.ones_like(output_pos), torch.zeros_like(output_pos)])\n","        loss = criterion(output, targets)\n","        losses.update(loss.item())\n","        \n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        \n","    model.eval()\n","    \n","    with torch.no_grad():\n","        accuracy = AverageMeter('accuracy')\n","        for batch, _ in valid_dl_merlin:\n","            aid1, aid2 = batch['aid'], batch['aid_next']\n","            output_pos = model(aid1, aid2)\n","            output_neg = model(aid1, aid2[torch.randperm(aid2.shape[0])])\n","            accuracy_batch = torch.cat([output_pos.sigmoid() > 0.5, output_neg.sigmoid() < 0.5]).float().mean()\n","            accuracy.update(accuracy_batch, aid1.shape[0])\n","            \n","    print(f'{epoch+1:02d}: * TrainLoss {losses.avg:.3f}  * Accuracy {accuracy.avg:.3f}')"],"metadata":{"id":"ogOiJF2MmjW-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["embeddings"],"metadata":{"id":"xmSrHVAOmk63"}},{"cell_type":"code","source":["embeddings = model.aid_factors.weight.detach().cpu().numpy()"],"metadata":{"id":"c4BJ-1dkmjel"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["check nearest neighbors search "],"metadata":{"id":"dFQ62Lw5mnVH"}},{"cell_type":"code","source":["%%time\n","\n","from annoy import AnnoyIndex\n","\n","fm = AnnoyIndex(32, 'euclidean')\n","for i, v in enumerate(embeddings) :\n","    fm.add_item(i,v)\n","    \n","fm.build(10)"],"metadata":{"id":"bVccuaCAmnz0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fm.get_nns_by_item(123,20)"],"metadata":{"id":"ChyW8aCjmpMc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["create submission for Factorization Matrix"],"metadata":{"id":"09ixNmP3mqVk"}},{"cell_type":"code","source":["from collections import defaultdict\n","\n","session_types = ['clicks', 'carts', 'orders']\n","test_session_AIDs = test.to_pandas().reset_index(drop=True).groupby('session')['aid'].apply(list)\n","test_session_types = test.to_pandas().reset_index(drop=True).groupby('session')['type2id'].apply(list)\n","\n","labels = []\n","\n","type_weight_multipliers = {0: 1, 1: 6, 2: 3}\n","for AIDs, types in zip(test_session_AIDs, test_session_types):\n","    if len(AIDs) >= 20:\n","        # if we have enough aids (over equals 20) we don't need to look for candidates! we just use the old logic\n","        weights=np.logspace(0.1,1,len(AIDs),base=2, endpoint=True)-1\n","        aids_temp=defaultdict(lambda: 0)\n","        for aid,w,t in zip(AIDs,weights,types): \n","            aids_temp[aid]+= w * type_weight_multipliers[t]\n","            \n","        sorted_aids=[k for k, v in sorted(aids_temp.items(), key=lambda item: -item[1])]\n","        labels.append(sorted_aids[:20])\n","    else:\n","        # here we don't have 20 aids to output -- we will use approximate nearest neighbor search and our embeddings\n","        # to generate candidates!\n","        AIDs = list(dict.fromkeys(AIDs[::-1]))\n","        # let's grab the most recent aid\n","        most_recent_aid = AIDs[0]\n","        \n","        \n","        # and look for some neighbors!\n","        nns = fm.get_nns_by_item(most_recent_aid, 21)[1:]\n","                        \n","        labels.append((AIDs+nns)[:20])"],"metadata":{"id":"ObTmfHC3mrWf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["session_types = ['clicks','carts','orders']\n","labels_as_strings = [' '.join([str(l) for l in lls]) for lls in labels]\n","\n","predictions = pd.DataFrame(data={'session_type': test_aids.index, 'labels': labels_as_strings})\n","\n","prediction_dfs = []\n","\n","\n","for st in session_types:\n","    prediction = predictions.copy()\n","    prediction.session_type = prediction.session_type.astype('str') + f'_{st}'\n","    prediction_dfs.append(prediction)\n","\n","fm_submission = pd.concat(prediction_dfs).reset_index(drop=True)"],"metadata":{"id":"STwfqfJDmsbq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fm_submission.head()"],"metadata":{"id":"wQxgL6Svmtkm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["w2v_submission.head()"],"metadata":{"id":"FZR_U3G9mtnp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["sub for indexing"],"metadata":{"id":"CmlHhBCMmwIw"}},{"cell_type":"code","source":["sub = pd.merge(fm_submission, w2v_submission, on=['session_type', 'labels'], how='left')\n","sub = sub.set_index('session_type')"],"metadata":{"id":"gyMovpl6mtp6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["w_labels = w2v_submission['labels']\n","w_labels = w_labels.str.split(' ')\n","f_labels= fm_submission['labels']\n","f_labels = f_labels.str.split(' ')"],"metadata":{"id":"TWzX_Z3hmtso"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sub_lists = []\n","for i in range(len(sub)) :\n","    sub_list = [item for sublist in zip(w_labels[i], f_labels[i]) for item in sublist]\n","    sub_list = list(dict.fromkeys(sub_list))\n","    sub_lists.append(sub_list[:20])\n","sub_lists[:10]"],"metadata":{"id":"Lr-psc1ZmtvF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["labels_as_strings = [' '.join([str(l) for l in lls]) for lls in sub_lists]\n","submission = pd.DataFrame(data={'session_type': sub.index, 'labels': labels_as_strings})\n","\n","submission = submission.sort_values('session_type', axis=0, ascending=True)\n","submission.to_csv('submission.csv', index=False)"],"metadata":{"id":"AAFjE5W8mz3K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["submission"],"metadata":{"id":"vLsMXvcTm1D0"},"execution_count":null,"outputs":[]}]}