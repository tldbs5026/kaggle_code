{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOGBl2EqkEShRylPK1XoDTe"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":[],"metadata":{"id":"d-ShwcnYkXVv"}},{"cell_type":"markdown","source":["# load lib and data"],"metadata":{"id":"A6vEztKVBlu1"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"_AboVim67FtQ"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import os\n","from pathlib import Path\n","import json\n","\n","from datetime import timedelta\n","from tqdm.notebook import tqdm\n","from collections import Counter\n","from heapq import nlargest\n","\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","%matplotlib inline\n","sns.set_theme()\n","\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","source":["#paths\n","\n","data_path = Path(\"/kaggle/input/otto-recommender-system\")\n","train_df = data_path/'train.jsonl'\n","test_df = data_path/'test.jsonl'\n","sample_sub_path = Path('/kaggle/input/otto-recommender-system/sample_submission.csv')"],"metadata":{"id":"UYDPZ2PmPsll"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# json to DF"],"metadata":{"id":"wDkkCQurBqlt"}},{"cell_type":"code","source":["def read_jsonl(target: str) -> pd.DataFrame():\n","    sessions = pd.DataFrame()\n","    chunks = pd.read_json(data_path / f'{target}.jsonl', lines=True, chunksize=1000)\n","\n","    for e, chunk in enumerate(chunks):\n","        event_dict = {\n","            'session': [],\n","            'aid': [],\n","            'ts': [],\n","            'type': [],\n","        }\n","        if e < 2:\n","            for session, events in zip(chunk['session'].tolist(), chunk['events'].tolist()):\n","                for event in events:\n","                    event_dict['session'].append(session)\n","                    event_dict['aid'].append(event['aid'])\n","                    event_dict['ts'].append(event['ts'])\n","                    event_dict['type'].append(event['type'])\n","            chunk_session = pd.DataFrame(event_dict)\n","            sessions = pd.concat([sessions, chunk_session])\n","        else:\n","            break\n","    return sessions.reset_index(drop=True)\n","train_sessions = read_jsonl('train')\n","test_sessions = read_jsonl('test')"],"metadata":{"id":"MSQCawNRPtZq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_sessions.head(3)"],"metadata":{"id":"1wpvJwoqBu5N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_sessions.head(3)"],"metadata":{"id":"cWmSd5JQBwGt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["type2id = {'clicks' :0, \"carts\" : 1, 'orders' : 2}\n","\n","train_sessions['type2id'] = train_sessions['type'].apply(lambda x : type2id[x])\n","test_sessions['type2id'] = test_sessions['type'].apply(lambda x : type2id[x])"],"metadata":{"id":"TkwFCPBHBxLL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_sessions.head()"],"metadata":{"id":"Zlh0t5mcBySR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_sessions.head()"],"metadata":{"id":"4YUewUJxByWh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install polars\n","\n","import multiprocessing\n","import polars as pl\n","from gensim.test.utils import common_texts\n","from gensim.models import Word2Vec"],"metadata":{"id":"Tc9aft42PuK3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## calculating the scores used for co-visitation matrix\n","\n","reference : https://www.kaggle.com/code/radek1/co-visitation-matrix-simplified-imprvd-logic, https://www.kaggle.com/code/alberteinsten/cudf-pandas-proof-of-concept-lgbm-ranker"],"metadata":{"id":"dFWmDNKvB0q-"}},{"cell_type":"code","source":["def add_session_length(df) :\n","    df['session_length'] = train_sessions.groupby('session')['ts'].transform('count')\n","    return df\n","\n","def add_action_num_reverse_chrono(df):\n","    df['action_num_reverse_chrono'] = df.session_length - df.groupby('session').cumcount() - 1\n","    return df\n","\n","def add_log_recency_score(df):\n","    linear_interpolation = 0.1 + ((1-0.1) / (df['session_length']-1)) * (df['session_length']-df['action_num_reverse_chrono']-1)\n","    df['log_recency_score'] = (2 ** linear_interpolation - 1).fillna(1.0)\n","    return df\n","\n","def add_type_weighted_log_recency_score(df):\n","    type_weights = {0:1, 1:6, 2:3}\n","    df['type_weighted_log_recency_score'] = df['log_recency_score'] / df['type2id'].map(type_weights)\n","    return df\n","\n","def apply(df, pipeline) :\n","    for f in pipeline :\n","        df = f(df)\n","\n","    return df\n"],"metadata":{"id":"Px4cPEOUBugs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pipeline = [add_session_length, add_action_num_reverse_chrono, add_log_recency_score, add_type_weighted_log_recency_score]\n","\n","apply(train_sessions, pipeline)\n","apply(test_sessions, pipeline)"],"metadata":{"id":"H8pJyvm1Pvc2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_sessions.head()"],"metadata":{"id":"-QHhNrMuB5Ha"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_sessions.head()"],"metadata":{"id":"2O0WahY-B5Kb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_sessions['gt'] =1"],"metadata":{"id":"rjSI3mg1B7RJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train = train_sessions.merge(test_sessions, on=['session','ts','type', 'aid','type2id','session_length','action_num_reverse_chrono','log_recency_score','type_weighted_log_recency_score'], how='left')\n","\n","train['gt'] = train['gt'].fillna(0)\n","train = train.sort_values('session').reset_index(drop=True)"],"metadata":{"id":"JhHdzhWFB7UT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train.gt.value_counts()"],"metadata":{"id":"dDYWwZ-_B7Xy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 여기서부터 word2vec의 similarity통한 예측값을 하나의 columns으로 받은 후 그 값을 포함하여 새로운 예측값을 만드는 것."],"metadata":{"id":"DGpHAg3RB_T2"}},{"cell_type":"markdown","source":["# Word2Vec"],"metadata":{"id":"W6K-k-vbCMwl"}},{"cell_type":"code","source":["import hashlib\n","import os\n","from gensim.models import Word2Vec\n","from gensim.models import KeyedVectors\n","os.environ[\"PYTHONHASHSEED\"] = str(42)\n","def hashfxn(x):\n","    return int(hashlib.md5(str(x).encode()).hexdigest(), 16)"],"metadata":{"id":"xFqp4hK3B7bL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["preprocessing and create extra columns\n","\n","reference : https://www.kaggle.com/code/alberteinsten/cudf-pandas-proof-of-concept-lgbm-ranker, https://www.kaggle.com/code/radek1/word2vec-how-to-training-and-submission,\n","https://www.kaggle.com/code/cdeotte/candidate-rerank-model-lb-0-575"],"metadata":{"id":"te-Fc7ssCQhJ"}},{"cell_type":"code","source":["raw_corpus = []\n","for session, group_df in tqdm(train_sessions.groupby(['session'])):\n","    raw_corpus.append(list(group_df['aid'].astype(str) + '_' + group_df['type']))\n","for session, group_df in tqdm(test_sessions.groupby(['session'])):\n","    raw_corpus.append(list(group_df['aid'].astype(str) + '_' + group_df['type']))"],"metadata":{"id":"-WnmpDD9B7fi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(raw_corpus)"],"metadata":{"id":"f9nxvFA8B7iY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["w2vec =Word2Vec(sentences=raw_corpus, vector_size=100, window=5, min_count=1, sg=0, workers=-1, seed=42, hashfxn=hashfxn)"],"metadata":{"id":"7h-oBhBOCS5N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_sessions.head(2)"],"metadata":{"id":"JqnwY1UECS8p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_sessions.head(3)"],"metadata":{"id":"KAafKgQ2CTAl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sub_list = []\n","\n","for session, group_df in tqdm(test_sessions.groupby(['session'])):\n","    aid_list = []\n","    results = w2vec.wv.most_similar(positive=list(group_df['aid'].astype(str) + '_' + group_df['type']), topn=100)\n","    for result in results:\n","        aid = result[0].split('_')[0]\n","        if aid not in aid_list:\n","            aid_list.append(aid)\n","        if len(aid_list) == 20:\n","            aid_list = ' '.join(aid_list)\n","            break\n","\n","    sub_list.append([f'{session}_clicks', aid_list])\n","    sub_list.append([f'{session}_carts', aid_list])\n","    sub_list.append([f'{session}_orders', aid_list])\n","\n","sub_list[:1]"],"metadata":{"id":"uIyLh0XJCTDz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["submission = pd.DataFrame(sub_list, columns=['session_type','labels'])\n","submission.head(3)"],"metadata":{"id":"SA-ZJc3mCTHm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%time\n","\n","from annoy import AnnoyIndex\n","\n","aid2idx = {aid: i for i, aid in enumerate(w2vec.wv.index_to_key)}\n","index = AnnoyIndex(100, 'euclidean')\n","\n","for aid, idx in aid2idx.items():\n","#     print(aid)\n","#     print(idx)\n","    index.add_item(idx, w2vec.wv.vectors[idx])\n","index.build(20)"],"metadata":{"id":"DT9kDtoKCTK7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_sessions.head()\n","test_sessions['aid']=test_sessions['aid'].astype(str)"],"metadata":{"id":"GwGif9iEn7ti"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_aids = test_sessions.groupby('session')['aid'].apply(list)\n","test_types = test_sessions.groupby('session')['type2id'].apply(list)"],"metadata":{"id":"tlPf7dcbn7w7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_aids"],"metadata":{"id":"X4tRgNB1n70E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_types"],"metadata":{"id":"8awM2i8sn721"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["this format referenced by @radek.\n","https://www.kaggle.com/code/radek1/word2vec-how-to-training-and-submission\n","\n","defaultdict\n"],"metadata":{"id":"DQYhnQNooAVd"}},{"cell_type":"code","source":["from collections import defaultdict\n","\n","labels = []\n","type_weight_multipliers = {0: 1, 1: 6, 2: 3}\n","\n","for AIDs, types in zip(test_aids,test_types):\n","#     print(test_aids,test_types)\n","    if len(AIDs) >= 20:\n","        # if we have enough aids (over equals 20) we don't need to look for candidates! we just use the old logic\n","        weights=np.logspace(0.1,1,len(AIDs),base=2, endpoint=True)-1\n","        aids_temp=defaultdict(lambda: 0)\n","        for aid,w,t in zip(AIDs,weights,types):\n","            aids_temp[aid]+= w * type_weight_multipliers[t]\n","\n","        sorted_aids=[k for k, v in sorted(aids_temp.items(), key=lambda item: -item[1])]\n","        labels.append(sorted_aids[:20])\n","    else:\n","        # here we don't have 20 aids to output -- we will use word2vec embeddings to generate candidates!\n","        AIDs = list(dict.fromkeys(AIDs[::-1]))\n","#         print(AIDs)\n","        # let's grab the most recent aid\n","        most_recent_aid = AIDs[0]\n","#         print(aid2idx[most_recent_aid])\n","        # and look for some neighbors!\n","        nns = [w2vec.wv.index_to_key[i] for i in index.get_nns_by_item(aid2idx[most_recent_aid], 21)[1:]]\n","\n","        labels.append((AIDs+nns)[:20])"],"metadata":{"id":"NKJb_fgKoAvV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["let's make submission files for word2vec"],"metadata":{"id":"iotDSk8ioECs"}},{"cell_type":"code","source":["session_types = ['clicks','carts','orders']\n","labels_as_strings = [' '.join([str(l) for l in lls]) for lls in labels]\n","\n","predictions = pd.DataFrame(data={'session_type': test_aids.index, 'labels': labels_as_strings})\n","\n","prediction_dfs = []\n","\n","\n","for st in session_types:\n","    prediction = predictions.copy()\n","    prediction.session_type = prediction.session_type.astype('str') + f'_{st}'\n","    prediction_dfs.append(prediction)\n","\n","w2v_submission = pd.concat(prediction_dfs).reset_index(drop=True)\n","w2v_submission"],"metadata":{"id":"P5ralityoEiE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["submission1 = w2v_submission\n","submission1.to_csv('submission.csv', index=False)"],"metadata":{"id":"g6tjYntFoGsJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#FM\n","\n","\n","\n","https://deepctr-doc.readthedocs.io/en/latest/Features.html#sparsefeat\n","\n","https://deepctr-doc.readthedocs.io/en/latest/Examples.html\n","\n","https://projectlog-eraser.tistory.com/41\n","\n","https://arxiv.org/pdf/1703.04247v1.pdf\n","\n","https://aplab.tistory.com/entry/%EC%B6%94%EC%B2%9C-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98-%ED%83%90%EC%83%89-Deep-FM-%EC%95%8C%EC%95%84%EB%B3%B4%EA%B8%B0"],"metadata":{"id":"NBemPFjsMbxx"}},{"cell_type":"code","source":["!pip install polars\n","import polars as pl"],"metadata":{"id":"kRWv47JmMhgj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# vectorize train and test data"],"metadata":{"id":"nL4PJkrDCdsR"}},{"cell_type":"code","source":["train.head()"],"metadata":{"id":"Tprx7FW6CTON"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# train['aid'] = train['aid'].astype(str)\n","train['aid_next'] = train.aid.shift(-1)"],"metadata":{"id":"u0xBhozWPvgs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train.isnull().sum()\n","train.aid_next.dropna(inplace=True)"],"metadata":{"id":"6LQSvy3aMl1O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_pairs = train[['aid','aid_next']]\n","train_pairs.dropna(inplace=True)"],"metadata":{"id":"Fq3oDxI6MnnW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_pairs.to_parquet('pl_train_pairs.parquet')"],"metadata":{"id":"XVtiFB44MqbD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pl_train_pairs=pl.read_parquet('pl_train_pairs.parquet')"],"metadata":{"id":"mOloKMRdMqq0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cardinality_aids = max(pl_train_pairs['aid'].max(),pl_train_pairs['aid_next'].max())\n","cardinality_aids"],"metadata":{"id":"qGkwCBvsMquZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from torch import nn\n","from torch.utils.data import Dataset, DataLoader\n","\n","class ClickDataset(Dataset) :\n","    def __init__(self,pairs) :\n","        self.aid1 = pairs['aid'].to_numpy()\n","        self.aid2 = pairs['aid_next'].to_numpy()\n","    def __getitem__(self, idx) :\n","        aid1 = self.aid1[idx]\n","        aid2 = self.aid2[idx]\n","        return [aid1,aid2]\n","    def __len__(self) :\n","        return len(self.aid1)\n","\n","train_ds = ClickDataset(pl_train_pairs[:100000])\n","test_ds = ClickDataset(pl_train_pairs[100000:])"],"metadata":{"id":"KT1fCn-1Mqxm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_ds = ClickDataset(train_pairs)\n","train_dl_pytorch = DataLoader(train_ds, 65535, True, num_workers=2)"],"metadata":{"id":"EK4OFMVwMusE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%time\n","\n","for batch in train_dl_pytorch :\n","    aid1,aid2 = batch[0], batch[1]"],"metadata":{"id":"Ok6llim6MuvV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install merlin-dataloader==0.0.2"],"metadata":{"id":"_2PkT_z2MuyX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pl_train_pairs[:100000].to_pandas().to_parquet('train_pairs.parquet')\n","pl_train_pairs[100000:].to_pandas().to_parquet('valid_pairs.parquet')"],"metadata":{"id":"ePkqkqigMu1d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pl_train=pl.read_parquet('train_pairs.parquet')\n","pl_train"],"metadata":{"id":"jxzQnMH5Mu4i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from merlin.loader.torch import Loader\n","from merlin.io import Dataset"],"metadata":{"id":"fCCsixHlMu66"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_ds = Dataset('train_pairs.parquet')\n","train_ds"],"metadata":{"id":"m7L-j3OVM0Ha"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dl_merlin = Loader(train_ds, 65535, True)"],"metadata":{"id":"3D3PL6K1M0Ku"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%time\n","\n","for batch, _ in train_dl_merlin:\n","    aid1, aid2 = batch['aid'], batch['aid_next']"],"metadata":{"id":"9zzVgorSM0Nt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class MatrixFactorization(nn.Module) :\n","    def __init__(self, n_aids, n_factors) :\n","        super().__init__()\n","        self.aid_factors = nn.Embedding(n_aids, n_factors, spars=True)\n","\n","    def forward(self, aid1,aid2) :\n","        aid1 = self.aid_factors(aid1)\n","        aid2 = self.aid_factors(aid2)\n","        return (aid1*aid2).sum(dim=1)\n","\n","class AverageMeter(object) :\n","    def __init__(self, name, fmt=':f') :\n","        self.name = name\n","        self.fmt =fmt\n","        self.reset()\n","\n","    def reset(self) :\n","        self.avg=0\n","        self.val=0\n","        self.sum=0\n","        self.count = 0\n","    def update(self, val, n=1) :\n","        self.val = val\n","        self.sum +=val*n\n","        self.count +=n\n","        self.avg = self.sum / self.count\n","\n","    def __str__(self) :\n","        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n","        return fmtstr.format(**self.__dict__)\n","\n","valid_ds = Dataset('valid_paris.parquet')\n","valid_dl_merlin = Loader(valid_ds, 65536, True)"],"metadata":{"id":"EkpMC_cHM0Qm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.optim import SparseAdam\n","\n","num_epochs = 1\n","lr = 0.1\n","\n","model = MatrixFactorization(cardinality_aids+1, 32)\n","optimizer = SparseAdam(model.parameters(), lr=lr)\n","criterion = nn.BCEwithLogitsLoss()"],"metadata":{"id":"eDX9PxTAM0Tb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%time\n","\n","for epoch in range(num_epochs) :\n","    for batch,_ in train_dl_merlin :\n","        model.train()\n","        losses = AverageMeter('Loss', ':.4e')\n","\n","        aid1,aid2 = batch['aid1'], batch['aid_next']\n","        output_pos = model(aid1, aid2)\n","        output_neg = model(aid1,aid2[torch.randperm(aid2.shape[0])])\n","\n","        output = torch.cat([output_pos, output_neg])\n","        targets = torch.cat([torch.ones_like(output_pos), torch.zeroes_like(output_pos)])\n","        loss = criterion(output, targets)\n","        losses.update(loss.item())\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","    model.eval()\n","\n","    with torch.no_grad():\n","        accuracy = AverageMeter('accuracy')\n","        for batch, _ in valid_dl_merlin :\n","            aid1,aid2 = batch['aid1'], batch['aid_next']\n","            output_pos = model(aid1, aid2)\n","            output_neg = model(aid1,aid2[torch.randperm(aid2.shape[0])])\n","\n","            accuracy_batch = torch.cat([output_pos.sigmoid() > 0.5, output_neg.sigmoid() < 0.5]).float().mean()\n","            accuracy.update(accuracy_batch, aid1.shape[0])\n","\n","\n","    print(f'{epoch+1:0.2d} : * Trainloss {losses.avg:.3f} * accuracy {accuracy.avg:.3f}')"],"metadata":{"id":"mmFJlloeM0VQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["embeddings = model.aid_factors.weight.detach().numpy()"],"metadata":{"id":"c_AZL8YAM4nn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%time\n","\n","from annoy import AnnoyIndex\n","\n","fm = AnnoyIndex(32, 'euclidian')\n","for i, v in enumerate(embeddings) :\n","    index.add_item(i,v)\n","\n","index.build(10)"],"metadata":{"id":"IoPTKdHZM4qv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fm.get_nns_by_item(123,10)"],"metadata":{"id":"oPlsR1fjM4tk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from collections import defaultdict\n","\n","labels = []\n","type_weight_multipliers = {0: 1, 1: 6, 2: 3}\n","\n","for AIDs, types in zip(test_aids,test_types):\n","#     print(test_aids,test_types)\n","    if len(AIDs) >= 20:\n","        # if we have enough aids (over equals 20) we don't need to look for candidates! we just use the old logic\n","        weights=np.logspace(0.1,1,len(AIDs),base=2, endpoint=True)-1\n","        aids_temp=defaultdict(lambda: 0)\n","        for aid,w,t in zip(AIDs,weights,types):\n","            aids_temp[aid]+= w * type_weight_multipliers[t]\n","\n","        sorted_aids=[k for k, v in sorted(aids_temp.items(), key=lambda item: -item[1])]\n","        labels.append(sorted_aids[:20])\n","    else:\n","        # here we don't have 20 aids to output -- we will use word2vec embeddings to generate candidates!\n","        AIDs = list(dict.fromkeys(AIDs[::-1]))\n","#         print(AIDs)\n","        # let's grab the most recent aid\n","        most_recent_aid = AIDs[0]\n","#         print(aid2idx[most_recent_aid])\n","        # and look for some neighbors!\n","        nns = [fm.wv.index_to_key[i] for i in index.get_nns_by_item(fm[most_recent_aid], 21)[1:]]\n","\n","        labels.append((AIDs+nns)[:20])"],"metadata":{"id":"LJkfi6S0M4wS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["session_types = ['clicks','carts','orders']\n","labels_as_strings = [' '.join([str(l) for l in lls]) for lls in labels]\n","\n","predictions = pd.DataFrame(data={'session_type': test_aids.index, 'labels': labels_as_strings})\n","\n","prediction_dfs = []\n","\n","\n","for st in session_types:\n","    prediction = predictions.copy()\n","    prediction.session_type = prediction.session_type.astype('str') + f'_{st}'\n","    prediction_dfs.append(prediction)\n","\n","fm_submission = pd.concat(prediction_dfs).reset_index(drop=True)\n","fm_submission"],"metadata":{"id":"vI_bccdkM4yK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# DeepFM"],"metadata":{"id":"pCYwoUZCJrW0"}},{"cell_type":"code","source":["#load library\n","from deepctr.models import DeepFM\n","from deepcrt.feature_column import SparseFeat, get_feature_names\n","\n","#load data and preprocessing\n","df = data[[x for x in data.columns if 'mapping' in x or x == 'price']]\n","\n","#define feature and target\n","sparse_features = list(df.columns)[1:]\n","target = ['AMT']\n","\n","#define feature columns\n","fixlen_feature_columns = [SparseFeat(feat, df[feat].nunique(), embedding_dim=6) \\\n","                          for feat in sparse_features]\n","linear_feature_columns = fixlen_feature_columns\n","dnn_feature_columns=fixlen_feature_columns\n","feature_names = get_feature_names(line_feature_columns + dnn_feature_columns)\n","\n","#split train and test set\n","\n","x_train,x_test,y_train,y_test = train_test_split(df[sparse_fefatures], df[target], test_size=0.2, random_state=42)\n","\n","#input data\n","x_train = {name:x_train[name].values for name in feature_names}\n","x_test = {name:x_test[name].values for name in feature_names}\n","\n","#create a model the compile\n","model = DeepFM(linear_feature_columns, dnn_feature_columns, task='regression')\n","model.compile(optimizer=optimizer.SparseAdam(lr=0.01), loss='cross_entropy')\n","criteion=  nn.BCEWithLogitLoss()\n","\n","hist = model.fit(x_train, y_train, batch_size=256, epochs=500, verbose=1,validation_split=0.2)\n","\n","\n","\n"],"metadata":{"id":"lqPUf2tSJsU5"},"execution_count":null,"outputs":[]}]}