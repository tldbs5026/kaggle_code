{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPyi6mQgqN5L6SPsLKetaH+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"TG3NjjwwXh9n","executionInfo":{"status":"aborted","timestamp":1671508004599,"user_tz":-540,"elapsed":6,"user":{"displayName":"김시윤","userId":"10463797081079810917"}}},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from tqdm.notebook import tqdm\n","\n","from pathlib import Path\n","\n","data_path = Path('/kaggle/input/otto-recommender-system/')"]},{"cell_type":"code","source":["def read_jsonl(target: str) -> pd.DataFrame():\n","    sessions = pd.DataFrame()\n","    chunks = pd.read_json(data_path / f'{target}.jsonl', lines=True, chunksize=150)\n","\n","    for e, chunk in enumerate(chunks):\n","        event_dict = {\n","            'session': [],\n","            'aid': [],\n","            'ts': [],\n","            'type': [],\n","        }\n","        if e < 2:\n","            for session, events in zip(chunk['session'].tolist(), chunk['events'].tolist()):\n","                for event in events:\n","                    event_dict['session'].append(session)\n","                    event_dict['aid'].append(event['aid'])\n","                    event_dict['ts'].append(event['ts'])\n","                    event_dict['type'].append(event['type'])\n","            chunk_session = pd.DataFrame(event_dict)\n","            sessions = pd.concat([sessions, chunk_session])\n","        else:\n","            break\n","    return sessions.reset_index(drop=True)\n","\n","train= read_jsonl('train')\n","test= read_jsonl('test')"],"metadata":{"id":"fY4oBcIkXlo6","executionInfo":{"status":"aborted","timestamp":1671508004600,"user_tz":-540,"elapsed":6,"user":{"displayName":"김시윤","userId":"10463797081079810917"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":174},"id":"h9KIu_bio4QH","executionInfo":{"status":"error","timestamp":1671508004598,"user_tz":-540,"elapsed":7,"user":{"displayName":"김시윤","userId":"10463797081079810917"}},"outputId":"70b1b781-e0d5-4845-d988-b533f8ed5dfa"},"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-3b77fa18a747>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"]}]},{"cell_type":"markdown","source":["Word2Vec"],"metadata":{"id":"N1Om1uaRXmbK"}},{"cell_type":"code","source":["import hashlib\n","import os\n","from gensim.models import Word2Vec\n","from gensim.models import KeyedVectors\n","from gensim.test.utils import common_texts\n","os.environ[\"PYTHONHASHSEED\"] = str(42)\n","def hashfxn(x):\n","    return int(hashlib.md5(str(x).encode()).hexdigest(), 16)"],"metadata":{"id":"IHsGGjTmXntr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["raw_corpus = []\n","for session, group_df in tqdm(train.groupby(['session'])):\n","    raw_corpus.append(list(group_df['aid'].astype(str) + '_' + group_df['type']))\n","for session, group_df in tqdm(test.groupby(['session'])):\n","    raw_corpus.append(list(group_df['aid'].astype(str) + '_' + group_df['type']))"],"metadata":{"id":"TMgbp-l-XpTB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["w2vec=Word2Vec(sentences=raw_corpus, vector_size=100, window=5, min_count=1, sg=0, workers=-1, seed=42, hashfxn=hashfxn)"],"metadata":{"id":"oBx_PUIfXqOL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["With the model fully train, let us use similarity btw trained representations of our aids to create a submission.\n","\n","The search functionality where we look for nearest neighbors in the embedding space is built into gensim, but it is unfortunately super slow. Let's use annoy which is much fater."],"metadata":{"id":"GDhCNxkQ6bqL"}},{"cell_type":"code","source":["%%time\n","\n","from annoy import AnnoyIndex\n","\n","aid2idx = {aid: i for i, aid in enumerate(w2vec.wv.index_to_key)}\n","index = AnnoyIndex(32, 'euclidean')\n","\n","for aid, idx in aid2idx.items():\n","    index.add_item(idx, w2vec.wv.vectors[idx])\n","    \n","index.build(10)"],"metadata":{"id":"BQNTiSjVYB_O"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["output submission"],"metadata":{"id":"YoAJ2HDQYKOh"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","\n","from collections import defaultdict\n","\n","sample_sub = pd.read_csv('../input/otto-recommender-system//sample_submission.csv')\n","\n","session_types = ['clicks', 'carts', 'orders']\n","test_session_AIDs = test.to_pandas().reset_index(drop=True).groupby('session')['aid'].apply(list)\n","test_session_types = test.to_pandas().reset_index(drop=True).groupby('session')['type'].apply(list)\n","\n","labels = []\n","\n","type_weight_multipliers = {0: 1, 1: 6, 2: 3}\n","for AIDs, types in zip(test_session_AIDs, test_session_types):\n","    if len(AIDs) >= 20:\n","        # if we have enough aids (over equals 20) we don't need to look for candidates! we just use the old logic\n","        weights=np.logspace(0.1,1,len(AIDs),base=2, endpoint=True)-1\n","        aids_temp=defaultdict(lambda: 0)\n","        for aid,w,t in zip(AIDs,weights,types): \n","            aids_temp[aid]+= w * type_weight_multipliers[t]\n","            \n","        sorted_aids=[k for k, v in sorted(aids_temp.items(), key=lambda item: -item[1])]\n","        labels.append(sorted_aids[:20])\n","    else:\n","        # here we don't have 20 aids to output -- we will use word2vec embeddings to generate candidates!\n","        AIDs = list(dict.fromkeys(AIDs[::-1]))\n","        \n","        # let's grab the most recent aid\n","        most_recent_aid = AIDs[0]\n","        \n","        # and look for some neighbors!\n","        nns = [w2vec.wv.index_to_key[i] for i in index.get_nns_by_item(aid2idx[most_recent_aid], 21)[1:]]\n","                        \n","        labels.append((AIDs+nns)[:20])"],"metadata":{"id":"hTW3bxD8XrTd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Word2vec to generate Candidates/feature for training"],"metadata":{"id":"0T3HzKpZaqwm"}},{"cell_type":"markdown","source":["Just like a covisiation matrix, for any AID Word2Vec can give us a list of AIds resembling our query AID. The output will be ordered starting with AIds that are most alike.\n","\n","In order for us to visualize what is happening, let me give you a simplified example."],"metadata":{"id":"-W_riFSnavMs"}},{"cell_type":"markdown","source":["##Mock data\n","\n","In our data, we have aids organized by session."],"metadata":{"id":"SlNQQumla8H8"}},{"cell_type":"code","source":["data = pl.DataFrame(data={'session' : [0,0,1,1], 'aid' : [10,20,20,30], 'type' : [0,0,1,0]})\n","data"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":193},"id":"fKXkPUWEXrtr","executionInfo":{"status":"error","timestamp":1671252736168,"user_tz":-540,"elapsed":5,"user":{"displayName":"김시윤","userId":"10463797081079810917"}},"outputId":"c6cfed02-7c22-442c-dd42-9b60d9575f41"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-7255d9838781>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'session'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'aid'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'type'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'pl' is not defined"]}]},{"cell_type":"markdown","source":["We can use word2vec to generate candidates. For instance, maybe using word2ved we would generate the following candidates for the sessions in our data :\n","\n","{0 : [11,20], 1 : [25,6]}\n","\n","We can reshape our candidates to look as follows :"],"metadata":{"id":"XVIa22wna_fZ"}},{"cell_type":"code","source":["candidates = pl.DataFrame(data={'session' : [0,0,1,1], 'aid' : [11,20,25,6]})\n","\n","cancidates"],"metadata":{"id":"Vn3klHwDbYzt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["As you see, for our candidates we don't have too much information apart from session and aid. This is exactly like the output word2ved can give us\n","\n","And that is okay. Our ranker can deal with that, For some rwos we will have information in this or that column, for another we won't. This is not an issue to a ranking model.\n","\n","Here, out ranker will see that we don't have type information for candidates. But we will create another imnportnat column that will allow it to uniquely identify our candidates as coming from word2vec."],"metadata":{"id":"jNOfTPrebiC4"}},{"cell_type":"markdown","source":["##1. Add ordering information to our candidates.\n","\n","The order is important. A candidate appearing earlier in the list of candidates in some sense has a higher score, is more similar to the AIDs in a session."],"metadata":{"id":"ZVjgyykDb5m2"}},{"cell_type":"code","source":["candidates = candidates.with_column(pl.col('aid').cumcount().over('session').alias('word2vec_rank')+1)\n","\n","candidates"],"metadata":{"id":"3uUvntgVcELH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##2. Merge this information onto candidates\n","\n","Now, we need to take this information and add this onto our original data.\n","\n","But how do we add candidates?! If we just concat these df together, we will have duplicate entry for sesion 0 for aid of 20.\n","\n","What we need to do is a join but a specific kind\n","\n","We want to keep the rows that are already in data, append information to them where there is a match AND create new rows if there isn't.\n","\n","so, using ***outer join***"],"metadata":{"id":"JE5hDN46cXaz"}},{"cell_type":"code","source":["data = data.join(candidates, on=['session','aid'], how='outer')\n"],"metadata":{"id":"Yg2lMcFFdAuU"},"execution_count":null,"outputs":[]}]}