{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["tTp9c5sEjtjQ","Sl_xL_JPmv0f","ulQKBCZD5AqN","h6xmt-w45aq5","krYs2zFqEPCH"],"authorship_tag":"ABX9TyO1twCIUx9krHk8aoYw5VFP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#1.load the data"],"metadata":{"id":"tTp9c5sEjtjQ"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import os\n","from pathlib import Path\n","import json\n","\n","from datetime import timedelta\n","from tqdm.notebook import tqdm\n","from collection import Counter\n","from heapq import nlargest\n","\n","#그림 그리기\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","%matplotlib inline\n","sns.set_theme()\n","\n","import warnings\n","warnings.filterwarnings('ignore')"],"metadata":{"id":"oi34KHvFa-s5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_path = Path(\"/kaggle/input/otto-recommender-system\")\n","\n","train= data_path/'train.jsonl'\n","test= data_path/'test.jsonl'\n","\n","sample_sub = Path(\"/kaggle/input/otto-recommender-system/sample_submission.csv\")"],"metadata":{"id":"4uMqEkyqbm-d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open(train,'r') as f :\n","  print(f'total lines in train data is : {len(f.readlines())}')\n","  "],"metadata":{"id":"saZxbW7Mb7Nx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#2. sample_looking"],"metadata":{"id":"ZKwuIESUjvLW"}},{"cell_type":"markdown","source":["2-1. events"],"metadata":{"id":"4wtWaveynaNa"}},{"cell_type":"code","source":["sample_size =5\n","\n","chunks = pd.read_json(train, lines=True, chunksize=sample_size)\n","\n","for chunk in chunks :\n","  sample_train_df = chunk\n","  break"],"metadata":{"id":"XUq7l4QycLS_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sample_train_df.set_index('session', drop=True, inplace=True)\n","\n","for i, row in tqdm(sample_train_df.iterrows(), total=len(sample_train_df)) :\n","  actions = row['events']\n","  print(actions)"],"metadata":{"id":"EK9NCrdGcRaE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#extract first session of sample_train_df\n","\n","example_session = sample_train_df.iloc[0].item()\n","\n","example_session[0]"],"metadata":{"id":"ZDm6CfXveRZB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#time of session\n","time_elapsed = example_session[-1]['ts'] - example_session[0]['ts']\n","\n","\n","#하나의 세션에서 dict의 개수 추출\n","for action in example_session :\n","  action_counts[action['type']] = action_counts.get(action['type'],0) +1"],"metadata":{"id":"NzM2T6YdczSa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#extract info from each session and make it to the df\n","\n","action_counts_list, article_id_list, session_length_ts_list, session_length_action_list = [[] for i in range(4)]\n","\n","overall_action_counts = {}\n","overall_article_id_counts = {}\n","\n","for i, row in tqdm(sample_df_train_df.iterrows(), total=len(sample_df_train_df)) :\n","  actions = row['events']\n","\n","  action_counts = {}\n","  article_id_counts = {}\n","\n","  action in actions :\n","  action_counts[action['type']] = action_counts.get(action['type'],0) +1\n","  article_id_counts[action['aid']] = article_id_counts.get(action['aid'],0) +1\n","\n","  overall_action_counts[action['type']] = overall_action_counts.get(action['type'],0) +1\n","  overall_article_id_counts[action['aid']] = overall_article_id_counts.get(actio['aid'],0) +1\n","\n","  session_length_time = action['ts'][-1] - action['ts'][0]\n","\n","  #add to list\n","  action_counts_list.append(action_counts)\n","  article_id_list.append(article_id_counts)\n","  session_length_ts_list.append(session_length_time)\n","  session_length_action_list.append(len(actions))\n","\n","sample_train_df['action_counts'] = action_counts_list\n","sample_train_df['article_id_list'] =article_id_list\n","sample_train_df['session_length_time'] =session_length_time_list\n","sample_train_df['session_length_hours'] = sample_train_df['session_length_time']*2.77778e-7\n","sample_train_df['session_length_action'] = session_length_action_list"],"metadata":{"id":"O2Z4U78Te0lt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["total_actions = sum(overall_action_counts.values())\n","\n","plt.figrue(figsize=(8,6))\n","sns.barplot(overall_action_counts.keys(), [i/total_actions for i in overall_action_counts.values()])\n","plt.title('action')\n","plt.xlabel('type')\n","plt.ylabel('counts')\n","plt.show()"],"metadata":{"id":"Cx7rUmgJmS1W"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##2-1. check null values\n","\n","In recommendation systems, we do nto prefer to use stuff like fillna.\n","\n","instead, we make kind of most frequent(popularity of all)."],"metadata":{"id":"kJk0I0CDjzXj"}},{"cell_type":"markdown","source":["##2-2.data optimization"],"metadata":{"id":"3s3TIqjcj16G"}},{"cell_type":"markdown","source":["####Baseline"],"metadata":{"id":"Sl_xL_JPmv0f"}},{"cell_type":"markdown","source":["2-2-1. type"],"metadata":{"id":"-3uLSWGSneCA"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"W3kYwj2ZjoPn"},"outputs":[],"source":["with open(test, 'r') as f :\n","  print(f'test data has {len(f.readlines())} lines')"]},{"cell_type":"code","source":["sample_size=1000\n","\n","chunks = pd.read_json(test, lines=True, chunksize=sample_size)\n","\n","for chunk in chunks :\n","  sample_test_df = chunk\n","  break"],"metadata":{"id":"UlVpXpXim5IS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sample_subsmission = pd.read_csv(sample_sub)"],"metadata":{"id":"A6wBVnWrnCnG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sample_size =15000\n","\n","chunks = pd.read_json(train, lines=True, chunksize=sample_size)\n","\n","clicks_article_list = []\n","carts_article_list = []\n","orders_article_list = []\n","\n","for session, events in enumerate(chunks) :\n","  if session >2 :\n","    break\n","  \n","  sample_train_df = events\n","\n","  for i, row in events.iterrows() :\n","    actions = row['events']\n","\n","    for action in actions :\n","      if action['type'] == 'clicks':\n","        clicks_article_list.append(action['aid'])\n","      elif action['type'] == 'carts':\n","        carts_article_list.append(action['aid'])\n","      else:\n","        orders_article_list.append(action['aid'])"],"metadata":{"id":"gQymYKTCnMAe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["clicks_article_freq = Counter(clicks_article_list)\n","carts_article_freq = Counter(carts_article_list)\n","orders_article_freq = Counter(orders_article_list)"],"metadata":{"id":"5iBnYl6coHJW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["top_clicks_article = nlargest(20, clicks_article_freq, key=clicks_article_freq.get)\n","top_carts_article = nlargest(20, carts_article_freq, key=carts_article_freq.get)\n","top_orders_article = nlargest(20, orders_article_freq, key=orders_article_freq.get)"],"metadata":{"id":"kQzVZfa7oOeB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["frequent_articles = {'clicks': top_click_article, 'carts':top_carts_article, 'order':top_order_article}"],"metadata":{"id":"T7gYE7_9o0lN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for action in ['clicks','carts','order'] :\n","  print(f'most frequent articles for {action} : {frequent_articles[action]}')"],"metadata":{"id":"vFdbPuFUo7U5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_data = pd.read_json(test, lines=True, chunksize=1000)\n","\n","preds = []\n","\n","for chunk in tqdm(test, total=len(test)) :\n","\n","  for i, row in chunk.iterrows() :\n","    actions = row['events']\n","    article_id_list = []\n","\n","    for action in actions :\n","      article_id_list.append(action['aid'])\n","\n","    article_freq = Counter(article_id_list)\n","    top_articles = nlargest(20, article_freq, key=article_freq)\n","\n","    #pad with most popular items in training\n","    padding_size = -(20 - len(top_articles))\n","    for action in ['clicks','carts','order'] :\n","      top_articles = top_articles + frequent_articles[action][padding_size:]\n","      preds.append(\" \".join([str(id) for id in top_articles]))"],"metadata":{"id":"yqTynScapIiJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sample_submission['labels'] = preds\n","\n","sample_submission.to_csv('submission.csv', index=False)"],"metadata":{"id":"n2YtY0MDp13C"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###2-2-1. Word2Vec\n","\n","reference : https://www.kaggle.com/code/takaito/otto-word2vec-tutorial"],"metadata":{"id":"Qh7hUtZyx4xf"}},{"cell_type":"code","source":[],"metadata":{"id":"8CC9TV5JDiwb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#3. Ranking Method\n","\n","- with XGB : https://medium.com/predictly-on-tech/learning-to-rank-using-xgboost-83de0166229d\n","\n","- with NN : https://towardsdatascience.com/learning-to-rank-with-python-scikit-learn-327a5cfd81f"],"metadata":{"id":"Av4MP2D742yf"}},{"cell_type":"markdown","source":["1. 데이터를 불러오기\n","2. word2vec을 통한 vectorization\n","3. vectorization된 결과를 바탕으로 xgboosting(or NNModeling)\n","- 이때, weight=각각 0.1,0.3,0.6으로\n","- 예를들어, model = xgb.XGBRanker(  \n","    tree_method='gpu_hist',\n","    booster='gbtree',\n","    objective='rank:pairwise',\n","    random_state=42, \n","    learning_rate=0.1,\n","    colsample_bytree=0.9, \n","    eta=0.05, \n","    max_depth=6, \n","    n_estimators=110, \n","    subsample=0.75 \n","    )\n","\n","model.fit(X_train, y_train, group=groups, verbose=True)\n","4. 추가적으로 hyper parameter를 학습시키고싶으면 \n","grid search(bayesian optimization이 더 효과적)사용\n","- https://www.cognex.com/ko-kr/blogs/deep-learning/research/overview-bayesian-optimization-effective-hyperparameter-search-technique-deep-learning-2\n"],"metadata":{"id":"sEmfi1Y6tmPe"}},{"cell_type":"markdown","source":["##XGB"],"metadata":{"id":"gw95yPjItIPf"}},{"cell_type":"code","source":["from nvtabular import *\n","from merlin.schema.tags import Tags\n","import polars as pl\n","import xgboost as xgv\n","\n","from merlin.core.utils import Distributed\n","from merlin.models.xgb import XGBoost\n","from nvtabular.ops import AddTags"],"metadata":{"id":"oThybgN5tJkg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## bayesian optimization\n","\n","reference : https://www.cognex.com/ko-kr/blogs/deep-learning/research/overview-bayesian-optimization-effective-hyperparameter-search-technique-deep-learning-2"],"metadata":{"id":"XwoE47gJvjIE"}},{"cell_type":"code","source":["from bayes_opt import BayesianOptimization\n","import numpy as np"],"metadata":{"id":"X_6tfPnavmlT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["1. 목적 함수 정의"],"metadata":{"id":"ax3SaHj0vsJD"}},{"cell_type":"code","source":["def target(x) :\n","  return np.exp(-(x-3)**2) + np.exp(-(3*x-2)**2) + 1/x**2+1"],"metadata":{"id":"YCfaklrpvrZ2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2. BayesianOptimization 객체 생성\n","\n","Bayesianopt의 객체내에는 Surrogate Model인 GP가 내장되어있으며, _gp로 표현됨\n","\n","- target : 목적함수 f(x)\n","- {'x' : (-2,6)} : 입력값 x의 탐색 대상 구간의 dict"],"metadata":{"id":"Skx9WbWMv8OA"}},{"cell_type":"code","source":["bayes_optimizer = BayesianOPtimization(target, {'x' : (-2,6)}, randon_state=42)"],"metadata":{"id":"zCyJ-r6CwHQw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["3. 실행\n","\n","- init_points : 맨 처음 일부 Random Search방법으로 조사할 입력값의 갯수\n","- acq : Acquisition Function 선택 //\n","- acq('ei') : Expected Improvement\n","- xi : exploration-explotation간의 상대적 강도를 조절해주는 파라미터"],"metadata":{"id":"lcM394_wwa_E"}},{"cell_type":"code","source":["bayes_optimizer.maximize(init_points=2, n_iter=14, acq='ei', xi=0.01)"],"metadata":{"id":"e9t_J8LYwcCp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##statistics\n","\n","- Support\n","- Confidence\n","- Lift"],"metadata":{"id":"ulQKBCZD5AqN"}},{"cell_type":"markdown","source":["3-2. Apriori"],"metadata":{"id":"FRMKxyIV5PjO"}},{"cell_type":"markdown","source":["3-3. Similarity\n","\n","- cosine similarity\n","- pearson corr"],"metadata":{"id":"TF74rS_R5Rnt"}},{"cell_type":"markdown","source":["#Collaboritive Filtering\n","\n","- User Based\n","- Content based\n"],"metadata":{"id":"h6xmt-w45aq5"}},{"cell_type":"markdown","source":["##3-5. Word2Vec\n","\n","reference : https://www.kaggle.com/code/radek1/word2vec-how-to-training-and-submission"],"metadata":{"id":"-mu3l6zo8jz8"}},{"cell_type":"markdown","source":["##3-6. Filtering with model\n","\n","\n"],"metadata":{"id":"0gE13TEC5fex"}},{"cell_type":"markdown","source":["NeuralNet\n","\n","reference : https://towardsdatascience.com/learning-to-rank-with-python-scikit-learn-327a5cfd81f"],"metadata":{"id":"aPxnMc9L6Bh7"}},{"cell_type":"markdown","source":["#4. Modeling with hugging_face\n","\n"],"metadata":{"id":"krYs2zFqEPCH"}},{"cell_type":"markdown","source":["##4-1. make my data to train/test split\n","\n","https://www.kaggle.com/code/theoviel/pretraining-with-merlin-s-transformers4rec"],"metadata":{"id":"0X7RzZlqESd8"}}]}